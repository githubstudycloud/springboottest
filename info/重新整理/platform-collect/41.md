I'll help create a comprehensive guide for setting up and configuring various databases and message queues with Spring Boot 3.2.9. Let me outline the structure first:



I've created an outline of the comprehensive guide. Let's start with MySQL configuration, as it's one of the most commonly used databases. Would you like me to provide the detailed MySQL configuration section first?

For MySQL, I'll cover:
1. Installation procedures for both single instance and cluster setups
2. Spring Boot configuration with example code
3. Multi-source configuration and dynamic switching
4. Connection pool management
5. Transaction management
6. Best practices for production deployment

Would you like me to proceed with the MySQL section?# Database and Message Queue Setup Guide with Spring Boot 3.2.9

## Table of Contents

1. MySQL Configuration
    - Single Instance Setup
    - Cluster Setup (Master-Slave)
    - Spring Boot Configuration
    - Multi-source Configuration
    - Dynamic Data Source Switching
    - Best Practices

2. Redis Configuration
    - Standalone Setup
    - Cluster Setup
    - Sentinel Configuration
    - Spring Boot Integration
    - Multi-instance Management
    - Connection Pool Configuration
    - Common Operations

3. RabbitMQ Setup
    - Single Node Installation
    - Cluster Configuration
    - High Availability Setup
    - Spring Boot Integration
    - Exchange and Queue Management
    - Message Reliability
    - Consumer Configuration

4. Kafka Configuration
    - Single Broker Setup
    - Cluster Setup
    - ZooKeeper Integration
    - Spring Boot Configuration
    - Topic Management
    - Producer/Consumer Setup
    - Message Patterns

5. ZooKeeper Setup
    - Standalone Mode
    - Cluster Mode
    - Configuration Parameters
    - Spring Boot Integration
    - Service Registration
    - Distributed Lock Implementation
    - Watch Mechanisms

6. ClickHouse Configuration
    - Single Node Setup
    - Distributed Setup
    - Replication Configuration
    - Spring Boot Integration
    - Data Models
    - Query Optimization
    - Batch Processing

7. MongoDB Setup
    - Standalone Setup
    - Replica Set Configuration
    - Sharding Setup
    - Spring Boot Integration
    - Document Operations
    - Index Strategies
    - Transaction Management

## Implementation Guidelines

For each technology:
1. Installation and Setup
2. Basic Configuration
3. Cluster Setup
4. Spring Boot Integration
5. Multi-source Management
6. Common Operations
7. Best Practices
8. Monitoring and Maintenance

Would you like me to start with any particular section in detail?I'll start by detailing the MySQL configuration section following our outline.



I've completed the comprehensive MySQL configuration guide. Let's move on to Redis configuration. Would you like me to proceed with the Redis section, which will cover:

1. Standalone Redis setup
2. Redis Cluster configuration
3. Redis Sentinel setup
4. Spring Boot integration
5. Connection pool management
6. Common Redis operations and patterns
7. High availability considerations

Would you like me to continue with the Redis configuration section?# MySQL Configuration Guide with Spring Boot 3.2.9

## 1. Single Instance Setup

### 1.1 Docker Installation
```bash
# Pull MySQL 8.0 image
docker pull mysql:8.0

# Create and run MySQL container
docker run --name mysql8 -p 3306:3306 \
  -e MYSQL_ROOT_PASSWORD=root123 \
  -v mysql_data:/var/lib/mysql \
  -d mysql:8.0
```

### 1.2 Basic Configuration
Key configuration file (my.cnf):
```ini
[mysqld]
# Basic Settings
server-id=1
port=3306
user=mysql
default_authentication_plugin=mysql_native_password

# Character Set
character-set-server=utf8mb4
collation-server=utf8mb4_general_ci

# Connection Settings
max_connections=1000
max_connect_errors=1000

# Buffer Settings
innodb_buffer_pool_size=1G
innodb_log_buffer_size=16M

# Log Settings
slow_query_log=1
slow_query_log_file=/var/log/mysql/slow.log
long_query_time=2
```

## 2. Master-Slave Cluster Setup

### 2.1 Master Configuration
```ini
[mysqld]
server-id=1
log-bin=mysql-bin
binlog_format=ROW
sync_binlog=1
expire_logs_days=7

# Replication settings
gtid_mode=ON
enforce_gtid_consistency=ON
```

### 2.2 Slave Configuration
```ini
[mysqld]
server-id=2
relay-log=slave-relay-bin
read_only=1

# Replication settings
gtid_mode=ON
enforce_gtid_consistency=ON
```

### 2.3 Replication Setup
```sql
-- On Master
CREATE USER 'repl'@'%' IDENTIFIED BY 'repl123';
GRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';

-- On Slave
CHANGE MASTER TO
    MASTER_HOST='master_host',
    MASTER_USER='repl',
    MASTER_PASSWORD='repl123',
    MASTER_AUTO_POSITION=1;
START SLAVE;
```

## 3. Spring Boot Configuration

### 3.1 Basic Dependencies (pom.xml)
```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-jpa</artifactId>
    </dependency>
    
    <!-- MySQL Connector -->
    <dependency>
        <groupId>com.mysql</groupId>
        <artifactId>mysql-connector-j</artifactId>
        <version>8.0.33</version>
    </dependency>
    
    <!-- Connection Pool -->
    <dependency>
        <groupId>com.zaxxer</groupId>
        <artifactId>HikariCP</artifactId>
        <version>5.0.1</version>
    </dependency>
</dependencies>
```

### 3.2 Single DataSource Configuration
```java
@Configuration
public class MySQLConfig {
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.hikari")
    public HikariConfig hikariConfig() {
        return new HikariConfig();
    }
    
    @Bean
    @Primary
    public DataSource dataSource() {
        return new HikariDataSource(hikariConfig());
    }
    
    @Bean
    public JdbcTemplate jdbcTemplate(DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }
}
```

application.yml:
```yaml
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/mydb?useSSL=false&serverTimezone=UTC
    username: root
    password: root123
    driver-class-name: com.mysql.cj.jdbc.Driver
    hikari:
      pool-name: HikariPool-1
      maximum-pool-size: 10
      minimum-idle: 5
      idle-timeout: 300000
      connection-timeout: 20000
      connection-test-query: SELECT 1
```

## 4. Multi-source Configuration

### 4.1 Configuration Classes
```java
@Configuration
public class MultiDataSourceConfig {
    
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.master")
    public DataSourceProperties masterDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    @ConfigurationProperties(prefix = "spring.datasource.slave")
    public DataSourceProperties slaveDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.master.hikari")
    public HikariConfig masterHikariConfig() {
        return new HikariConfig();
    }
    
    @Bean
    @ConfigurationProperties(prefix = "spring.datasource.slave.hikari")
    public HikariConfig slaveHikariConfig() {
        return new HikariConfig();
    }
    
    @Bean(name = "masterDataSource")
    @Primary
    public DataSource masterDataSource() {
        return new HikariDataSource(masterHikariConfig());
    }
    
    @Bean(name = "slaveDataSource")
    public DataSource slaveDataSource() {
        return new HikariDataSource(slaveHikariConfig());
    }
}
```

### 4.2 Dynamic DataSource Implementation
```java
public class DynamicDataSourceContextHolder {
    private static final ThreadLocal<String> contextHolder = new ThreadLocal<>();
    
    public static void setDataSourceType(String dataSourceType) {
        contextHolder.set(dataSourceType);
    }
    
    public static String getDataSourceType() {
        return contextHolder.get();
    }
    
    public static void clearDataSourceType() {
        contextHolder.remove();
    }
}

public class DynamicDataSource extends AbstractRoutingDataSource {
    @Override
    protected Object determineCurrentLookupKey() {
        return DynamicDataSourceContextHolder.getDataSourceType();
    }
}

@Configuration
public class DynamicDataSourceConfig {
    
    @Resource(name = "masterDataSource")
    private DataSource masterDataSource;
    
    @Resource(name = "slaveDataSource")
    private DataSource slaveDataSource;
    
    @Bean
    @Primary
    public DynamicDataSource dynamicDataSource() {
        Map<Object, Object> targetDataSources = new HashMap<>();
        targetDataSources.put("master", masterDataSource);
        targetDataSources.put("slave", slaveDataSource);
        
        DynamicDataSource dataSource = new DynamicDataSource();
        dataSource.setDefaultTargetDataSource(masterDataSource);
        dataSource.setTargetDataSources(targetDataSources);
        return dataSource;
    }
}
```

### 4.3 DataSource Switching Annotation
```java
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface DataSource {
    String value() default "master";
}

@Aspect
@Component
public class DataSourceAspect {
    
    @Pointcut("@annotation(com.example.annotation.DataSource)")
    public void dataSourcePointcut() {}
    
    @Around("dataSourcePointcut()")
    public Object around(ProceedingJoinPoint point) throws Throwable {
        MethodSignature signature = (MethodSignature) point.getSignature();
        Method method = signature.getMethod();
        
        DataSource ds = method.getAnnotation(DataSource.class);
        if (ds != null) {
            DynamicDataSourceContextHolder.setDataSourceType(ds.value());
        }
        
        try {
            return point.proceed();
        } finally {
            DynamicDataSourceContextHolder.clearDataSourceType();
        }
    }
}
```

### 4.4 Usage Example
```java
@Service
public class UserService {
    
    @Resource
    private JdbcTemplate jdbcTemplate;
    
    @DataSource("master")
    public void createUser(User user) {
        jdbcTemplate.update("INSERT INTO users (name, email) VALUES (?, ?)",
            user.getName(), user.getEmail());
    }
    
    @DataSource("slave")
    public List<User> getAllUsers() {
        return jdbcTemplate.query("SELECT * FROM users",
            (rs, rowNum) -> new User(rs.getLong("id"), 
                                   rs.getString("name"),
                                   rs.getString("email")));
    }
}
```

## 5. Transaction Management

### 5.1 Transaction Configuration
```java
@Configuration
@EnableTransactionManagement
public class TransactionConfig {
    
    @Resource
    private DynamicDataSource dynamicDataSource;
    
    @Bean
    public PlatformTransactionManager transactionManager() {
        return new DataSourceTransactionManager(dynamicDataSource);
    }
    
    @Bean
    public TransactionTemplate transactionTemplate() {
        return new TransactionTemplate(transactionManager());
    }
}
```

### 5.2 Transaction Usage
```java
@Service
@Transactional(rollbackFor = Exception.class)
public class TransactionalService {
    
    @Resource
    private JdbcTemplate jdbcTemplate;
    @Resource
    private TransactionTemplate transactionTemplate;
    
    @Transactional(readOnly = true)
    public User getUser(Long id) {
        return jdbcTemplate.queryForObject(
            "SELECT * FROM users WHERE id = ?",
            new Object[]{id},
            (rs, rowNum) -> new User(
                rs.getLong("id"),
                rs.getString("name"),
                rs.getString("email")
            ));
    }
    
    public void updateUserWithTemplate(User user) {
        transactionTemplate.execute(status -> {
            try {
                jdbcTemplate.update(
                    "UPDATE users SET name = ?, email = ? WHERE id = ?",
                    user.getName(), user.getEmail(), user.getId());
                return null;
            } catch (Exception e) {
                status.setRollbackOnly();
                throw e;
            }
        });
    }
}
```

## 6. Best Practices

### 6.1 Connection Pool Configuration
```yaml
spring:
  datasource:
    hikari:
      # Basic Settings
      maximum-pool-size: 10
      minimum-idle: 5
      idle-timeout: 300000
      connection-timeout: 20000
      max-lifetime: 1200000
      
      # Performance Settings
      auto-commit: true
      connection-test-query: SELECT 1
      validation-timeout: 5000
      leak-detection-threshold: 60000
      
      # Reliability Settings
      register-mbeans: true
      pool-name: HikariPool-1
```

### 6.2 Exception Handling
```java
@ControllerAdvice
public class GlobalExceptionHandler {
    
    @ExceptionHandler(DataAccessException.class)
    public ResponseEntity<String> handleDataAccessException(DataAccessException e) {
        // Log the error
        log.error("Database error occurred", e);
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
            .body("Database operation failed");
    }
    
    @ExceptionHandler(TransactionException.class)
    public ResponseEntity<String> handleTransactionException(TransactionException e) {
        log.error("Transaction error occurred", e);
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
            .body("Transaction failed");
    }
}
```

### 6.3 Monitoring and Metrics
```java
@Configuration
public class DatabaseMonitoringConfig {
    
    @Bean
    public MetricRegistry metricRegistry() {
        return new MetricRegistry();
    }
    
    @Bean
    public DataSourceHealthIndicator dataSourceHealthIndicator(
            DataSource dataSource) {
        return new DataSourceHealthIndicator(dataSource, "SELECT 1");
    }
}
```

### 6.4 SQL Performance Optimization
```java
@Repository
public class OptimizedUserRepository {
    
    @Resource
    private JdbcTemplate jdbcTemplate;
    
    // Use batch operations for bulk inserts
    public void batchInsertUsers(List<User> users) {
        jdbcTemplate.batchUpdate(
            "INSERT INTO users (name, email) VALUES (?, ?)",
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                    throws SQLException {
                    User user = users.get(i);
                    ps.setString(1, user.getName());
                    ps.setString(2, user.getEmail());
                }
                
                @Override
                public int getBatchSize() {
                    return users.size();
                }
            });
    }
    
    // Use pagination for large result sets
    public Page<User> getUsersWithPagination(Pageable pageable) {
        String sql = "SELECT * FROM users LIMIT ? OFFSET ?";
        List<User> users = jdbcTemplate.query(
            sql,
            new Object[]{pageable.getPageSize(), 
                        pageable.getOffset()},
            (rs, rowNum) -> new User(
                rs.getLong("id"),
                rs.getString("name"),
                rs.getString("email")
            ));
            
        sql = "SELECT COUNT(*) FROM users";
        Integer total = jdbcTemplate.queryForObject(
            sql, Integer.class);
            
        return new PageImpl<>(users, pageable, total);
    }
}
```

## 7. Production Deployment Considerations

### 7.1 High Availability Setup
- Implement master-slave replication
- Configure connection timeouts
- Set up connection pooling
- Enable monitoring and alerting
- Regular backup scheduling

### 7.2 Security Best Practices
- Use encrypted connections (SSL/TLS)
- Implement proper user permissions
- Regular security audits
- Password rotation policies
- Network security configuration

### 7.3 Performance Optimization
- Index optimization
- Query optimization
- Connection pool tuning
- Regular maintenance
- Performance monitoring

This completes the MySQL configuration section. Would you like me to continue with the Redis configuration next?I'll proceed with the Redis configuration section, following the same comprehensive approach we used for MySQL.



I've completed the Redis configuration guide with detailed explanations of standalone, cluster, and sentinel setups, along with Spring Boot integration and best practices. Next, I'll cover RabbitMQ configuration, which will include:

1. Single node RabbitMQ setup
2. RabbitMQ cluster configuration
3. High availability setup with mirrored queues
4. Spring Boot AMQP integration
5. Exchange and queue management
6. Message reliability and transactions
7. Monitoring and maintenance

Would you like me to proceed with the RabbitMQ configuration section?# Redis Configuration Guide with Spring Boot 3.2.9

## 1. Standalone Redis Setup

### 1.1 Docker Installation
Redis can be quickly deployed using Docker for development environments. Here's how to set up a single Redis instance:

```bash
# Pull the Redis 7.x image
docker pull redis:7.2

# Run Redis container with persistence enabled
docker run --name redis7 \
  -p 6379:6379 \
  -v redis_data:/data \
  -d redis:7.2 redis-server \
  --appendonly yes \
  --requirepass yourpassword
```

### 1.2 Basic Redis Configuration
The redis.conf file is the heart of Redis configuration. Here are the essential settings for a production environment:

```conf
# Network
bind 0.0.0.0
port 6379
protected-mode yes
requirepass yourpassword

# Persistence
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec

# Memory Management
maxmemory 2gb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# Connection Settings
timeout 0
tcp-keepalive 300
databases 16

# Logging
loglevel notice
logfile "/var/log/redis/redis.log"
```

## 2. Redis Cluster Setup

### 2.1 Cluster Configuration
To set up a Redis cluster, we need at least three master nodes and three replica nodes. Here's a basic cluster configuration:

```conf
# Cluster Configuration for Each Node
port 6379
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes

# Node Specific Settings
# For node 1
port 6379
cluster-announce-ip 192.168.1.101
cluster-announce-port 6379

# For node 2
port 6379
cluster-announce-ip 192.168.1.102
cluster-announce-port 6379

# Continue for other nodes...
```

### 2.2 Cluster Creation Script
```bash
#!/bin/bash
# Create Redis cluster with 3 masters and 3 replicas
redis-cli --cluster create \
  192.168.1.101:6379 \
  192.168.1.102:6379 \
  192.168.1.103:6379 \
  192.168.1.104:6379 \
  192.168.1.105:6379 \
  192.168.1.106:6379 \
  --cluster-replicas 1 \
  --cluster-yes
```

## 3. Redis Sentinel Configuration

### 3.1 Sentinel Setup
Sentinel provides high availability for Redis. Here's a basic sentinel configuration:

```conf
# sentinel.conf
port 26379
sentinel monitor mymaster 192.168.1.100 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 60000
sentinel parallel-syncs mymaster 1
sentinel auth-pass mymaster yourpassword
```

### 3.2 Docker Compose for Sentinel Setup
```yaml
version: '3.8'
services:
  redis-master:
    image: redis:7.2
    command: redis-server --requirepass yourpassword
    ports:
      - "6379:6379"
      
  redis-slave1:
    image: redis:7.2
    command: redis-server --slaveof redis-master 6379 --requirepass yourpassword
    depends_on:
      - redis-master
      
  redis-slave2:
    image: redis:7.2
    command: redis-server --slaveof redis-master 6379 --requirepass yourpassword
    depends_on:
      - redis-master
      
  sentinel1:
    image: redis:7.2
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    volumes:
      - ./sentinel.conf:/usr/local/etc/redis/sentinel.conf
      
  sentinel2:
    image: redis:7.2
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    volumes:
      - ./sentinel.conf:/usr/local/etc/redis/sentinel.conf
```

## 4. Spring Boot Integration

### 4.1 Dependencies Configuration
Add the following dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Redis Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-redis</artifactId>
    </dependency>
    
    <!-- Apache Commons Pool for connection pooling -->
    <dependency>
        <groupId>org.apache.commons</groupId>
        <artifactId>commons-pool2</artifactId>
    </dependency>
</dependencies>
```

### 4.2 Single Redis Configuration
```java
@Configuration
public class RedisConfig {
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Configure serializers
        Jackson2JsonRedisSerializer<Object> jsonSerializer = new Jackson2JsonRedisSerializer<>(Object.class);
        StringRedisSerializer stringSerializer = new StringRedisSerializer();
        
        // Configure object mapper for JSON serialization
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        objectMapper.activateDefaultTyping(LaissezFaireSubTypeValidator.instance, 
            ObjectMapper.DefaultTyping.NON_FINAL);
        jsonSerializer.setObjectMapper(objectMapper);
        
        // Set serializers
        template.setKeySerializer(stringSerializer);
        template.setValueSerializer(jsonSerializer);
        template.setHashKeySerializer(stringSerializer);
        template.setHashValueSerializer(jsonSerializer);
        template.afterPropertiesSet();
        
        return template;
    }
    
    @Bean
    public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory connectionFactory) {
        StringRedisTemplate template = new StringRedisTemplate();
        template.setConnectionFactory(connectionFactory);
        return template;
    }
}
```

### 4.3 Redis Cluster Configuration
```java
@Configuration
public class RedisClusterConfig {
    
    @Bean
    public RedisClusterConfiguration redisClusterConfiguration() {
        RedisClusterConfiguration config = new RedisClusterConfiguration();
        Set<RedisNode> nodes = new HashSet<>();
        // Add cluster nodes
        nodes.add(new RedisNode("192.168.1.101", 6379));
        nodes.add(new RedisNode("192.168.1.102", 6379));
        nodes.add(new RedisNode("192.168.1.103", 6379));
        config.setClusterNodes(nodes);
        config.setMaxRedirects(3);
        return config;
    }
    
    @Bean
    public LettuceConnectionFactory connectionFactory(RedisClusterConfiguration clusterConfig) {
        LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder()
            .clientOptions(ClientOptions.builder()
                .socketOptions(SocketOptions.builder()
                    .connectTimeout(Duration.ofSeconds(10))
                    .build())
                .build())
            .commandTimeout(Duration.ofSeconds(5))
            .build();
        
        return new LettuceConnectionFactory(clusterConfig, clientConfig);
    }
}
```

### 4.4 Redis Sentinel Configuration
```java
@Configuration
public class RedisSentinelConfig {
    
    @Bean
    public RedisSentinelConfiguration sentinelConfiguration() {
        RedisSentinelConfiguration config = new RedisSentinelConfiguration()
            .master("mymaster")
            .sentinel("192.168.1.101", 26379)
            .sentinel("192.168.1.102", 26379)
            .sentinel("192.168.1.103", 26379);
        config.setPassword("yourpassword");
        return config;
    }
    
    @Bean
    public LettuceConnectionFactory connectionFactory(RedisSentinelConfiguration sentinelConfig) {
        return new LettuceConnectionFactory(sentinelConfig);
    }
}
```

## 5. Advanced Redis Operations

### 5.1 Redis Cache Service
```java
@Service
@Slf4j
public class RedisCacheService {
    
    private final RedisTemplate<String, Object> redisTemplate;
    private final StringRedisTemplate stringRedisTemplate;
    
    public RedisCacheService(RedisTemplate<String, Object> redisTemplate,
                           StringRedisTemplate stringRedisTemplate) {
        this.redisTemplate = redisTemplate;
        this.stringRedisTemplate = stringRedisTemplate;
    }
    
    /**
     * Cache data with expiration
     */
    public void setCacheWithExpire(String key, Object value, long timeout, TimeUnit unit) {
        try {
            redisTemplate.opsForValue().set(key, value, timeout, unit);
        } catch (Exception e) {
            log.error("Error setting cache with expire", e);
            throw new CacheException("Failed to set cache", e);
        }
    }
    
    /**
     * Get cached data with type conversion
     */
    public <T> T getCacheData(String key, Class<T> clazz) {
        try {
            Object value = redisTemplate.opsForValue().get(key);
            if (value == null) {
                return null;
            }
            return convertValue(value, clazz);
        } catch (Exception e) {
            log.error("Error getting cache data", e);
            throw new CacheException("Failed to get cache", e);
        }
    }
    
    /**
     * Implement distributed lock
     */
    public boolean lock(String key, String value, long timeout, TimeUnit unit) {
        try {
            return stringRedisTemplate.opsForValue()
                .setIfAbsent(key, value, timeout, unit);
        } catch (Exception e) {
            log.error("Error acquiring lock", e);
            return false;
        }
    }
    
    /**
     * Release distributed lock
     */
    public boolean unlock(String key, String value) {
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then " +
                       "return redis.call('del', KEYS[1]) else return 0 end";
        try {
            DefaultRedisScript<Long> redisScript = new DefaultRedisScript<>();
            redisScript.setScriptText(script);
            redisScript.setResultType(Long.class);
            Long result = stringRedisTemplate.execute(redisScript, 
                Collections.singletonList(key), value);
            return result != null && result == 1L;
        } catch (Exception e) {
            log.error("Error releasing lock", e);
            return false;
        }
    }
}
```

### 5.2 Redis Cache Manager
```java
@Configuration
@EnableCaching
public class RedisCacheManagerConfig {
    
    @Bean
    public RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) {
        RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()
            .entryTtl(Duration.ofMinutes(30))
            .serializeKeysWith(RedisSerializationContext.SerializationPair
                .fromSerializer(new StringRedisSerializer()))
            .serializeValuesWith(RedisSerializationContext.SerializationPair
                .fromSerializer(new GenericJackson2JsonRedisSerializer()))
            .disableCachingNullValues();
            
        // Configure different TTL for specific caches
        Map<String, RedisCacheConfiguration> configMap = new HashMap<>();
        configMap.put("userCache", config.entryTtl(Duration.ofMinutes(10)));
        configMap.put("productCache", config.entryTtl(Duration.ofHours(1)));
        
        return RedisCacheManager.builder(connectionFactory)
            .cacheDefaults(config)
            .withInitialCacheConfigurations(configMap)
            .build();
    }
}
```

## 6. Redis Best Practices

### 6.1 Key Design Guidelines
```java
public class RedisKeyGenerator {
    
    private static final String SEPARATOR = ":";
    
    public static String generateKey(String business, String module, String function, String id) {
        return String.join(SEPARATOR, business, module, function, id);
    }
    
    // Example usage:
    // user:profile:basic:123
    // order:payment:status:456
    // product:inventory:stock:789
}
```

### 6.2 Exception Handling
```java
@ControllerAdvice
public class RedisCacheExceptionHandler {
    
    @ExceptionHandler(RedisConnectionFailureException.class)
    public ResponseEntity<String> handleRedisConnectionFailure(RedisConnectionFailureException e) {
        log.error("Redis connection failed", e);
        return ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)
            .body("Cache service temporarily unavailable");
    }
    
    @ExceptionHandler(CacheException.class)
    public ResponseEntity<String> handleCacheException(CacheException e) {
        log.error("Cache operation failed", e);
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
            .body("Cache operation failed");
    }
}
```

### 6.3 Monitoring and Metrics
```java
@Configuration
public class RedisMonitoringConfig {
    
    @Bean
    public RedisHealthIndicator redisHealthIndicator(RedisConnectionFactory connectionFactory) {
        return new RedisHealthIndicator(connectionFactory);
    }
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public RedisMetricsCollector redisMetricsCollector(MeterRegistry registry) {
        return new RedisMetricsCollector(registry);
    }
}
```

## 7. Production Deployment Guidelines

### 7.1 Memory Management
- Set appropriate maxmemory based on available system resources
- Choose suitable eviction policies (allkeys-lru for cache, volatile-lru for session storage)
- Monitor memory usage and implement alerts
- Regularly clean up expired keys

### 7.2 Security Considerations
- Enable protected mode
- Use strong passwords
- Configure SSL/TLS for encryption
- Implement network security (firewall rules, security groups)
- Regular security audits

### 7.3 Backup Strategy
- Configure AOF persistence for data durability
- Regular RDB snapshots
- Implement backup rotation
- Test recovery procedures

### 7.4 Performance Optimization
- Use pipelining for batch operations
- Implement proper key expiration
- Monitor slow logs
- Use appropriate data structures
- Regular performance audits

This concludes the Redis configuration section. Would you like me to continue with the RabbitMQ configuration next?# RabbitMQ Configuration Guide with Spring Boot 3.2.9

## 1. Single Node Installation and Setup

### 1.1 Docker-based Installation

Let's start with a Docker-based installation, which provides a clean and isolated environment for RabbitMQ. The following command sets up a RabbitMQ instance with the management plugin enabled:

```bash
# Pull the official RabbitMQ image with management plugin
docker pull rabbitmq:3.12-management

# Run RabbitMQ container with management console
docker run -d --name rabbitmq \
    -p 5672:5672 \
    -p 15672:15672 \
    -e RABBITMQ_DEFAULT_USER=admin \
    -e RABBITMQ_DEFAULT_PASS=admin123 \
    --hostname rabbit1 \
    rabbitmq:3.12-management
```

The command above sets up RabbitMQ with the following configurations:
- Port 5672: AMQP protocol port for client connections
- Port 15672: HTTP port for management UI access
- Default credentials: admin/admin123

### 1.2 Basic Configuration

RabbitMQ's configuration can be customized through the rabbitmq.conf file. Here's a basic configuration that sets up essential parameters:

```conf
# Network and TCP Settings
listeners.tcp.default = 5672
management.listener.port = 15672
management.listener.ssl = false

# Memory and Disk Settings
vm_memory_high_watermark.relative = 0.7
disk_free_limit.relative = 2.0

# Authentication and Authorization
default_user = admin
default_pass = admin123
default_vhost = /

# Resource Limits
channel_max = 2048
max_connections = 1000

# Logging
log.file.level = info
log.console = true
```

## 2. Cluster Configuration

### 2.1 Multi-node Cluster Setup

Setting up a RabbitMQ cluster involves configuring multiple nodes to work together. Here's a Docker Compose configuration for a three-node cluster:

```yaml
version: '3.8'
services:
  rabbit1:
    image: rabbitmq:3.12-management
    hostname: rabbit1
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5672:5672"
      - "15672:15672"

  rabbit2:
    image: rabbitmq:3.12-management
    hostname: rabbit2
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5673:5672"
      - "15673:15672"
    command: |
      bash -c "rabbitmq-server & 
               sleep 20s && 
               rabbitmqctl stop_app &&
               rabbitmqctl reset &&
               rabbitmqctl join_cluster rabbit@rabbit1 &&
               rabbitmqctl start_app &&
               tail -f /dev/null"

  rabbit3:
    image: rabbitmq:3.12-management
    hostname: rabbit3
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5674:5672"
      - "15674:15672"
    command: |
      bash -c "rabbitmq-server &
               sleep 25s &&
               rabbitmqctl stop_app &&
               rabbitmqctl reset &&
               rabbitmqctl join_cluster rabbit@rabbit1 &&
               rabbitmqctl start_app &&
               tail -f /dev/null"
```

### 2.2 High Availability Configuration

To ensure message durability and high availability, we need to configure queue mirroring. Here's how to set up queue mirroring policies:

```bash
# Set ha-mode to all for all queues
rabbitmqctl set_policy ha-all ".*" \
    '{"ha-mode":"all", "ha-sync-mode":"automatic"}' \
    --priority 1 \
    --apply-to queues
```

## 3. Spring Boot Integration

### 3.1 Dependencies Configuration

First, add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot AMQP Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
    </dependency>
    
    <!-- For JSON message conversion -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

### 3.2 RabbitMQ Configuration

Let's create a comprehensive RabbitMQ configuration class that sets up exchanges, queues, and bindings:

```java
@Configuration
public class RabbitMQConfig {
    
    // Exchange names
    public static final String DIRECT_EXCHANGE = "direct.exchange";
    public static final String TOPIC_EXCHANGE = "topic.exchange";
    public static final String FANOUT_EXCHANGE = "fanout.exchange";
    
    // Queue names
    public static final String DIRECT_QUEUE = "direct.queue";
    public static final String TOPIC_QUEUE = "topic.queue";
    public static final String FANOUT_QUEUE = "fanout.queue";
    
    @Bean
    public ConnectionFactory connectionFactory() {
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        connectionFactory.setAddresses("localhost:5672,localhost:5673,localhost:5674");
        connectionFactory.setUsername("admin");
        connectionFactory.setPassword("admin123");
        connectionFactory.setVirtualHost("/");
        
        // Enable publisher confirms and returns
        connectionFactory.setPublisherConfirmType(CachingConnectionFactory.ConfirmType.CORRELATED);
        connectionFactory.setPublisherReturns(true);
        
        return connectionFactory;
    }
    
    @Bean
    public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate template = new RabbitTemplate(connectionFactory);
        template.setMessageConverter(new Jackson2JsonMessageConverter());
        
        // Configure mandatory flag for publisher returns
        template.setMandatory(true);
        
        // Configure callbacks for publisher confirms and returns
        template.setConfirmCallback((correlation, ack, reason) -> {
            if (!ack) {
                log.error("Message publish failed: {}", reason);
            }
        });
        
        template.setReturnsCallback(returned -> {
            log.error("Message returned: {}", returned.getMessage());
        });
        
        return template;
    }
    
    @Bean
    public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) {
        return new RabbitAdmin(connectionFactory);
    }
    
    // Exchange Declarations
    @Bean
    public DirectExchange directExchange() {
        return ExchangeBuilder.directExchange(DIRECT_EXCHANGE)
            .durable(true)
            .build();
    }
    
    @Bean
    public TopicExchange topicExchange() {
        return ExchangeBuilder.topicExchange(TOPIC_EXCHANGE)
            .durable(true)
            .build();
    }
    
    @Bean
    public FanoutExchange fanoutExchange() {
        return ExchangeBuilder.fanoutExchange(FANOUT_EXCHANGE)
            .durable(true)
            .build();
    }
    
    // Queue Declarations
    @Bean
    public Queue directQueue() {
        return QueueBuilder.durable(DIRECT_QUEUE)
            .withArgument("x-dead-letter-exchange", "dlx.exchange")
            .withArgument("x-dead-letter-routing-key", "dlx.routing.key")
            .withArgument("x-message-ttl", 60000) // 1 minute TTL
            .build();
    }
    
    @Bean
    public Queue topicQueue() {
        return QueueBuilder.durable(TOPIC_QUEUE)
            .withArgument("x-max-length", 1000)
            .withArgument("x-overflow", "reject-publish")
            .build();
    }
    
    @Bean
    public Queue fanoutQueue() {
        return QueueBuilder.durable(FANOUT_QUEUE)
            .build();
    }
    
    // Binding Declarations
    @Bean
    public Binding directBinding() {
        return BindingBuilder
            .bind(directQueue())
            .to(directExchange())
            .with("direct.routing.key");
    }
    
    @Bean
    public Binding topicBinding() {
        return BindingBuilder
            .bind(topicQueue())
            .to(topicExchange())
            .with("topic.#");
    }
    
    @Bean
    public Binding fanoutBinding() {
        return BindingBuilder
            .bind(fanoutQueue())
            .to(fanoutExchange());
    }
}
```

### 3.3 Message Producer Service

Here's a service class that demonstrates different message publishing patterns:

```java
@Service
@Slf4j
public class RabbitMQProducer {
    
    private final RabbitTemplate rabbitTemplate;
    private final ObjectMapper objectMapper;
    
    public RabbitMQProducer(RabbitTemplate rabbitTemplate, ObjectMapper objectMapper) {
        this.rabbitTemplate = rabbitTemplate;
        this.objectMapper = objectMapper;
    }
    
    public void sendDirectMessage(Object message, String routingKey) {
        try {
            MessageProperties properties = new MessageProperties();
            properties.setDeliveryMode(MessageDeliveryMode.PERSISTENT);
            properties.setContentType(MessageProperties.CONTENT_TYPE_JSON);
            
            Message amqpMessage = new Message(
                objectMapper.writeValueAsBytes(message),
                properties
            );
            
            CorrelationData correlationData = new CorrelationData(
                UUID.randomUUID().toString());
            
            rabbitTemplate.send(
                RabbitMQConfig.DIRECT_EXCHANGE,
                routingKey,
                amqpMessage,
                correlationData
            );
        } catch (Exception e) {
            log.error("Error sending direct message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
    
    public void sendTopicMessage(Object message, String routingKey) {
        try {
            rabbitTemplate.convertAndSend(
                RabbitMQConfig.TOPIC_EXCHANGE,
                routingKey,
                message,
                msg -> {
                    MessageProperties props = msg.getMessageProperties();
                    props.setDeliveryMode(MessageDeliveryMode.PERSISTENT);
                    props.setContentType(MessageProperties.CONTENT_TYPE_JSON);
                    return msg;
                }
            );
        } catch (Exception e) {
            log.error("Error sending topic message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
    
    public void sendFanoutMessage(Object message) {
        try {
            rabbitTemplate.convertAndSend(
                RabbitMQConfig.FANOUT_EXCHANGE,
                "",
                message
            );
        } catch (Exception e) {
            log.error("Error sending fanout message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
}
```

### 3.4 Message Consumer Service

Here's a service class that demonstrates message consumption patterns:

```java
@Service
@Slf4j
public class RabbitMQConsumer {
    
    @RabbitListener(queues = RabbitMQConfig.DIRECT_QUEUE)
    public void handleDirectMessage(Message message) {
        try {
            log.info("Received direct message: {}", 
                new String(message.getBody()));
            
            // Process message
            processMessage(message);
            
        } catch (Exception e) {
            // Handle exception and potentially retry or move to DLQ
            handleMessageError(message, e);
        }
    }
    
    @RabbitListener(queues = RabbitMQConfig.TOPIC_QUEUE)
    public void handleTopicMessage(
            @Payload Object payload,
            @Header(AmqpHeaders.RECEIVED_ROUTING_KEY) String routingKey) {
        try {
            log.info("Received topic message with routing key {}: {}", 
                routingKey, payload);
                
            // Process message based on routing key
            processTopicMessage(payload, routingKey);
            
        } catch (Exception e) {
            log.error("Error processing topic message", e);
            throw new AmqpRejectAndDontRequeueException(e);
        }
    }
    
    @RabbitListener(queues = RabbitMQConfig.FANOUT_QUEUE)
    public void handleFanoutMessage(
            @Payload Object payload,
            @Headers Map<String, Object> headers,
            Channel channel,
            @Header(AmqpHeaders.DELIVERY_TAG) long tag) {
        try {
            log.info("Received fanout message: {}", payload);
            
            // Process message
            processFanoutMessage(payload);
            
            // Manual acknowledgment
            channel.basicAck(tag, false);
            
        } catch (Exception e) {
            try {
                // Negative acknowledgment and requeue
                channel.basicNack(tag, false, true);
            } catch (IOException ex) {
                log.error("Error during message nack", ex);
            }
        }
    }
    
    private void processMessage(Message message) {
        // Implement message processing logic
    }
    
    private void processTopicMessage(Object payload, String routingKey) {
        // Implement topic-specific processing logic
    }
    
    private void processFanoutMessage(Object payload) {
        // Implement fanout message processing logic
    }
    
    private void handleMessageError(Message message, Exception e) {
        log.error("Error processing message", e);
        // Implement error handling logic
    }
}
```

## 4. Error Handling and Retry Mechanism

### 4.1 Retry Configuration

Here's how to configure retry policies for message processing:

```java
@Configuration
public class RabbitRetryConfig {
    
    @Bean
    public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(
            ConnectionFactory connectionFactory) {
        SimpleRabbitListenerContainerFactory factory = 
            new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        
        // Configure retry policy
        factory.setAdviceChain(RetryInterceptorBuilder
            .stateless()
            .maxAttempts(3)
            .backOffOptions(1000, 2.0, 10000)
            .recoverer(new RejectAndDontRequeueRecoverer())
            .build());
            
        return factory;
    }
    
    @Bean
    public MessageRecoverer messageRecoverer(RabbitTemplate rabbitTemplate) {
        return new Republ# RabbitMQ Configuration Guide with Spring Boot 3.2.9

## 1. Single Node Installation and Setup

### 1.1 Docker-based Installation

Let's start with a Docker-based installation, which provides a clean and isolated environment for RabbitMQ. The following command sets up a RabbitMQ instance with the management plugin enabled:

```bash
# Pull the official RabbitMQ image with management plugin
docker pull rabbitmq:3.12-management

# Run RabbitMQ container with management console
docker run -d --name rabbitmq \
    -p 5672:5672 \
    -p 15672:15672 \
    -e RABBITMQ_DEFAULT_USER=admin \
    -e RABBITMQ_DEFAULT_PASS=admin123 \
    --hostname rabbit1 \
    rabbitmq:3.12-management
```

The command above sets up RabbitMQ with the following configurations:
- Port 5672: AMQP protocol port for client connections
- Port 15672: HTTP port for management UI access
- Default credentials: admin/admin123

### 1.2 Basic Configuration

RabbitMQ's configuration can be customized through the rabbitmq.conf file. Here's a basic configuration that sets up essential parameters:

```conf
# Network and TCP Settings
listeners.tcp.default = 5672
management.listener.port = 15672
management.listener.ssl = false

# Memory and Disk Settings
vm_memory_high_watermark.relative = 0.7
disk_free_limit.relative = 2.0

# Authentication and Authorization
default_user = admin
default_pass = admin123
default_vhost = /

# Resource Limits
channel_max = 2048
max_connections = 1000

# Logging
log.file.level = info
log.console = true
```

## 2. Cluster Configuration

### 2.1 Multi-node Cluster Setup

Setting up a RabbitMQ cluster involves configuring multiple nodes to work together. Here's a Docker Compose configuration for a three-node cluster:

```yaml
version: '3.8'
services:
  rabbit1:
    image: rabbitmq:3.12-management
    hostname: rabbit1
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5672:5672"
      - "15672:15672"

  rabbit2:
    image: rabbitmq:3.12-management
    hostname: rabbit2
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5673:5672"
      - "15673:15672"
    command: |
      bash -c "rabbitmq-server & 
               sleep 20s && 
               rabbitmqctl stop_app &&
               rabbitmqctl reset &&
               rabbitmqctl join_cluster rabbit@rabbit1 &&
               rabbitmqctl start_app &&
               tail -f /dev/null"

  rabbit3:
    image: rabbitmq:3.12-management
    hostname: rabbit3
    environment:
      - RABBITMQ_ERLANG_COOKIE=SWQOKODSQALRPCLNMEQG
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5674:5672"
      - "15674:15672"
    command: |
      bash -c "rabbitmq-server &
               sleep 25s &&
               rabbitmqctl stop_app &&
               rabbitmqctl reset &&
               rabbitmqctl join_cluster rabbit@rabbit1 &&
               rabbitmqctl start_app &&
               tail -f /dev/null"
```

### 2.2 High Availability Configuration

To ensure message durability and high availability, we need to configure queue mirroring. Here's how to set up queue mirroring policies:

```bash
# Set ha-mode to all for all queues
rabbitmqctl set_policy ha-all ".*" \
    '{"ha-mode":"all", "ha-sync-mode":"automatic"}' \
    --priority 1 \
    --apply-to queues
```

## 3. Spring Boot Integration

### 3.1 Dependencies Configuration

First, add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot AMQP Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
    </dependency>
    
    <!-- For JSON message conversion -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

### 3.2 RabbitMQ Configuration

Let's create a comprehensive RabbitMQ configuration class that sets up exchanges, queues, and bindings:

```java
@Configuration
public class RabbitMQConfig {
    
    // Exchange names
    public static final String DIRECT_EXCHANGE = "direct.exchange";
    public static final String TOPIC_EXCHANGE = "topic.exchange";
    public static final String FANOUT_EXCHANGE = "fanout.exchange";
    
    // Queue names
    public static final String DIRECT_QUEUE = "direct.queue";
    public static final String TOPIC_QUEUE = "topic.queue";
    public static final String FANOUT_QUEUE = "fanout.queue";
    
    @Bean
    public ConnectionFactory connectionFactory() {
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        connectionFactory.setAddresses("localhost:5672,localhost:5673,localhost:5674");
        connectionFactory.setUsername("admin");
        connectionFactory.setPassword("admin123");
        connectionFactory.setVirtualHost("/");
        
        // Enable publisher confirms and returns
        connectionFactory.setPublisherConfirmType(CachingConnectionFactory.ConfirmType.CORRELATED);
        connectionFactory.setPublisherReturns(true);
        
        return connectionFactory;
    }
    
    @Bean
    public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate template = new RabbitTemplate(connectionFactory);
        template.setMessageConverter(new Jackson2JsonMessageConverter());
        
        // Configure mandatory flag for publisher returns
        template.setMandatory(true);
        
        // Configure callbacks for publisher confirms and returns
        template.setConfirmCallback((correlation, ack, reason) -> {
            if (!ack) {
                log.error("Message publish failed: {}", reason);
            }
        });
        
        template.setReturnsCallback(returned -> {
            log.error("Message returned: {}", returned.getMessage());
        });
        
        return template;
    }
    
    @Bean
    public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) {
        return new RabbitAdmin(connectionFactory);
    }
    
    // Exchange Declarations
    @Bean
    public DirectExchange directExchange() {
        return ExchangeBuilder.directExchange(DIRECT_EXCHANGE)
            .durable(true)
            .build();
    }
    
    @Bean
    public TopicExchange topicExchange() {
        return ExchangeBuilder.topicExchange(TOPIC_EXCHANGE)
            .durable(true)
            .build();
    }
    
    @Bean
    public FanoutExchange fanoutExchange() {
        return ExchangeBuilder.fanoutExchange(FANOUT_EXCHANGE)
            .durable(true)
            .build();
    }
    
    // Queue Declarations
    @Bean
    public Queue directQueue() {
        return QueueBuilder.durable(DIRECT_QUEUE)
            .withArgument("x-dead-letter-exchange", "dlx.exchange")
            .withArgument("x-dead-letter-routing-key", "dlx.routing.key")
            .withArgument("x-message-ttl", 60000) // 1 minute TTL
            .build();
    }
    
    @Bean
    public Queue topicQueue() {
        return QueueBuilder.durable(TOPIC_QUEUE)
            .withArgument("x-max-length", 1000)
            .withArgument("x-overflow", "reject-publish")
            .build();
    }
    
    @Bean
    public Queue fanoutQueue() {
        return QueueBuilder.durable(FANOUT_QUEUE)
            .build();
    }
    
    // Binding Declarations
    @Bean
    public Binding directBinding() {
        return BindingBuilder
            .bind(directQueue())
            .to(directExchange())
            .with("direct.routing.key");
    }
    
    @Bean
    public Binding topicBinding() {
        return BindingBuilder
            .bind(topicQueue())
            .to(topicExchange())
            .with("topic.#");
    }
    
    @Bean
    public Binding fanoutBinding() {
        return BindingBuilder
            .bind(fanoutQueue())
            .to(fanoutExchange());
    }
}
```

### 3.3 Message Producer Service

Here's a service class that demonstrates different message publishing patterns:

```java
@Service
@Slf4j
public class RabbitMQProducer {
    
    private final RabbitTemplate rabbitTemplate;
    private final ObjectMapper objectMapper;
    
    public RabbitMQProducer(RabbitTemplate rabbitTemplate, ObjectMapper objectMapper) {
        this.rabbitTemplate = rabbitTemplate;
        this.objectMapper = objectMapper;
    }
    
    public void sendDirectMessage(Object message, String routingKey) {
        try {
            MessageProperties properties = new MessageProperties();
            properties.setDeliveryMode(MessageDeliveryMode.PERSISTENT);
            properties.setContentType(MessageProperties.CONTENT_TYPE_JSON);
            
            Message amqpMessage = new Message(
                objectMapper.writeValueAsBytes(message),
                properties
            );
            
            CorrelationData correlationData = new CorrelationData(
                UUID.randomUUID().toString());
            
            rabbitTemplate.send(
                RabbitMQConfig.DIRECT_EXCHANGE,
                routingKey,
                amqpMessage,
                correlationData
            );
        } catch (Exception e) {
            log.error("Error sending direct message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
    
    public void sendTopicMessage(Object message, String routingKey) {
        try {
            rabbitTemplate.convertAndSend(
                RabbitMQConfig.TOPIC_EXCHANGE,
                routingKey,
                message,
                msg -> {
                    MessageProperties props = msg.getMessageProperties();
                    props.setDeliveryMode(MessageDeliveryMode.PERSISTENT);
                    props.setContentType(MessageProperties.CONTENT_TYPE_JSON);
                    return msg;
                }
            );
        } catch (Exception e) {
            log.error("Error sending topic message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
    
    public void sendFanoutMessage(Object message) {
        try {
            rabbitTemplate.convertAndSend(
                RabbitMQConfig.FANOUT_EXCHANGE,
                "",
                message
            );
        } catch (Exception e) {
            log.error("Error sending fanout message", e);
            throw new AmqpException("Failed to send message", e);
        }
    }
}
```

### 3.4 Message Consumer Service

Here's a service class that demonstrates message consumption patterns:

```java
@Service
@Slf4j
public class RabbitMQConsumer {
    
    @RabbitListener(queues = RabbitMQConfig.DIRECT_QUEUE)
    public void handleDirectMessage(Message message) {
        try {
            log.info("Received direct message: {}", 
                new String(message.getBody()));
            
            // Process message
            processMessage(message);
            
        } catch (Exception e) {
            // Handle exception and potentially retry or move to DLQ
            handleMessageError(message, e);
        }
    }
    
    @RabbitListener(queues = RabbitMQConfig.TOPIC_QUEUE)
    public void handleTopicMessage(
            @Payload Object payload,
            @Header(AmqpHeaders.RECEIVED_ROUTING_KEY) String routingKey) {
        try {
            log.info("Received topic message with routing key {}: {}", 
                routingKey, payload);
                
            // Process message based on routing key
            processTopicMessage(payload, routingKey);
            
        } catch (Exception e) {
            log.error("Error processing topic message", e);
            throw new AmqpRejectAndDontRequeueException(e);
        }
    }
    
    @RabbitListener(queues = RabbitMQConfig.FANOUT_QUEUE)
    public void handleFanoutMessage(
            @Payload Object payload,
            @Headers Map<String, Object> headers,
            Channel channel,
            @Header(AmqpHeaders.DELIVERY_TAG) long tag) {
        try {
            log.info("Received fanout message: {}", payload);
            
            // Process message
            processFanoutMessage(payload);
            
            // Manual acknowledgment
            channel.basicAck(tag, false);
            
        } catch (Exception e) {
            try {
                // Negative acknowledgment and requeue
                channel.basicNack(tag, false, true);
            } catch (IOException ex) {
                log.error("Error during message nack", ex);
            }
        }
    }
    
    private void processMessage(Message message) {
        // Implement message processing logic
    }
    
    private void processTopicMessage(Object payload, String routingKey) {
        // Implement topic-specific processing logic
    }
    
    private void processFanoutMessage(Object payload) {
        // Implement fanout message processing logic
    }
    
    private void handleMessageError(Message message, Exception e) {
        log.error("Error processing message", e);
        // Implement error handling logic
    }
}
```

## 4. Error Handling and Retry Mechanism

### 4.1 Retry Configuration

Here's how to configure retry policies for message processing:

```java
@Configuration
public class RabbitRetryConfig {
    
    @Bean
    public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(
            ConnectionFactory connectionFactory) {
        SimpleRabbitListenerContainerFactory factory = 
            new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        
        // Configure retry policy
        factory.setAdviceChain(RetryInterceptorBuilder
            .stateless()
            .maxAttempts(3)
            .backOffOptions(1000, 2.0, 10000)
            .recoverer(new RejectAndDontRequeueRecoverer())
            .build());
            
        return factory;
    }
    
    @Bean
    public MessageRecoverer messageRecoverer(RabbitTemplate rabbitTemplate) {
        return new RepublishMessageRecoverer(rabbitTemplate, 
            "dlx.exchange", "dlx.routing.key");
    }
}

### 4.2 Dead Letter Queue Configuration

Here's how to set up dead letter queues for handling failed messages:

```java
@Configuration
public class DeadLetterConfig {
    
    @Bean
    public DirectExchange deadLetterExchange() {
        return new DirectExchange("dlx.exchange");
    }
    
    @Bean
    public Queue deadLetterQueue() {
        return QueueBuilder.durable("dlx.queue")
            .withArgument("x-message-ttl", 1000 * 60 * 60 * 24) // 24 hour retention
            .build();
    }
    
    @Bean
    public Binding deadLetterBinding() {
        return BindingBuilder.bind(deadLetterQueue())
            .to(deadLetterExchange())
            .with("dlx.routing.key");
    }
    
    @RabbitListener(queues = "dlx.queue")
    public void handleDeadLetterMessage(Message failedMessage) {
        MessageProperties properties = failedMessage.getMessageProperties();
        String originalQueue = properties.getHeader("x-original-queue");
        String failureReason = properties.getHeader("x-exception-message");
        
        log.error("Processing dead letter message from queue: {}, failure: {}", 
            originalQueue, failureReason);
            
        // Implement dead letter message handling strategy
        // For example: notify support, store in database, retry later
    }
}
```

## 5. Monitoring and Management

### 5.1 Health Indicators

Let's implement custom health indicators for monitoring RabbitMQ:

```java
@Component
public class RabbitMQHealthIndicator extends AbstractHealthIndicator {
    
    private final RabbitTemplate rabbitTemplate;
    private final RabbitAdmin rabbitAdmin;
    
    public RabbitMQHealthIndicator(RabbitTemplate rabbitTemplate, RabbitAdmin rabbitAdmin) {
        this.rabbitTemplate = rabbitTemplate;
        this.rabbitAdmin = rabbitAdmin;
    }
    
    @Override
    protected void doHealthCheck(Health.Builder builder) throws Exception {
        try {
            // Check connection
            ConnectionFactory cf = rabbitTemplate.getConnectionFactory();
            Connection conn = cf.createConnection();
            conn.close();
            
            // Get queue properties
            Properties properties = rabbitAdmin.getQueueProperties(RabbitMQConfig.DIRECT_QUEUE);
            
            if (properties != null) {
                builder.up()
                    .withDetail("queue.name", RabbitMQConfig.DIRECT_QUEUE)
                    .withDetail("queue.messageCount", 
                        properties.get("QUEUE_MESSAGE_COUNT"))
                    .withDetail("queue.consumerCount", 
                        properties.get("QUEUE_CONSUMER_COUNT"));
            } else {
                builder.down()
                    .withDetail("error", "Queue not found: " + RabbitMQConfig.DIRECT_QUEUE);
            }
            
        } catch (Exception e) {
            builder.down()
                .withDetail("error", e.getMessage());
        }
    }
}
```

### 5.2 Metrics Collection

Implement metrics collection for monitoring message flow:

```java
@Configuration
public class RabbitMetricsConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public RabbitMetricsCollector rabbitMetricsCollector(
            MeterRegistry registry, 
            ConnectionFactory connectionFactory) {
        return new RabbitMetricsCollector(registry) {
            private Counter publishCounter;
            private Counter consumeCounter;
            private Timer processTimer;
            
            @PostConstruct
            public void init() {
                publishCounter = registry.counter("rabbitmq.messages.published");
                consumeCounter = registry.counter("rabbitmq.messages.consumed");
                processTimer = registry.timer("rabbitmq.message.processing");
            }
            
            public void recordMessagePublished() {
                publishCounter.increment();
            }
            
            public void recordMessageConsumed() {
                consumeCounter.increment();
            }
            
            public Timer.Sample startMessageProcessing() {
                return Timer.start(registry);
            }
            
            public void stopMessageProcessing(Timer.Sample sample) {
                sample.stop(processTimer);
            }
        };
    }
}
```

## 6. Production Best Practices

### 6.1 High Availability Configuration

For production environments, implement proper HA configurations:

```java
@Configuration
public class RabbitMQProductionConfig {
    
    @Bean
    public ConnectionFactory productionConnectionFactory() {
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        
        // Configure multiple hosts for HA
        connectionFactory.setAddresses(
            "rabbit1:5672,rabbit2:5672,rabbit3:5672");
            
        // Connection pooling settings
        connectionFactory.setChannelCacheSize(25);
        connectionFactory.setChannelCheckoutTimeout(1000);
        
        // Recovery settings
        connectionFactory.setRequestedHeartbeat(Duration.ofSeconds(60));
        connectionFactory.setConnectionTimeout(Duration.ofSeconds(30));
        
        return connectionFactory;
    }
    
    @Bean
    public RabbitTemplate productionRabbitTemplate(
            ConnectionFactory productionConnectionFactory) {
        RabbitTemplate template = new RabbitTemplate(productionConnectionFactory);
        
        // Configure message persistence
        template.setMandatory(true);
        template.setMessageConverter(new Jackson2JsonMessageConverter());
        
        // Configure timeouts
        template.setReplyTimeout(60000);
        template.setReceiveTimeout(60000);
        
        return template;
    }
}
```

### 6.2 Message Reliability Settings

Implement reliable messaging patterns:

```java
@Service
@Slf4j
public class ReliableMessageService {
    
    private final RabbitTemplate rabbitTemplate;
    private final RabbitAdmin rabbitAdmin;
    
    @Transactional
    public void sendReliableMessage(String exchange, String routingKey, Object message) {
        CorrelationData correlationData = new CorrelationData(
            UUID.randomUUID().toString());
            
        correlationData.getFuture().addCallback(
            result -> {
                if (result.isAck()) {
                    log.info("Message confirmed: {}", correlationData.getId());
                } else {
                    log.error("Message not confirmed: {}, reason: {}", 
                        correlationData.getId(), 
                        result.getReason());
                    handlePublishFailure(message, correlationData);
                }
            },
            ex -> {
                log.error("Message confirmation failed", ex);
                handlePublishFailure(message, correlationData);
            }
        );
        
        rabbitTemplate.convertAndSend(exchange, routingKey, message, 
            msg -> {
                MessageProperties props = msg.getMessageProperties();
                props.setDeliveryMode(MessageDeliveryMode.PERSISTENT);
                props.setContentType(MessageProperties.CONTENT_TYPE_JSON);
                props.setPriority(0);
                return msg;
            },
            correlationData
        );
    }
    
    private void handlePublishFailure(Object message, CorrelationData correlationData) {
        // Implement failure handling strategy
        // For example: retry, store in database, notify support
    }
}
```

### 6.3 Consumer Concurrency Management

Configure consumer concurrency for optimal performance:

```java
@Configuration
public class ConsumerConcurrencyConfig {
    
    @Bean
    public SimpleRabbitListenerContainerFactory concurrentListenerFactory(
            ConnectionFactory connectionFactory) {
        SimpleRabbitListenerContainerFactory factory = 
            new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        
        // Configure concurrency
        factory.setConcurrentConsumers(5);
        factory.setMaxConcurrentConsumers(10);
        
        // Configure prefetch
        factory.setPrefetchCount(250);
        
        // Configure error handling
        factory.setDefaultRequeueRejected(false);
        factory.setErrorHandler(new ConditionalRejectingErrorHandler());
        
        return factory;
    }
}
```

## 7. Security Configuration

### 7.1 SSL/TLS Configuration

Configure secure connections for production:

```java
@Configuration
public class RabbitMQSecurityConfig {
    
    @Bean
    public RabbitTemplate secureRabbitTemplate(
            @Value("${rabbitmq.ssl.keyStore}") Resource keyStore,
            @Value("${rabbitmq.ssl.trustStore}") Resource trustStore,
            @Value("${rabbitmq.ssl.keyStore.password}") String keyStorePassword,
            @Value("${rabbitmq.ssl.trustStore.password}") String trustStorePassword) {
        
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        connectionFactory.getRabbitConnectionFactory().useSslProtocol();
        
        // Configure SSL properties
        connectionFactory.getRabbitConnectionFactory()
            .setKeyStore(keyStore.getFile())
            .setKeyStorePassword(keyStorePassword)
            .setTrustStore(trustStore.getFile())
            .setTrustStorePassword(trustStorePassword);
            
        return new RabbitTemplate(connectionFactory);
    }
}
```

## 8. Testing Configuration

### 8.1 Integration Testing Support

Set up test configurations for integration testing:

```java
@Configuration
@Profile("test")
public class RabbitMQTestConfig {
    
    @Bean
    public Queue testQueue() {
        return new Queue("test.queue", false, false, true);
    }
    
    @Bean
    public RabbitTemplate testRabbitTemplate(
            ConnectionFactory connectionFactory) {
        RabbitTemplate template = new RabbitTemplate(connectionFactory);
        template.setReceiveTimeout(10000);
        return template;
    }
    
    @Bean
    public RabbitListenerTestHarness testHarness(
            RabbitListenerEndpointRegistry registry) {
        return new RabbitListenerTestHarness(registry);
    }
}
```

This completes our comprehensive guide to RabbitMQ configuration with Spring Boot 3.2.9. Would you like me to proceed with the Kafka configuration section next?# Apache Kafka Configuration Guide with Spring Boot 3.2.9

## 1. Understanding Kafka Architecture

Before diving into the configuration, let's understand the key components of Kafka that we'll be working with:

- ZooKeeper: Manages cluster metadata and coordinates broker operations
- Brokers: The individual Kafka servers that store and manage topic partitions
- Topics: Categories for organizing message streams
- Partitions: Distributed segments of topics across brokers
- Producers: Applications that publish messages to topics
- Consumers: Applications that subscribe to topics and process messages
- Consumer Groups: Groups of consumers that work together to process messages

## 2. Single Broker Setup

### 2.1 Docker-based Installation

Let's start with a single broker setup using Docker:

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
```

### 2.2 Basic Broker Configuration

Here's a basic server.properties configuration for a single broker:

```properties
# Broker Core Settings
broker.id=1
listeners=PLAINTEXT://localhost:9092
advertised.listeners=PLAINTEXT://localhost:9092
num.network.threads=3
num.io.threads=8

# Topic Defaults
num.partitions=3
default.replication.factor=1

# Log Settings
log.dirs=/var/lib/kafka/data
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

# ZooKeeper Connection
zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=18000
```

## 3. Cluster Configuration

### 3.1 Multi-broker Setup

Let's configure a three-node Kafka cluster:

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2

  kafka2:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2

  kafka3:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
```

## 4. Spring Boot Integration

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
    </dependency>
    
    <!-- For JSON serialization -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

### 4.2 Kafka Configuration

Let's create a comprehensive Kafka configuration that supports both single broker and cluster setups:

```java
@Configuration
@EnableKafka
public class KafkaConfig {
    
    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;
    
    @Value("${spring.kafka.consumer.group-id}")
    private String consumerGroupId;
    
    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // Basic producer configuration
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
            StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
            JsonSerializer.class);
            
        // Reliability settings
        configProps.put(ProducerConfig.ACKS_CONFIG, "all");
        configProps.put(ProducerConfig.RETRIES_CONFIG, 3);
        configProps.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 1000);
        
        // Performance settings
        configProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        configProps.put(ProducerConfig.LINGER_MS_CONFIG, 1);
        configProps.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
        
        // Idempotence for exactly-once semantics
        configProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        
        return new DefaultKafkaProducerFactory<>(configProps);
    }
    
    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
    
    @Bean
    public ConsumerFactory<String, Object> consumerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // Basic consumer configuration
        configProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configProps.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroupId);
        configProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
            StringDeserializer.class);
        configProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
            JsonDeserializer.class);
            
        // Consumption settings
        configProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        configProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        configProps.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
        
        // Performance settings
        configProps.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1);
        configProps.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);
        configProps.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 1048576);
        
        // Reliability settings
        configProps.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        configProps.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);
        
        return new DefaultKafkaConsumerFactory<>(configProps);
    }
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Object> 
            kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        
        // Configure concurrency
        factory.setConcurrency(3);
        factory.getContainerProperties().setPollTimeout(3000);
        
        // Configure batch listening
        factory.setBatchListener(true);
        factory.getContainerProperties().setIdleBetweenPolls(1000);
        
        // Configure error handling
        factory.setErrorHandler(new SeekToCurrentErrorHandler(
            new DeadLetterPublishingRecoverer(kafkaTemplate()),
            new FixedBackOff(1000L, 3)));
            
        return factory;
    }
    
    @Bean
    public NewTopic deadLetterTopic() {
        return TopicBuilder.name("dead-letter-topic")
            .partitions(3)
            .replicas(3)
            .build();
    }
}
```

### 4.3 Message Production

Let's implement a producer service with different messaging patterns:

```java
@Service
@Slf4j
public class KafkaProducerService {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final ObjectMapper objectMapper;
    
    public KafkaProducerService(KafkaTemplate<String, Object> kafkaTemplate,
                               ObjectMapper objectMapper) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
    }
    
    /**
     * Sends a message with synchronous confirmation
     */
    public SendResult<String, Object> sendMessageSync(String topic, 
            Object message) throws Exception {
        ProducerRecord<String, Object> record = 
            new ProducerRecord<>(topic, message);
            
        try {
            return kafkaTemplate.send(record).get(10, TimeUnit.SECONDS);
        } catch (ExecutionException e) {
            log.error("Error sending message to topic {}", topic, e);
            throw new KafkaException("Failed to send message", e);
        }
    }
    
    /**
     * Sends a message asynchronously with callback
     */
    public void sendMessageAsync(String topic, Object message, 
            BiConsumer<SendResult<String, Object>, Throwable> callback) {
        ProducerRecord<String, Object> record = 
            new ProducerRecord<>(topic, message);
            
        ListenableFuture<SendResult<String, Object>> future = 
            kafkaTemplate.send(record);
            
        future.addCallback(
            result -> {
                if (callback != null) {
                    callback.accept(result, null);
                }
                logSuccess(result);
            },
            ex -> {
                if (callback != null) {
                    callback.accept(null, ex);
                }
                logError(topic, ex);
            }
        );
    }
    
    /**
     * Sends a message with a specific partition key
     */
    public void sendMessageWithKey(String topic, String key, 
            Object message) {
        try {
            ProducerRecord<String, Object> record = 
                new ProducerRecord<>(topic, key, message);
                
            kafkaTemplate.send(record)
                .addCallback(
                    result -> logSuccess(result),
                    ex -> logError(topic, ex)
                );
        } catch (Exception e) {
            log.error("Error preparing message for topic {}", topic, e);
            throw new KafkaException("Failed to prepare message", e);
        }
    }
    
    /**
     * Sends messages in batch
     */
    public void sendBatchMessages(String topic, 
            List<Object> messages) {
        messages.forEach(message -> 
            kafkaTemplate.send(topic, message)
                .addCallback(
                    result -> logSuccess(result),
                    ex -> logError(topic, ex)
                )
        );
    }
    
    private void logSuccess(SendResult<String, Object> result) {
        ProducerRecord<String, Object> record = result.getProducerRecord();
        RecordMetadata metadata = result.getRecordMetadata();
        
        log.info("Message sent successfully to topic {} partition {} offset {}", 
            metadata.topic(), 
            metadata.partition(), 
            metadata.offset());
    }
    
    private void logError(String topic, Throwable ex) {
        log.error("Failed to send message to topic {}", topic, ex);
    }
}
```

### 4.4 Message Consumption

Implementation of consumer patterns:

```java
@Service
@Slf4j
public class KafkaConsumerService {
    
    /**
     * Simple message consumer
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}")
    public void consumeMessage(@Payload Object message,
                             @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
                             @Header(KafkaHeaders.OFFSET) long offset) {
        log.info("Received message from partition {} offset {}: {}", 
            partition, offset, message);
            
        try {
            processMessage(message);
        } catch (Exception e) {
            log.error("Error processing message", e);
            throw new KafkaException("Message processing failed", e);
        }
    }
    
    /**
     * Batch message consumer
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}",
        batch = "true")
    public void consumeBatch(@Payload List<Object> messages,
                           @Header(KafkaHeaders.RECEIVED_PARTITION) List<Integer> partitions,
                           @Header(KafkaHeaders.OFFSET) List<Long> offsets) {
        log.info("Received batch of {} messages", messages.size());
        
        try {
            processBatch(messages, partitions, offsets);
        } catch (Exception e) {
            log.error("Error processing message batch", e);
            throw new KafkaException("Batch processing failed", e);
        }
    }
    
    /**
     * Concurrent message processing
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}",
        concurrency = "3")
    public void consumeConcurrent(@Payload Object message,
                                ConsumerRecord<String, Object> record,
                                Consumer<?, ?> consumer) {
        log.info("Received message on thread {}: {}", 
            Thread.currentThread().getName(), message);
            
        try {
            processMessageConcurrently(message);
            // Manual commit after successful processing
            consumer.commitSync(Collections.# Apache Kafka Configuration Guide with Spring Boot 3.2.9

## 1. Understanding Kafka Architecture

Before diving into the configuration, let's understand the key components of Kafka that we'll be working with:

- ZooKeeper: Manages cluster metadata and coordinates broker operations
- Brokers: The individual Kafka servers that store and manage topic partitions
- Topics: Categories for organizing message streams
- Partitions: Distributed segments of topics across brokers
- Producers: Applications that publish messages to topics
- Consumers: Applications that subscribe to topics and process messages
- Consumer Groups: Groups of consumers that work together to process messages

## 2. Single Broker Setup

### 2.1 Docker-based Installation

Let's start with a single broker setup using Docker:

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
```

### 2.2 Basic Broker Configuration

Here's a basic server.properties configuration for a single broker:

```properties
# Broker Core Settings
broker.id=1
listeners=PLAINTEXT://localhost:9092
advertised.listeners=PLAINTEXT://localhost:9092
num.network.threads=3
num.io.threads=8

# Topic Defaults
num.partitions=3
default.replication.factor=1

# Log Settings
log.dirs=/var/lib/kafka/data
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

# ZooKeeper Connection
zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=18000
```

## 3. Cluster Configuration

### 3.1 Multi-broker Setup

Let's configure a three-node Kafka cluster:

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2

  kafka2:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2

  kafka3:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
```

## 4. Spring Boot Integration

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
    </dependency>
    
    <!-- For JSON serialization -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

### 4.2 Kafka Configuration

Let's create a comprehensive Kafka configuration that supports both single broker and cluster setups:

```java
@Configuration
@EnableKafka
public class KafkaConfig {
    
    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;
    
    @Value("${spring.kafka.consumer.group-id}")
    private String consumerGroupId;
    
    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // Basic producer configuration
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
            StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
            JsonSerializer.class);
            
        // Reliability settings
        configProps.put(ProducerConfig.ACKS_CONFIG, "all");
        configProps.put(ProducerConfig.RETRIES_CONFIG, 3);
        configProps.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 1000);
        
        // Performance settings
        configProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        configProps.put(ProducerConfig.LINGER_MS_CONFIG, 1);
        configProps.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
        
        // Idempotence for exactly-once semantics
        configProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        
        return new DefaultKafkaProducerFactory<>(configProps);
    }
    
    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
    
    @Bean
    public ConsumerFactory<String, Object> consumerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // Basic consumer configuration
        configProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configProps.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroupId);
        configProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
            StringDeserializer.class);
        configProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
            JsonDeserializer.class);
            
        // Consumption settings
        configProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        configProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        configProps.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
        
        // Performance settings
        configProps.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1);
        configProps.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);
        configProps.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 1048576);
        
        // Reliability settings
        configProps.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        configProps.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);
        
        return new DefaultKafkaConsumerFactory<>(configProps);
    }
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Object> 
            kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        
        // Configure concurrency
        factory.setConcurrency(3);
        factory.getContainerProperties().setPollTimeout(3000);
        
        // Configure batch listening
        factory.setBatchListener(true);
        factory.getContainerProperties().setIdleBetweenPolls(1000);
        
        // Configure error handling
        factory.setErrorHandler(new SeekToCurrentErrorHandler(
            new DeadLetterPublishingRecoverer(kafkaTemplate()),
            new FixedBackOff(1000L, 3)));
            
        return factory;
    }
    
    @Bean
    public NewTopic deadLetterTopic() {
        return TopicBuilder.name("dead-letter-topic")
            .partitions(3)
            .replicas(3)
            .build();
    }
}
```

### 4.3 Message Production

Let's implement a producer service with different messaging patterns:

```java
@Service
@Slf4j
public class KafkaProducerService {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final ObjectMapper objectMapper;
    
    public KafkaProducerService(KafkaTemplate<String, Object> kafkaTemplate,
                               ObjectMapper objectMapper) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
    }
    
    /**
     * Sends a message with synchronous confirmation
     */
    public SendResult<String, Object> sendMessageSync(String topic, 
            Object message) throws Exception {
        ProducerRecord<String, Object> record = 
            new ProducerRecord<>(topic, message);
            
        try {
            return kafkaTemplate.send(record).get(10, TimeUnit.SECONDS);
        } catch (ExecutionException e) {
            log.error("Error sending message to topic {}", topic, e);
            throw new KafkaException("Failed to send message", e);
        }
    }
    
    /**
     * Sends a message asynchronously with callback
     */
    public void sendMessageAsync(String topic, Object message, 
            BiConsumer<SendResult<String, Object>, Throwable> callback) {
        ProducerRecord<String, Object> record = 
            new ProducerRecord<>(topic, message);
            
        ListenableFuture<SendResult<String, Object>> future = 
            kafkaTemplate.send(record);
            
        future.addCallback(
            result -> {
                if (callback != null) {
                    callback.accept(result, null);
                }
                logSuccess(result);
            },
            ex -> {
                if (callback != null) {
                    callback.accept(null, ex);
                }
                logError(topic, ex);
            }
        );
    }
    
    /**
     * Sends a message with a specific partition key
     */
    public void sendMessageWithKey(String topic, String key, 
            Object message) {
        try {
            ProducerRecord<String, Object> record = 
                new ProducerRecord<>(topic, key, message);
                
            kafkaTemplate.send(record)
                .addCallback(
                    result -> logSuccess(result),
                    ex -> logError(topic, ex)
                );
        } catch (Exception e) {
            log.error("Error preparing message for topic {}", topic, e);
            throw new KafkaException("Failed to prepare message", e);
        }
    }
    
    /**
     * Sends messages in batch
     */
    public void sendBatchMessages(String topic, 
            List<Object> messages) {
        messages.forEach(message -> 
            kafkaTemplate.send(topic, message)
                .addCallback(
                    result -> logSuccess(result),
                    ex -> logError(topic, ex)
                )
        );
    }
    
    private void logSuccess(SendResult<String, Object> result) {
        ProducerRecord<String, Object> record = result.getProducerRecord();
        RecordMetadata metadata = result.getRecordMetadata();
        
        log.info("Message sent successfully to topic {} partition {} offset {}", 
            metadata.topic(), 
            metadata.partition(), 
            metadata.offset());
    }
    
    private void logError(String topic, Throwable ex) {
        log.error("Failed to send message to topic {}", topic, ex);
    }
}
```

### 4.4 Message Consumption

Implementation of consumer patterns:

```java
@Service
@Slf4j
public class KafkaConsumerService {
    
    /**
     * Simple message consumer
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}")
    public void consumeMessage(@Payload Object message,
                             @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
                             @Header(KafkaHeaders.OFFSET) long offset) {
        log.info("Received message from partition {} offset {}: {}", 
            partition, offset, message);
            
        try {
            processMessage(message);
        } catch (Exception e) {
            log.error("Error processing message", e);
            throw new KafkaException("Message processing failed", e);
        }
    }
    
    /**
     * Batch message consumer
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}",
        batch = "true")
    public void consumeBatch(@Payload List<Object> messages,
                           @Header(KafkaHeaders.RECEIVED_PARTITION) List<Integer> partitions,
                           @Header(KafkaHeaders.OFFSET) List<Long> offsets) {
        log.info("Received batch of {} messages", messages.size());
        
        try {
            processBatch(messages, partitions, offsets);
        } catch (Exception e) {
            log.error("Error processing message batch", e);
            throw new KafkaException("Batch processing failed", e);
        }
    }
    
    /**
     * Concurrent message processing
     */
    @KafkaListener(
        topics = "${app.kafka.topic.name}",
        groupId = "${spring.kafka.consumer.group-id}",
        concurrency = "3")
    public void consumeConcurrent(@Payload Object message,
                                ConsumerRecord<String, Object> record,
                                Consumer<?, ?> consumer) {
        log.info("Received message on thread {}: {}", 
            Thread.currentThread().getName(), message);
            
        try {
            processMessageConcurrently(message);
            // Manual commit after successful processing
            consumer.commitSync(Collections.singletonMap(
                new TopicPartition(record.topic(), record.partition()),
                new OffsetAndMetadata(record.offset() + 1)
            ));
        } catch (Exception e) {
            log.error("Error processing message concurrently", e);
            throw new KafkaException("Concurrent processing failed", e);
        }
    }
    
    private void processMessage(Object message) {
        // Implement single message processing logic
    }
    
    private void processBatch(List<Object> messages, 
            List<Integer> partitions, List<Long> offsets) {
        // Implement batch processing logic
    }
    
    private void processMessageConcurrently(Object message) {
        // Implement concurrent processing logic
    }
}
```

## 5. Advanced Configuration and Patterns

### 5.1 Transactional Message Processing

Implementing transactional message handling for exactly-once semantics:

```java
@Configuration
public class KafkaTransactionConfig {
    
    @Bean
    public KafkaTransactionManager<String, Object> kafkaTransactionManager(
            ProducerFactory<String, Object> producerFactory) {
        return new KafkaTransactionManager<>(producerFactory);
    }
    
    @Bean
    public KafkaTemplate<String, Object> transactionalKafkaTemplate(
            ProducerFactory<String, Object> producerFactory) {
        KafkaTemplate<String, Object> template = new KafkaTemplate<>(producerFactory);
        template.setDefaultTopic("default-topic");
        return template;
    }
}

@Service
@Transactional(transactionManager = "kafkaTransactionManager")
public class TransactionalKafkaService {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    
    public void processMessageTransactionally(String topic, Object message) {
        kafkaTemplate.executeInTransaction(operations -> {
            try {
                // Business logic here
                operations.send(topic, message);
                // Additional processing
                return true;
            } catch (Exception e) {
                throw new KafkaException("Transaction failed", e);
            }
        });
    }
}
```

### 5.2 Error Handling and Recovery

Implementing comprehensive error handling strategies:

```java
@Configuration
public class KafkaErrorHandlingConfig {
    
    @Bean
    public ConsumerRecordRecoverer recoverer(
            KafkaTemplate<String, Object> template) {
        return (consumerRecord, exception) -> {
            // Send to dead letter topic with error details
            template.send("dead-letter-topic", 
                DeadLetterRecord.builder()
                    .originalTopic(consumerRecord.topic())
                    .originalOffset(consumerRecord.offset())
                    .originalPartition(consumerRecord.partition())
                    .failureReason(exception.getMessage())
                    .failureTimestamp(System.currentTimeMillis())
                    .originalPayload(consumerRecord.value())
                    .build()
            );
        };
    }
    
    @Bean
    public DefaultErrorHandler errorHandler(ConsumerRecordRecoverer recoverer) {
        ExponentialBackOffWithMaxRetries backOff = 
            new ExponentialBackOffWithMaxRetries(3);
        backOff.setInitialInterval(1000L);
        backOff.setMultiplier(2.0);
        backOff.setMaxInterval(10000L);
        
        DefaultErrorHandler errorHandler = new DefaultErrorHandler(
            recoverer, backOff);
            
        // Configure which exceptions to retry
        errorHandler.addRetryableExceptions(
            RetriableException.class,
            TransientException.class
        );
        
        // Configure which exceptions to not retry
        errorHandler.addNotRetryableExceptions(
            NonRetriableException.class,
            ValidationException.class
        );
        
        return errorHandler;
    }
}
```

### 5.3 Monitoring and Metrics

Setting up comprehensive monitoring:

```java
@Configuration
public class KafkaMonitoringConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public KafkaListenerMetrics kafkaListenerMetrics(
            MeterRegistry registry,
            KafkaListenerEndpointRegistry listenerRegistry) {
        return new KafkaListenerMetrics(registry, listenerRegistry);
    }
    
    @Bean
    public Timer.Builder consumerTimer(MeterRegistry registry) {
        return Timer.builder("kafka.consumer.processing")
            .description("Time taken to process Kafka messages")
            .publishPercentileHistogram()
            .register(registry);
    }
}

@Component
public class KafkaMetricsCollector {
    
    private final MeterRegistry registry;
    private final Counter messageReceived;
    private final Counter processingErrors;
    private final Timer processingTimer;
    
    public KafkaMetricsCollector(MeterRegistry registry) {
        this.registry = registry;
        
        this.messageReceived = Counter.builder("kafka.consumer.messages.received")
            .description("Number of messages received")
            .register(registry);
            
        this.processingErrors = Counter.builder("kafka.consumer.processing.errors")
            .description("Number of processing errors")
            .register(registry);
            
        this.processTimer = Timer.builder("kafka.consumer.processing.time")
            .description("Message processing time")
            .register(registry);
    }
    
    public void recordMessageReceived() {
        messageReceived.increment();
    }
    
    public void recordError() {
        processingErrors.increment();
    }
    
    public Timer.Sample startProcessingTimer() {
        return Timer.start(registry);
    }
    
    public void stopProcessingTimer(Timer.Sample sample) {
        sample.stop(processTimer);
    }
}
```

### 5.4 Security Configuration

Implementing secure Kafka connections:

```java
@Configuration
public class KafkaSecurityConfig {
    
    @Bean
    public Map<String, Object> secureProducerConfig(
            @Value("${kafka.security.protocol}") String protocol,
            @Value("${kafka.ssl.truststore.location}") String truststoreLocation,
            @Value("${kafka.ssl.truststore.password}") String truststorePassword,
            @Value("${kafka.ssl.keystore.location}") String keystoreLocation,
            @Value("${kafka.ssl.keystore.password}") String keystorePassword) {
        
        Map<String, Object> configs = new HashMap<>();
        
        // Security protocol configuration
        configs.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol);
        
        // SSL configuration
        if ("SSL".equals(protocol) || "SASL_SSL".equals(protocol)) {
            configs.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, 
                truststoreLocation);
            configs.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, 
                truststorePassword);
            configs.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, 
                keystoreLocation);
            configs.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, 
                keystorePassword);
        }
        
        return configs;
    }
}
```

## 6. Performance Optimization

### 6.1 Producer Optimization

```java
@Configuration
public class KafkaProducerOptimizationConfig {
    
    @Bean
    public Map<String, Object> optimizedProducerConfig() {
        Map<String, Object> configs = new HashMap<>();
        
        // Batch settings for throughput
        configs.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);
        configs.put(ProducerConfig.LINGER_MS_CONFIG, 20);
        
        // Buffer settings
        configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864); // 64MB
        
        // Compression
        configs.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
        
        // Performance settings
        configs.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        
        return configs;
    }
}
```

### 6.2 Consumer Optimization

```java
@Configuration
public class KafkaConsumerOptimizationConfig {
    
    @Bean
    public Map<String, Object> optimizedConsumerConfig() {
        Map<String, Object> configs = new HashMap<>();
        
        // Fetch settings
        configs.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 32768);
        configs.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);
        
        // Performance settings
        configs.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 2097152);
        configs.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
        
        return configs;
    }
}
```

## 7. Testing Support

### 7.1 Integration Testing Configuration

```java
@Configuration
@Profile("test")
public class KafkaTestConfig {
    
    @Bean
    public EmbeddedKafkaBroker embeddedKafkaBroker() {
        return new EmbeddedKafkaBroker(1, true, 2, "test-topic");
    }
    
    @Bean
    public KafkaTemplate<String, Object> testKafkaTemplate(
            EmbeddedKafkaBroker embeddedKafkaBroker) {
        Map<String, Object> configs = new HashMap<>();
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
            embeddedKafkaBroker.getBrokersAsString());
        
        ProducerFactory<String, Object> producerFactory = 
            new DefaultKafkaProducerFactory<>(configs);
            
        return new KafkaTemplate<>(producerFactory);
    }
}

@SpringBootTest
class KafkaIntegrationTest {
    
    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;
    
    @Autowired
    private EmbeddedKafkaBroker embeddedKafkaBroker;
    
    @Test
    void testMessageProcessing() {
        // Test implementation
    }
}
```

## 8. Production Deployment Guidelines

### 8.1 High Availability Considerations

When deploying Kafka in production, consider the following:

1. Cluster Setup:
    - Deploy minimum 3 brokers for fault tolerance
    - Configure appropriate replication factor (usually 3)
    - Enable auto leader rebalancing
    - Configure proper min.insync.replicas

2. Monitoring:
    - Monitor broker health
    - Track consumer lag
    - Monitor disk usage
    - Set up alerting for critical metrics

3. Backup Strategy:
    - Regular topic backups
    - Configuration backups
    - Disaster recovery planning

### 8.2 Security Best Practices

1. Network Security:
    - Enable SSL/TLS encryption
    - Implement authentication
    - Configure ACLs
    - Use security protocols

2. Access Control:
    - Implement SASL authentication
    - Configure role-based access
    - Regular security audits
    - Credential rotation

This completes our comprehensive guide to Apache Kafka configuration with Spring Boot 3.2.9. Would you like me to proceed with the ZooKeeper configuration section next?# ZooKeeper Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ZooKeeper Architecture

ZooKeeper plays a crucial role in distributed systems by providing reliable coordination services. Before diving into configuration details, let's understand its core concepts and components.

### 1.1 Key Concepts

ZooKeeper organizes its data in a hierarchical namespace, similar to a file system. Each node in this hierarchy is called a "znode" and can store data while also having children nodes. This structure enables ZooKeeper to provide:

- Configuration Management: Storing and distributing configuration across distributed systems
- Leader Election: Coordinating leader selection in distributed applications
- Distributed Locking: Providing synchronization mechanisms across distributed processes
- Service Discovery: Helping services find and communicate with each other

### 1.2 Core Components

A ZooKeeper deployment consists of several key components:

- Ensemble: A group of ZooKeeper servers working together
- Leader: The server that coordinates updates in the ZooKeeper ensemble
- Followers: Servers that replicate data from the leader
- Clients: Applications that connect to ZooKeeper for coordination services

## 2. Standalone Setup

Let's start with a single-server setup, which is useful for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  zookeeper:
    image: zookeeper:3.8.0
    ports:
      - "2181:2181"
      - "2888:2888"
      - "3888:3888"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=localhost:2888:3888;2181
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog

volumes:
  zookeeper_data:
  zookeeper_datalog:
```

### 2.2 Basic Configuration

The zoo.cfg file is the heart of ZooKeeper configuration. Here's a basic configuration explained:

```properties
# Basic timing settings
tickTime=2000
# Time for initial synchronization
initLimit=10
# Time for synchronization updates
syncLimit=5

# Data directory path
dataDir=/data
# Transaction log directory
dataLogDir=/datalog

# Client connection port
clientPort=2181

# Maximum client connections
maxClientCnxns=60

# Automatic purge configuration
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
```

## 3. Ensemble Configuration

For production environments, we need a ZooKeeper ensemble (cluster) for high availability and reliability.

### 3.1 Multi-server Setup

Here's a Docker Compose configuration for a three-node ensemble:

```yaml
version: '3.8'
services:
  zookeeper1:
    image: zookeeper:3.8.0
    hostname: zookeeper1
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper1_data:/data
      - zookeeper1_datalog:/datalog

  zookeeper2:
    image: zookeeper:3.8.0
    hostname: zookeeper2
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper2_data:/data
      - zookeeper2_datalog:/datalog

  zookeeper3:
    image: zookeeper:3.8.0
    hostname: zookeeper3
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper3_data:/data
      - zookeeper3_datalog:/datalog

volumes:
  zookeeper1_data:
  zookeeper1_datalog:
  zookeeper2_data:
  zookeeper2_datalog:
  zookeeper3_data:
  zookeeper3_datalog:
```

## 4. Spring Boot Integration

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
    </dependency>
    
    <!-- Curator Framework -->
    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-framework</artifactId>
        <version>5.5.0</version>
    </dependency>
    
    <!-- Curator Recipes -->
    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-recipes</artifactId>
        <version>5.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ZooKeeper Configuration

Let's create a comprehensive ZooKeeper configuration that supports both standalone and ensemble setups:

```java
@Configuration
public class ZooKeeperConfig {
    
    @Value("${zookeeper.connection-string}")
    private String connectionString;
    
    @Value("${zookeeper.session-timeout-ms}")
    private Integer sessionTimeout;
    
    @Value("${zookeeper.connection-timeout-ms}")
    private Integer connectionTimeout;
    
    @Value("${zookeeper.retry-base-sleep-ms}")
    private Integer retryBaseSleep;
    
    @Value("${zookeeper.retry-max-retries}")
    private Integer maxRetries;
    
    @Bean(initMethod = "start", destroyMethod = "close")
    public CuratorFramework curatorFramework() {
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            .sessionTimeoutMs(sessionTimeout)
            .connectionTimeoutMs(connectionTimeout)
            .retryPolicy(new ExponentialBackoffRetry(retryBaseSleep, maxRetries))
            .build();
    }
    
    @Bean
    public LeaderSelector leaderSelector(
            CuratorFramework client,
            @Value("${zookeeper.leader-path}") String leaderPath) {
        return new LeaderSelector(
            client,
            leaderPath,
            new LeaderSelectorListenerAdapter() {
                @Override
                public void takeLeadership(CuratorFramework client) {
                    // Implement leadership tasks
                }
            });
    }
    
    @Bean
    public InterProcessMutex distributedLock(
            CuratorFramework client,
            @Value("${zookeeper.lock-path}") String lockPath) {
        return new InterProcessMutex(client, lockPath);
    }
}
```

### 4.3 Distributed Services Implementation

Here's a service class that demonstrates common ZooKeeper patterns:

```java
@Service
@Slf4j
public class ZooKeeperService {
    
    private final CuratorFramework client;
    private final LeaderSelector leaderSelector;
    private final InterProcessMutex distributedLock;
    
    public ZooKeeperService(CuratorFramework client,
                           LeaderSelector leaderSelector,
                           InterProcessMutex distributedLock) {
        this.client = client;
        this.leaderSelector = leaderSelector;
        this.distributedLock = distributedLock;
    }
    
    /**
     * Store configuration data in ZooKeeper
     */
    public void storeConfig(String path, String data) throws Exception {
        try {
            // Create parent nodes if they don't exist
            client.create()
                .creatingParentsIfNeeded()
                .withMode(CreateMode.PERSISTENT)
                .forPath(path, data.getBytes());
                
            log.info("Stored configuration at path: {}", path);
        } catch (Exception e) {
            log.error("Failed to store configuration", e);
            throw e;
        }
    }
    
    /**
     * Retrieve configuration data from ZooKeeper
     */
    public String getConfig(String path) throws Exception {
        try {
            byte[] data = client.getData().forPath(path);
            return new String(data);
        } catch (Exception e) {
            log.error("Failed to retrieve configuration", e);
            throw e;
        }
    }
    
    /**
     * Implement leader election
     */
    public void participateInLeaderElection(String participantId) {
        leaderSelector.setId(participantId);
        leaderSelector.start();
        
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            leaderSelector.close();
        }));
    }
    
    /**
     * Use distributed lock for synchronization
     */
    public void executeWithLock(Runnable task) {
        try {
            if (distributedLock.acquire(10, TimeUnit.SECONDS)) {
                try {
                    task.run();
                } finally {
                    distributedLock.release();
                }
            } else {
                throw new TimeoutException("Failed to acquire lock");
            }
        } catch (Exception e) {
            log.error("Error executing task with distributed lock", e);
            throw new RuntimeException("Lock operation failed", e);
        }
    }
    
    /**
     * Implement service discovery
     */
    public void registerService(String servicePath, ServiceInstance<?> instance) {
        try {
            ServiceDiscovery<?> serviceDiscovery = ServiceDiscoveryBuilder.builder()
                .client(client)
                .basePath(servicePath)
                .serializer(new JsonInstanceSerializer<>(Object.class))
                .build();
                
            serviceDiscovery.registerService(instance);
            serviceDiscovery.start();
            
        } catch (Exception e) {
            log.error("Failed to register service", e);
            throw new RuntimeException("Service registration failed", e);
        }
    }
}
```

## 5. Advanced Patterns

### 5.1 Distributed Lock Implementation

```java
@Component
public class ZooKeeperLock {
    
    private final InterProcessMutex mutex;
    private final long defaultTimeout;
    
    public ZooKeeperLock(InterProcessMutex mutex,
            @Value("${zookeeper.lock.default-timeout-ms}") long defaultTimeout) {
        this.mutex = mutex;
        this.defaultTimeout = defaultTimeout;
    }
    
    public <T> T executeWithLock(Supplier<T> task) throws Exception {
        return executeWithLock(task, defaultTimeout);
    }
    
    public <T> T executeWithLock(Supplier<T> task, long timeoutMs) 
            throws Exception {
        try {
            if (mutex.acquire(timeoutMs, TimeUnit.MILLISECONDS)) {
                try {
                    return task.get();
                } finally {
                    mutex.release();
                }
            } else {
                throw new TimeoutException("Failed to acquire lock");
            }
        } catch (Exception e) {
            throw new RuntimeException("Lock operation failed", e);
        }
    }
}
```

### 5.2 Configuration Management

```java
@Component
public class ZooKeeperConfigManager {
    
    private final CuratorFramework client;
    private final String configRootPath;
    private final ObjectMapper objectMapper;
    
    public ZooKeeperConfigManager(CuratorFramework client,
            @Value("${zookeeper.config.root-path}") String configRootPath,
            ObjectMapper objectMapper) {
        this.client = client;
        this.configRootPath = configRootPath;
        this.objectMapper = objectMapper;
    }
    
    public <T> void storeConfig(String key, T config) throws Exception {
        String path = configRootPath + "/" + key;
        byte[] data = objectMapper.writeValueAsBytes(config);
        
        try {
            // Create or update configuration
            if (client.checkExists().forPath(path) != null) {
                client.setData().forPath(path, data);
            } else {
                client.create()
                    .creatingParentsIfNeeded()
                    .forPath(path, data);
            }
        } catch (Exception e) {
            throw new RuntimeException("Failed to store configuration", e);
        }
    }
    
    public <T> T getConfig(String key, Class<T> type) throws Exception {
        String path = configRootPath + "/" + key;
        
        try {
            byte[] data = client.getData().forPath(path);
            return objectMapper.readValue(data, type);
        } catch (Exception e) {
            throw new RuntimeException("Failed to retrieve configuration", e);
        }
    }
    
    public void watchConfig(String key, ConfigurationChangeListener listener) {
        String path = configRootPath + "/" + key;
        
        try {
            NodeCache nodeCache = new NodeCache(client, path);
            nodeCache.getListenable().addListener(() -> {
                byte[] data = nodeCache.getCurrentData().getData();
                listener.onConfigurationChange(data);
            });
            nodeCache.start();
        } catch (Exception e) {
            throw new RuntimeException("Failed to watch configuration", e);
        }
    }
}
```

## 6. Security Configuration

### 6.1 Authentication and ACL Setup

```java
@Configuration
public class ZooKeeperSecurityConfig {
    
    @Bean
    public CuratorFramework secureClient(
            @Value("${zookeeper.connection-string}") String connectionString,
            @Value("${zookeeper.auth.scheme}") String scheme,
            @Value("${zookeeper.auth.auth}") String auth) {
        
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            .authorization(scheme, auth.getBytes())
            .retryPolicy(new ExponentialBackoffRetry(1000, 3))
            .build();
    }
    
    @Bean
    public ACLProvider aclProvider(
            @Value("${zookeeper.acl.scheme}") String scheme,
            @Value("${zookeeper.acl.id}") String id,
            @Value("${zookeeper.acl.permissions}") int permissions) {
        
        return new ACLProvider() {
            @Override
            public List<ACL> getDefaultAcl() {
                return Collections.singletonList(
                    new ACL(permissions, new Id(scheme, id))
                );
            }
            
            @Override
            public List<ACL> getAclForPath(String path) {
                return getDefaultAcl();
            }
        };
    }
}
```

## 7. Monitoring and Management

### 7.1 Health Indicators

```java# ZooKeeper Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ZooKeeper Architecture

ZooKeeper plays a crucial role in distributed systems by providing reliable coordination services. Before diving into configuration details, let's understand its core concepts and components.

### 1.1 Key Concepts

ZooKeeper organizes its data in a hierarchical namespace, similar to a file system. Each node in this hierarchy is called a "znode" and can store data while also having children nodes. This structure enables ZooKeeper to provide:

- Configuration Management: Storing and distributing configuration across distributed systems
- Leader Election: Coordinating leader selection in distributed applications
- Distributed Locking: Providing synchronization mechanisms across distributed processes
- Service Discovery: Helping services find and communicate with each other

### 1.2 Core Components

A ZooKeeper deployment consists of several key components:

- Ensemble: A group of ZooKeeper servers working together
- Leader: The server that coordinates updates in the ZooKeeper ensemble
- Followers: Servers that replicate data from the leader
- Clients: Applications that connect to ZooKeeper for coordination services

## 2. Standalone Setup

Let's start with a single-server setup, which is useful for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  zookeeper:
    image: zookeeper:3.8.0
    ports:
      - "2181:2181"
      - "2888:2888"
      - "3888:3888"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=localhost:2888:3888;2181
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog

volumes:
  zookeeper_data:
  zookeeper_datalog:
```

### 2.2 Basic Configuration

The zoo.cfg file is the heart of ZooKeeper configuration. Here's a basic configuration explained:

```properties
# Basic timing settings
tickTime=2000
# Time for initial synchronization
initLimit=10
# Time for synchronization updates
syncLimit=5

# Data directory path
dataDir=/data
# Transaction log directory
dataLogDir=/datalog

# Client connection port
clientPort=2181

# Maximum client connections
maxClientCnxns=60

# Automatic purge configuration
autopurge.snapRetainCount=3
autopurge.purgeInterval=1
```

## 3. Ensemble Configuration

For production environments, we need a ZooKeeper ensemble (cluster) for high availability and reliability.

### 3.1 Multi-server Setup

Here's a Docker Compose configuration for a three-node ensemble:

```yaml
version: '3.8'
services:
  zookeeper1:
    image: zookeeper:3.8.0
    hostname: zookeeper1
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper1_data:/data
      - zookeeper1_datalog:/datalog

  zookeeper2:
    image: zookeeper:3.8.0
    hostname: zookeeper2
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper2_data:/data
      - zookeeper2_datalog:/datalog

  zookeeper3:
    image: zookeeper:3.8.0
    hostname: zookeeper3
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zookeeper1:2888:3888;2181 server.2=zookeeper2:2888:3888;2181 server.3=zookeeper3:2888:3888;2181
    volumes:
      - zookeeper3_data:/data
      - zookeeper3_datalog:/datalog

volumes:
  zookeeper1_data:
  zookeeper1_datalog:
  zookeeper2_data:
  zookeeper2_datalog:
  zookeeper3_data:
  zookeeper3_datalog:
```

## 4. Spring Boot Integration

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
    </dependency>
    
    <!-- Curator Framework -->
    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-framework</artifactId>
        <version>5.5.0</version>
    </dependency>
    
    <!-- Curator Recipes -->
    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-recipes</artifactId>
        <version>5.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ZooKeeper Configuration

Let's create a comprehensive ZooKeeper configuration that supports both standalone and ensemble setups:

```java
@Configuration
public class ZooKeeperConfig {
    
    @Value("${zookeeper.connection-string}")
    private String connectionString;
    
    @Value("${zookeeper.session-timeout-ms}")
    private Integer sessionTimeout;
    
    @Value("${zookeeper.connection-timeout-ms}")
    private Integer connectionTimeout;
    
    @Value("${zookeeper.retry-base-sleep-ms}")
    private Integer retryBaseSleep;
    
    @Value("${zookeeper.retry-max-retries}")
    private Integer maxRetries;
    
    @Bean(initMethod = "start", destroyMethod = "close")
    public CuratorFramework curatorFramework() {
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            .sessionTimeoutMs(sessionTimeout)
            .connectionTimeoutMs(connectionTimeout)
            .retryPolicy(new ExponentialBackoffRetry(retryBaseSleep, maxRetries))
            .build();
    }
    
    @Bean
    public LeaderSelector leaderSelector(
            CuratorFramework client,
            @Value("${zookeeper.leader-path}") String leaderPath) {
        return new LeaderSelector(
            client,
            leaderPath,
            new LeaderSelectorListenerAdapter() {
                @Override
                public void takeLeadership(CuratorFramework client) {
                    // Implement leadership tasks
                }
            });
    }
    
    @Bean
    public InterProcessMutex distributedLock(
            CuratorFramework client,
            @Value("${zookeeper.lock-path}") String lockPath) {
        return new InterProcessMutex(client, lockPath);
    }
}
```

### 4.3 Distributed Services Implementation

Here's a service class that demonstrates common ZooKeeper patterns:

```java
@Service
@Slf4j
public class ZooKeeperService {
    
    private final CuratorFramework client;
    private final LeaderSelector leaderSelector;
    private final InterProcessMutex distributedLock;
    
    public ZooKeeperService(CuratorFramework client,
                           LeaderSelector leaderSelector,
                           InterProcessMutex distributedLock) {
        this.client = client;
        this.leaderSelector = leaderSelector;
        this.distributedLock = distributedLock;
    }
    
    /**
     * Store configuration data in ZooKeeper
     */
    public void storeConfig(String path, String data) throws Exception {
        try {
            // Create parent nodes if they don't exist
            client.create()
                .creatingParentsIfNeeded()
                .withMode(CreateMode.PERSISTENT)
                .forPath(path, data.getBytes());
                
            log.info("Stored configuration at path: {}", path);
        } catch (Exception e) {
            log.error("Failed to store configuration", e);
            throw e;
        }
    }
    
    /**
     * Retrieve configuration data from ZooKeeper
     */
    public String getConfig(String path) throws Exception {
        try {
            byte[] data = client.getData().forPath(path);
            return new String(data);
        } catch (Exception e) {
            log.error("Failed to retrieve configuration", e);
            throw e;
        }
    }
    
    /**
     * Implement leader election
     */
    public void participateInLeaderElection(String participantId) {
        leaderSelector.setId(participantId);
        leaderSelector.start();
        
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            leaderSelector.close();
        }));
    }
    
    /**
     * Use distributed lock for synchronization
     */
    public void executeWithLock(Runnable task) {
        try {
            if (distributedLock.acquire(10, TimeUnit.SECONDS)) {
                try {
                    task.run();
                } finally {
                    distributedLock.release();
                }
            } else {
                throw new TimeoutException("Failed to acquire lock");
            }
        } catch (Exception e) {
            log.error("Error executing task with distributed lock", e);
            throw new RuntimeException("Lock operation failed", e);
        }
    }
    
    /**
     * Implement service discovery
     */
    public void registerService(String servicePath, ServiceInstance<?> instance) {
        try {
            ServiceDiscovery<?> serviceDiscovery = ServiceDiscoveryBuilder.builder()
                .client(client)
                .basePath(servicePath)
                .serializer(new JsonInstanceSerializer<>(Object.class))
                .build();
                
            serviceDiscovery.registerService(instance);
            serviceDiscovery.start();
            
        } catch (Exception e) {
            log.error("Failed to register service", e);
            throw new RuntimeException("Service registration failed", e);
        }
    }
}
```

## 5. Advanced Patterns

### 5.1 Distributed Lock Implementation

```java
@Component
public class ZooKeeperLock {
    
    private final InterProcessMutex mutex;
    private final long defaultTimeout;
    
    public ZooKeeperLock(InterProcessMutex mutex,
            @Value("${zookeeper.lock.default-timeout-ms}") long defaultTimeout) {
        this.mutex = mutex;
        this.defaultTimeout = defaultTimeout;
    }
    
    public <T> T executeWithLock(Supplier<T> task) throws Exception {
        return executeWithLock(task, defaultTimeout);
    }
    
    public <T> T executeWithLock(Supplier<T> task, long timeoutMs) 
            throws Exception {
        try {
            if (mutex.acquire(timeoutMs, TimeUnit.MILLISECONDS)) {
                try {
                    return task.get();
                } finally {
                    mutex.release();
                }
            } else {
                throw new TimeoutException("Failed to acquire lock");
            }
        } catch (Exception e) {
            throw new RuntimeException("Lock operation failed", e);
        }
    }
}
```

### 5.2 Configuration Management

```java
@Component
public class ZooKeeperConfigManager {
    
    private final CuratorFramework client;
    private final String configRootPath;
    private final ObjectMapper objectMapper;
    
    public ZooKeeperConfigManager(CuratorFramework client,
            @Value("${zookeeper.config.root-path}") String configRootPath,
            ObjectMapper objectMapper) {
        this.client = client;
        this.configRootPath = configRootPath;
        this.objectMapper = objectMapper;
    }
    
    public <T> void storeConfig(String key, T config) throws Exception {
        String path = configRootPath + "/" + key;
        byte[] data = objectMapper.writeValueAsBytes(config);
        
        try {
            // Create or update configuration
            if (client.checkExists().forPath(path) != null) {
                client.setData().forPath(path, data);
            } else {
                client.create()
                    .creatingParentsIfNeeded()
                    .forPath(path, data);
            }
        } catch (Exception e) {
            throw new RuntimeException("Failed to store configuration", e);
        }
    }
    
    public <T> T getConfig(String key, Class<T> type) throws Exception {
        String path = configRootPath + "/" + key;
        
        try {
            byte[] data = client.getData().forPath(path);
            return objectMapper.readValue(data, type);
        } catch (Exception e) {
            throw new RuntimeException("Failed to retrieve configuration", e);
        }
    }
    
    public void watchConfig(String key, ConfigurationChangeListener listener) {
        String path = configRootPath + "/" + key;
        
        try {
            NodeCache nodeCache = new NodeCache(client, path);
            nodeCache.getListenable().addListener(() -> {
                byte[] data = nodeCache.getCurrentData().getData();
                listener.onConfigurationChange(data);
            });
            nodeCache.start();
        } catch (Exception e) {
            throw new RuntimeException("Failed to watch configuration", e);
        }
    }
}
```

## 6. Security Configuration

### 6.1 Authentication and ACL Setup

```java
@Configuration
public class ZooKeeperSecurityConfig {
    
    @Bean
    public CuratorFramework secureClient(
            @Value("${zookeeper.connection-string}") String connectionString,
            @Value("${zookeeper.auth.scheme}") String scheme,
            @Value("${zookeeper.auth.auth}") String auth) {
        
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            .authorization(scheme, auth.getBytes())
            .retryPolicy(new ExponentialBackoffRetry(1000, 3))
            .build();
    }
    
    @Bean
    public ACLProvider aclProvider(
            @Value("${zookeeper.acl.scheme}") String scheme,
            @Value("${zookeeper.acl.id}") String id,
            @Value("${zookeeper.acl.permissions}") int permissions) {
        
        return new ACLProvider() {
            @Override
            public List<ACL> getDefaultAcl() {
                return Collections.singletonList(
                    new ACL(permissions, new Id(scheme, id))
                );
            }
            
            @Override
            public List<ACL> getAclForPath(String path) {
                return getDefaultAcl();
            }
        };
    }
}
```

## 7. Monitoring and Management

### 7.1 Health Indicators

Creating robust health monitoring is essential for maintaining a reliable ZooKeeper deployment. Here's how to implement comprehensive health checks:

```java
@Component
public class ZooKeeperHealthIndicator extends AbstractHealthIndicator {
    
    private final CuratorFramework client;
    private final String healthCheckPath;
    
    public ZooKeeperHealthIndicator(
            CuratorFramework client,
            @Value("${zookeeper.health-check-path:/health}") String healthCheckPath) {
        this.client = client;
        this.healthCheckPath = healthCheckPath;
    }
    
    @Override
    protected void doHealthCheck(Health.Builder builder) throws Exception {
        try {
            // Check connection state
            if (client.getZookeeperClient().isConnected()) {
                // Perform write test
                byte[] testData = "health-check".getBytes();
                String testPath = healthCheckPath + "/test-" + System.currentTimeMillis();
                
                client.create()
                    .creatingParentsIfNeeded()
                    .withMode(CreateMode.EPHEMERAL)
                    .forPath(testPath, testData);
                
                // Read test data
                byte[] readData = client.getData().forPath(testPath);
                
                // Clean up test node
                client.delete().forPath(testPath);
                
                if (Arrays.equals(testData, readData)) {
                    builder.up()
                        .withDetail("status", "Connected and operational")
                        .withDetail("mode", getServerMode())
                        .withDetail("sessionId", client.getZookeeperClient()
                            .getZooKeeper().getSessionId());
                } else {
                    builder.down()
                        .withDetail("error", "Data integrity check failed");
                }
            } else {
                builder.down()
                    .withDetail("error", "ZooKeeper client is not connected");
            }
        } catch (Exception e) {
            builder.down()
                .withDetail("error", e.getMessage());
        }
    }
    
    private String getServerMode() {
        try {
            return new String(client.getData()
                .forPath("/zookeeper/config/mode"));
        } catch (Exception e) {
            return "unknown";
        }
    }
}
```

### 7.2 Metrics Collection

Implementing comprehensive metrics collection helps track the health and performance of your ZooKeeper ensemble:

```java
@Configuration
public class ZooKeeperMetricsConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public ZooKeeperMetricsCollector metricsCollector(
            MeterRegistry registry,
            CuratorFramework client) {
        return new ZooKeeperMetricsCollector(registry, client);
    }
}

@Component
@Slf4j
public class ZooKeeperMetricsCollector {
    
    private final MeterRegistry registry;
    private final CuratorFramework client;
    private final Map<String, Timer> operationTimers;
    private final Counter connectionLossCounter;
    private final Counter operationErrorCounter;
    
    public ZooKeeperMetricsCollector(MeterRegistry registry, 
            CuratorFramework client) {
        this.registry = registry;
        this.client = client;
        this.operationTimers = new ConcurrentHashMap<>();
        
        // Create base metrics
        this.connectionLossCounter = Counter.builder("zk.connection.losses")
            .description("Number of connection losses to ZooKeeper")
            .register(registry);
            
        this.operationErrorCounter = Counter.builder("zk.operation.errors")
            .description("Number of failed ZooKeeper operations")
            .register(registry);
            
        // Start monitoring
        startConnectionStateMonitoring();
    }
    
    private void startConnectionStateMonitoring() {
        client.getConnectionStateListenable().addListener((client, state) -> {
            if (state == ConnectionState.LOST) {
                connectionLossCounter.increment();
                log.warn("ZooKeeper connection lost");
            }
        });
    }
    
    public Timer.Sample startOperation(String operationType) {
        return Timer.start(registry);
    }
    
    public void recordOperationCompletion(Timer.Sample sample, 
            String operationType) {
        Timer timer = operationTimers.computeIfAbsent(operationType,
            type -> Timer.builder("zk.operation.duration")
                .tag("type", type)
                .description("Time taken for ZooKeeper operations")
                .register(registry));
                
        sample.stop(timer);
    }
    
    public void recordOperationError(String operationType) {
        operationErrorCounter.increment();
        registry.counter("zk.operation.errors", "type", operationType)
            .increment();
    }
}
```

### 7.3 Operational Monitoring

Let's implement a comprehensive monitoring service that tracks various operational aspects:

```java
@Service
@Slf4j
public class ZooKeeperMonitoringService {
    
    private final CuratorFramework client;
    private final ZooKeeperMetricsCollector metricsCollector;
    private final ScheduledExecutorService executor;
    
    public ZooKeeperMonitoringService(
            CuratorFramework client,
            ZooKeeperMetricsCollector metricsCollector) {
        this.client = client;
        this.metricsCollector = metricsCollector;
        this.executor = Executors.newScheduledThreadPool(1);
        
        // Start monitoring tasks
        startPerformanceMonitoring();
    }
    
    private void startPerformanceMonitoring() {
        executor.scheduleAtFixedRate(() -> {
            try {
                collectPerformanceMetrics();
            } catch (Exception e) {
                log.error("Error collecting performance metrics", e);
            }
        }, 0, 1, TimeUnit.MINUTES);
    }
    
    private void collectPerformanceMetrics() throws Exception {
        // Measure latency
        Timer.Sample sample = metricsCollector.startOperation("latency-check");
        
        String testPath = "/monitoring/latency-test";
        try {
            // Write operation
            client.create()
                .creatingParentsIfNeeded()
                .withMode(CreateMode.EPHEMERAL)
                .forPath(testPath, "test".getBytes());
                
            // Read operation
            client.getData().forPath(testPath);
            
            // Delete operation
            client.delete().forPath(testPath);
            
            metricsCollector.recordOperationCompletion(sample, "latency-check");
        } catch (Exception e) {
            metricsCollector.recordOperationError("latency-check");
            throw e;
        }
    }
}
```

## 8. Production Best Practices

### 8.1 High Availability Configuration

When deploying ZooKeeper in production, consider these essential configurations for high availability:

```java
@Configuration
public class ZooKeeperProductionConfig {
    
    @Bean
    public CuratorFramework productionClient(
            @Value("${zookeeper.connection-string}") String connectionString) {
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);
        
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            // Set reasonable timeouts
            .sessionTimeoutMs(60000)
            .connectionTimeoutMs(15000)
            // Configure retry policy
            .retryPolicy(retryPolicy)
            // Enable compression for data
            .compression(true)
            // Set namespace for application
            .namespace("myapp")
            .build();
    }
    
    @Bean
    public CuratorCache curatorCache(CuratorFramework client,
            @Value("${zookeeper.cache.path}") String cachePath) {
        return CuratorCache.build(client, cachePath);
    }
}
```

### 8.2 Disaster Recovery

Implementing robust backup and recovery mechanisms is crucial for production environments:

```java
@Service
@Slf4j
public class ZooKeeperBackupService {
    
    private final CuratorFramework client;
    private final Path backupDirectory;
    private final ObjectMapper objectMapper;
    
    public ZooKeeperBackupService(
            CuratorFramework client,
            @Value("${zookeeper.backup.directory}") String backupDirectory,
            ObjectMapper objectMapper) {
        this.client = client;
        this.backupDirectory = Paths.get(backupDirectory);
        this.objectMapper = objectMapper;
    }
    
    public void createBackup(String rootPath) throws Exception {
        Map<String, byte[]> backup = new HashMap<>();
        collectData(rootPath, backup);
        
        String timestamp = LocalDateTime.now()
            .format(DateTimeFormatter.ISO_LOCAL_DATE_TIME);
        Path backupFile = backupDirectory
            .resolve("zk-backup-" + timestamp + ".json");
        
        // Convert binary data to Base64 for JSON storage
        Map<String, String> encodedBackup = backup.entrySet().stream()
            .collect(Collectors.toMap(
                Map.Entry::getKey,
                e -> Base64.getEncoder().encodeToString(e.getValue())
            ));
            
        objectMapper.writeValue(backupFile.toFile(), encodedBackup);
        log.info("Backup created at: {}", backupFile);
    }
    
    private void collectData(String path, Map<String, byte[]> backup) 
            throws Exception {
        // Store current node's data
        byte[] data = client.getData().forPath(path);
        if (data != null && data.length > 0) {
            backup.put(path, data);
        }
        
        // Recursively collect children's data
        for (String child : client.getChildren().forPath(path)) {
            collectData(path + "/" + child, backup);
        }
    }
    
    public void restoreBackup(Path backupFile) throws Exception {
        Map<String, String> encodedBackup = objectMapper
            .readValue(backupFile.toFile(), 
                new TypeReference<Map<String, String>>() {});
                
        // Sort paths to ensure parent nodes are created first
        List<String> paths = new ArrayList<>(encodedBackup.keySet());
        paths.sort(Comparator.comparingInt(String::length));
        
        for (String path : paths) {
            byte[] data = Base64.getDecoder()
                .decode(encodedBackup.get(path));
                
            try {
                client.create()
                    .creatingParentsIfNeeded()
                    .forPath(path, data);
                    
                log.info("Restored node: {}", path);
            } catch (NodeExistsException e) {
                // Update existing node
                client.setData().forPath(path, data);
                log.info("Updated existing node: {}", path);
            }
        }
    }
}
```

### 8.3 Performance Tuning

Fine-tuning ZooKeeper for optimal performance in production:

```java
@Configuration
public class ZooKeeperPerformanceConfig {
    
    @Bean
    public CuratorFramework performanceOptimizedClient(
            @Value("${zookeeper.connection-string}") String connectionString) {
        return CuratorFrameworkFactory.builder()
            .connectString(connectionString)
            // Optimize connection handling
            .sessionTimeoutMs(60000)
            .connectionTimeoutMs(15000)
            // Configure thread pool
            .threadFactory(new ThreadFactoryBuilder()
                .setNameFormat("curator-connection-thread-%d")
                .setDaemon(true)
                .build())
            // Enable compression for large data
            .compression(true)
            // Set maximum retries
            .retryPolicy(new ExponentialBackoffRetry(1000, 3))
            .build();
    }
    
    @Bean
    public ExecutorService zkOperationExecutor() {
        return new ThreadPoolExecutor(
            10, // Core pool size
            50, // Maximum pool size
            60L, // Keep alive time
            TimeUnit.SECONDS,
            new ArrayBlockingQueue<>(1000), // Work queue
            new ThreadFactoryBuilder()
                .setNameFormat("zk-operation-thread-%d")
                .setDaemon(true)
                .build(),
            new ThreadPoolExecutor.CallerRunsPolicy() // Rejection policy
        );
    }
}
```

## 9. Testing Support

### 9.1 Integration Testing Configuration

Setting up proper testing infrastructure for ZooKeeper-dependent components:

```java
@Configuration
@Profile("test")
public class ZooKeeperTestConfig {
    
    @Bean(initMethod = "start", destroyMethod = "stop")
    public TestingServer testingServer() throws Exception {
        return new TestingServer(2181, true);
    }
    
    @Bean
    public CuratorFramework testClient(TestingServer testingServer) {
        CuratorFramework client = CuratorFrameworkFactory.builder()
            .connectString(testingServer.getConnectString())
            .retryPolicy(new RetryOneTime(1000))
            .build();
            
        client.start();
        return client;
    }
    
    @Bean
    public ZooKeeperTestHelper testHelper(CuratorFramework testClient) {
        return new ZooKeeperTestHelper(testClient);
    }
}

@Component
public class ZooKeeperTestHelper {
    
    private final CuratorFramework client;
    
    public ZooKeeperTestHelper(CuratorFramework client) {
        this.client = client;
    }
    
    public void createTestData(String path, byte[] data) throws Exception {
        client.create()
            .creatingParentsIfNeeded()
            .forPath(path, data);
    }
    
    public void clearTestData(String path) throws Exception {
        try {
            client.delete()
                .deletingChildrenIfNeeded()
                .forPath(path);
        } catch (KeeperException.NoNodeException e) {
            // Node doesn't exist, ignore
        }
    }
    
    public void waitForConnection() throws Exception {
        client.blockUntilConnected(10, TimeUnit.SECONDS);
    }
}
```

This completes our comprehensive guide to ZooKeeper configuration with Spring Boot 3.2.9. Would you like me to continue with the ClickHouse configuration section next?# ClickHouse Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ClickHouse Architecture

Before diving into configuration details, let's understand why ClickHouse is particularly well-suited for analytical workloads and how its architecture supports high-performance analytics.

ClickHouse is a column-oriented database management system designed for online analytical processing (OLAP). Its architecture is optimized for:
- Fast query processing on large datasets
- Real-time data ingestion
- Efficient compression
- Distributed query execution
- Fault tolerance and high availability

### 1.1 Key Concepts

Several fundamental concepts make ClickHouse powerful:

1. Column-oriented Storage: Data is stored by columns rather than rows, enabling:
    - Better compression ratios
    - Faster analytical queries
    - Efficient scanning of specific columns

2. Data Types and Encodings:
    - Specialized data types for analytics
    - Custom compression methods per column
    - Efficient numeric and string encodings

3. Distributed Architecture:
    - Sharding for horizontal scaling
    - Replication for fault tolerance
    - Distributed query execution

## 2. Single Node Setup

Let's start with a basic single-node ClickHouse installation using Docker, which is perfect for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
      - "9009:9009"   # Internal port for replication
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config:/etc/clickhouse-server/config.d
      - ./users:/etc/clickhouse-server/users.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

volumes:
  clickhouse_data:
```

### 2.2 Basic Configuration

The configuration consists of several XML files. Here's the basic setup:

```xml
<!-- config.xml -->
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>

    <memory>
        <!-- Recommended to set to 80% of available RAM -->
        <max_server_memory_usage>8589934592</max_server_memory_usage>
        <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
    </memory>

    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                </volumes>
            </default>
        </policies>
    </storage_configuration>

    <!-- Performance settings -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <default_max_threads>8</default_max_threads>
</clickhouse>

<!-- users.xml -->
<?xml version="1.0"?>
<clickhouse>
    <users>
        <default>
            <password>default_password</password>
            <networks>
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
        </default>
    </users>

    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by>
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort>
        </default>
    </profiles>

    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

## 3. Distributed Setup

For production environments, we'll configure a ClickHouse cluster with multiple nodes for high availability and better performance.

### 3.1 Multi-node Cluster Configuration

```yaml
version: '3.8'
services:
  clickhouse1:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse1_data:/var/lib/clickhouse
      - ./config/node1:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=1

  clickhouse2:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - clickhouse2_data:/var/lib/clickhouse
      - ./config/node2:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=2

  clickhouse3:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - clickhouse3_data:/var/lib/clickhouse
      - ./config/node3:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=1

  clickhouse4:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse4
    ports:
      - "8126:8123"
      - "9003:9000"
    volumes:
      - clickhouse4_data:/var/lib/clickhouse
      - ./config/node4:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=2

volumes:
  clickhouse1_data:
  clickhouse2_data:
  clickhouse3_data:
  clickhouse4_data:
```

### 3.2 Cluster Configuration

Let's configure the cluster with two shards and two replicas per shard:

```xml
<!-- metrika.xml -->
<?xml version="1.0"?>
<clickhouse>
    <remote_servers>
        <analytics_cluster>
            <!-- First shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Second shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </analytics_cluster>
    </remote_servers>

    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <macros>
        <shard>{CLICKHOUSE_SHARD}</shard>
        <replica>{CLICKHOUSE_REPLICA}</replica>
    </macros>
</clickhouse>
```

## 4. Spring Boot Integration

Now let's integrate ClickHouse with Spring Boot for seamless data access and management.

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    
    <!-- ClickHouse JDBC Driver -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ClickHouse Configuration

Let's create a comprehensive configuration that supports both single-node and cluster setups:

```java
@Configuration
@EnableTransactionManagement
public class ClickHouseConfig {
    
    @Bean
    @ConfigurationProperties(prefix = "clickhouse.datasource")
    public DataSourceProperties clickHouseDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    public DataSource clickHouseDataSource() {
        ClickHouseDataSource dataSource = new ClickHouseDataSource(
            createClickHouseProperties());
            
        // Configure connection pooling
        HikariConfig config = new HikariConfig();
        config.setDataSource(dataSource);
        config.setMaximumPoolSize(10);
        config.setMinimumIdle(5);
        config.setIdleTimeout(300000);
        config.setConnectionTimeout(20000);
        
        return new HikariDataSource(config);
    }
    
    private ClickHouseProperties createClickHouseProperties() {
        ClickHouseProperties properties = new ClickHouseProperties();
        properties.setUser("default");
        properties.setPassword("default_password");
        properties.setDatabase("default");
        
        // Performance settings
        properties.setUseServerTimeZone(true);
        properties.setUseTimeZone("UTC");
        properties.setUseServerTimeZoneForDates(true);
        
        // Connection settings
        properties.setSocketTimeout(30000);
        properties.setConnectionTimeout(10000);
        
        // Buffer settings
        properties.setBufferSize(65536);
        
        return properties;
    }
    
    @Bean
    public ClickHouseTemplate clickHouseTemplate(
            DataSource clickHouseDataSource) {
        return new ClickHouseTemplate(clickHouseDataSource);
    }
}
```

### 4.3 ClickHouse Template

Let's create a template class for executing queries:

```java
@Component
@Slf4j
public class ClickHouseTemplate {
    
    private final JdbcTemplate jdbcTemplate;
    
    public ClickHouseTemplate(DataSource dataSource) {
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    /**
     * Execute a batch insert operation
     */
    public <T> void batchInsert(String tableName, 
            List<T> records, 
            ClickHouseInsertMapper<T> mapper) {
        String insertSql = createInsertSql(tableName, mapper.getColumns());
        
        jdbcTemplate.batchUpdate(insertSql, 
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                        throws SQLException {
                    mapper.mapRow(ps, records.get(i));
                }
                
                @Override
                public int getBatchSize() {
                    return records.size();
                }
            });
    }
    
    /**
     * Execute analytical query
     */
    public <T> List<T> query(String sql, 
            Class<T> resultType, 
            Object... params) {
        try {
            return jdbcTemplate.query(
                sql, 
                new BeanPropertyRowMapper<>(resultType), 
                params
            );
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute analytical query with custom row mapper
     */
    public <T> List<T> query(String sql, 
            RowMapper<T> rowMapper, 
            Object... params) {
        try {
            return jdbcTemplate.query(sql, rowMapper, params);
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute mutation query (INSERT, UPDATE, DELETE)
     */
    public int update(String sql, Object... params) {
        try {
            return jdbcTemplate.update(sql, params);
        } catch (DataAccessException e) {
            log.error("Error executing update: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute update", e);
        }
    }
    
    private String createInsertSql(String tableName, String[] columns) {
        StringBuilder sql = new StringBuilder()
            .append("INSERT INTO ")
            .append(tableName)
            .append(" (")
            .append(String.join(", ", columns))
            .append(") VALUES (")
            .append(String.join(", ", 
                Collections.nCopies(columns.length, "?")))
            .append(")");
            
        return sql.toString();
    }
}
```

### 4.4 Data Models and Repositories

Example of working with ClickHouse tables:

```java
@Data
@AllArgsConstructor
public class ClickHouseEvent {
    private LocalDateTime timestamp;
    private String eventType;
    private String userId;
    private Map<String, String> properties;
    private Long value;
}

@Repository
@Slf4j
public class ClickHouseEventRepository {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    private static final String CREATE_TABLE_SQL = """
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id String,
            properties Map(String, String),
            value Int64
        )
        ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (timestamp, event_type)
        """;
        
    private static final String INSERT_EVENT_SQL = """
        INSERT INTO events (
            timestamp, event_type, user_id, properties, value
        ) VALUES (?, ?, ?, ?, ?)
        """;
        
    public ClickHouseEventRepository(ClickHouseTemplate clickHouseTemplate) {
        this.clickHouseTemplate = clickHouseTemplate;
        initializeTable();
    }
    
    private void initializeTable() {
        try {
            clickHouseTemplate.update(CREATE_TABLE_SQL);
        } catch (} catch (Exception e) {
            log.error("Failed to initialize events table", e);
            throw new ClickHouseInitializationException(
                "Could not create events table", e);
        }
    }
    
    /**
     * Stores a single event in ClickHouse. The method ensures proper handling of
     * the Map type for properties and uses prepared statements for efficiency.
     */
    public void storeEvent(ClickHouseEvent event) {
        try {
            clickHouseTemplate.update(
                INSERT_EVENT_SQL,
                event.getTimestamp(),
                event.getEventType(),
                event.getUserId(),
                convertPropertiesToClickHouseFormat(event.getProperties()),
                event.getValue()
            );
        } catch (Exception e) {
            log.error("Failed to store event: {}", event, e);
            throw new ClickHouseOperationException(
                "Could not store event", e);
        }
    }
    
    /**
     * Performs batch insertion of events for better performance when dealing with
     * multiple records. This method uses the ClickHouse native format for efficient
     * data loading.
     */
    public void storeBatchEvents(List<ClickHouseEvent> events) {
        try {
            clickHouseTemplate.batchInsert(
                "events",
                events,
                new ClickHouseInsertMapper<ClickHouseEvent>() {
                    @Override
                    public String[] getColumns() {
                        return new String[] {
                            "timestamp", "event_type", "user_id", 
                            "properties", "value"
                        };
                    }
                    
                    @Override
                    public void mapRow(PreparedStatement ps, ClickHouseEvent event) 
                            throws SQLException {
                        ps.setObject(1, event.getTimestamp());
                        ps.setString(2, event.getEventType());
                        ps.setString(3, event.getUserId());
                        ps.setObject(4, convertPropertiesToClickHouseFormat(
                            event.getProperties()));
                        ps.setLong(5, event.getValue());
                    }
                }
            );
        } catch (Exception e) {
            log.error("Failed to store batch events", e);
            throw new ClickHouseOperationException(
                "Could not store batch events", e);
        }
    }
    
    /**
     * Queries events within a specific time range. This method demonstrates
     * how to work with ClickHouse's powerful date and time functions.
     */
    public List<ClickHouseEvent> queryEventsByTimeRange(
            LocalDateTime startTime,
            LocalDateTime endTime,
            String eventType) {
        String sql = """
            SELECT
                timestamp,
                event_type,
                user_id,
                properties,
                value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
                AND event_type = ?
            ORDER BY timestamp
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new ClickHouseEvent(
                rs.getObject("timestamp", LocalDateTime.class),
                rs.getString("event_type"),
                rs.getString("user_id"),
                convertClickHouseMapToJava(rs.getString("properties")),
                rs.getLong("value")
            ),
            startTime,
            endTime,
            eventType
        );
    }
    
    /**
     * Performs an analytical query to aggregate events. This demonstrates
     * ClickHouse's analytical capabilities and performance optimizations.
     */
    public Map<String, Long> aggregateEventsByType(
            LocalDateTime startTime,
            LocalDateTime endTime) {
        String sql = """
            SELECT
                event_type,
                count() as event_count,
                sum(value) as total_value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY event_type
            WITH TOTALS
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new AbstractMap.SimpleEntry<>(
                rs.getString("event_type"),
                rs.getLong("event_count")
            ),
            startTime,
            endTime
        ).stream().collect(Collectors.toMap(
            Map.Entry::getKey,
            Map.Entry::getValue
        ));
    }
    
    private String convertPropertiesToClickHouseFormat(
            Map<String, String> properties) {
        // Convert Java Map to ClickHouse Map format
        return properties.entrySet().stream()
            .map(e -> String.format("'%s':'%s'",
                e.getKey().replace("'", "\\'"),
                e.getValue().replace("'", "\\'"))
            )
            .collect(Collectors.joining(",", "{", "}"));
    }
    
    private Map<String, String> convertClickHouseMapToJava(String mapStr) {
        // Convert ClickHouse Map string format to Java Map
        if (mapStr == null || mapStr.length() < 2) {
            return Collections.emptyMap();
        }
        
        String content = mapStr.substring(1, mapStr.length() - 1);
        Map<String, String> result = new HashMap<>();
        
        Pattern pattern = Pattern.compile("'([^']*)'\\s*:\\s*'([^']*)'");
        Matcher matcher = pattern.matcher(content);
        
        while (matcher.find()) {
            result.put(
                matcher.group(1).replace("\\'", "'"),
                matcher.group(2).replace("\\'", "'")
            );
        }
        
        return result;
    }
}
```

## 5. Advanced Features and Optimizations

### 5.1 Query Optimization Service

Let's create a service that helps optimize ClickHouse queries for better performance:

```java
@Service
@Slf4j
public class ClickHouseQueryOptimizer {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Analyzes and optimizes a given query using ClickHouse's explain functionality.
     * This helps understand query execution plans and potential optimizations.
     */
    public String analyzeQuery(String query) {
        String explainQuery = String.format(
            "EXPLAIN indexes = 1, actions = 1, description = 1 %s",
            query
        );
        
        try {
            return clickHouseTemplate.query(
                explainQuery,
                (rs, rowNum) -> rs.getString(1)
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze query: {}", query, e);
            throw new ClickHouseQueryException(
                "Query analysis failed", e);
        }
    }
    
    /**
     * Suggests optimal table settings based on data patterns and query types.
     * This helps improve query performance through better table design.
     */
    public TableOptimizationRecommendations analyzeTableStructure(
            String tableName) {
        String analyzeQuery = """
            SELECT
                count() as total_rows,
                max(bytes_on_disk) as disk_usage,
                max(compression_ratio) as compression_ratio
            FROM system.parts
            WHERE table = ?
            """;
            
        try {
            return clickHouseTemplate.query(
                analyzeQuery,
                (rs, rowNum) -> new TableOptimizationRecommendations(
                    rs.getLong("total_rows"),
                    rs.getLong("disk_usage"),
                    rs.getDouble("compression_ratio")
                ),
                tableName
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze table: {}", tableName, e);
            throw new ClickHouseQueryException(
                "Table analysis failed", e);
        }
    }
}

@Data
@AllArgsConstructor
public class TableOptimizationRecommendations {
    private long totalRows;
    private long diskUsage;
    private double compressionRatio;
    
    public String getPartitioningRecommendation() {
        if (totalRows > 100_000_000) {
            return "Consider partitioning by date for better query performance";
        }
        return "Current partitioning scheme appears adequate";
    }
    
    public String getCompressionRecommendation() {
        if (compressionRatio < 3.0) {
            return "Consider using more aggressive compression settings";
        }
        return "Compression ratio is within acceptable range";
    }
}
```

### 5.2 Monitoring and Metrics

Implementing comprehensive monitoring for ClickHouse:

```java
@Configuration
public class ClickHouseMonitoringConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public ClickHouseMetricsCollector metricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        return new ClickHouseMetricsCollector(registry, clickHouseTemplate);
    }
}

@Component
@Slf4j
public class ClickHouseMetricsCollector {
    
    private final MeterRegistry registry;
    private final ClickHouseTemplate clickHouseTemplate;
    private final Timer queryTimer;
    private final Timer insertTimer;
    private final Counter errorCounter;
    
    public ClickHouseMetricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        this.registry = registry;
        this.clickHouseTemplate = clickHouseTemplate;
        
        // Initialize metrics
        this.queryTimer = Timer.builder("clickhouse.query.duration")
            .description("Time taken for ClickHouse queries")
            .register(registry);
            
        this.insertTimer = Timer.builder("clickhouse.insert.duration")
            .description("Time taken for ClickHouse inserts")
            .register(registry);
            
        this.errorCounter = Counter.builder("clickhouse.errors")
            .description("Number of ClickHouse errors")
            .register(registry);
            
        // Start collecting system metrics
        scheduleMetricsCollection();
    }
    
    private void scheduleMetricsCollection() {
        Executors.newSingleThreadScheduledExecutor()
            .scheduleAtFixedRate(
                this::collectSystemMetrics,
                0,
                1,
                TimeUnit.MINUTES
            );
    }
    
    private void collectSystemMetrics() {
        try {
            // Collect memory metrics
            String memoryQuery = """
                SELECT
                    metric,
                    value
                FROM system.metrics
                WHERE metric IN ('MemoryTracking', 'MemoryTrackingInBackgroundProcesses')
                """;
                
            clickHouseTemplate.query(
                memoryQuery,
                (rs, rowNum) -> {
                    String metric = rs.getString("metric");
                    double value = rs.getDouble("value");
                    registry.gauge(
                        "clickhouse.memory." + metric.toLowerCase(),
                        value
                    );
                    return null;
                }
            );
            
            // Collect query metrics
            String queryMetricsQuery = """
                SELECT
                    event_type,
                    count() as count
                FROM system.query_log
                WHERE event_time >= now() - INTERVAL 1 MINUTE
                GROUP BY event_type
                """;
                
            clickHouseTemplate.query(
                queryMetricsQuery,
                (rs, rowNum) -> {
                    String eventType = rs.getString("event_type");
                    long count = rs.getLong("count");
                    registry.counter(
                        "clickhouse.queries",
                        "type",
                        eventType
                    ).increment(count);
                    return null;
                }
            );
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            errorCounter.increment();
        }
    }
    
    public Timer.Sample startQueryTimer() {
        return Timer.start(registry);
    }
    
    public void stopQueryTimer(Timer.Sample sample) {
        sample.stop(queryTimer);
    }
    
    public Timer.Sample startInsertTimer() {
        return Timer.start(registry);
    }
    
    public void stopInsertTimer(Timer.Sample sample) {
        sample.stop(insertTimer);
    }
    
    public void recordError(String errorType) {
        errorCounter.increment();
        registry.counter(
            "clickhouse.errors",
            "type",
            errorType
        ).increment();
    }
}
```

### 5.3 Data Lifecycle Management

Implementing data lifecycle management for ClickHouse tables:

```java
@Service
@Slf4j
public class ClickHouseDataLifecycleManager {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Manages the lifecycle of data in ClickHouse tables, including
     * partitioning, TTL, and data archiving strategies.
     */
    public void configureTableLifecycle(
            String tableName,
            Period retentionPeriod,
            String partitioningColumn) {
        try {
            // Configure TTL for data retention
            String ttlQuery = String.format("""
                ALTER TABLE %s
                MODIFY TTL
                    %s + INTERVAL %d MONTH
                    DELETE
                """,
                tableName,
                partitioningColumn,
                retentionPeriod.toTotalMonths()
            );
            
            clickHouseTemplate.update(ttlQuery);
            
            // Configure storage policy for cold data
            String storagePolicyQuery = String.format("""
                ALTER TABLE %s
                MODIFY SETTING storage_policy = 'tiered'
                """,
                tableName
            );
            
            clickHouseTemplate.update(storagePolicyQuery);
            
            log.info("Configured lifecycle management for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to configure table lifecycle: {}", 
                tableName, e);
            throw new ClickHouseOperationException(
                "Lifecycle configuration failed", e);
        }
    }
    
    /**
     * Optimizes table storage by merging partitions and reorganizing data.
     * This helps maintain query performance over time.
     */
    public void optimizeTable(String tableName) {
        try {
            String optimizeQuery = String.format(
                "OPTIMIZE TABLE %s FINAL",
                tableName
            );
            
            clickHouseTemplate.update(optimizeQuery);
            log.info("Optimized table: {}", tableName);
            
        } catch (Exception e) {
            log.error("Failed to optimize table: {}", tableName, e);
            throw new ClickHouseOperationException(
                "Table optimization failed", e);
        }
    }
    
    /**
     * Archives old data to a different storage tier while maintaining
     * accessibility for historical analysis.
     */
    public void archiveOldData(
            String sourceTable,
            String archiveTable,
            LocalDateTime cutoffDate) {
        try {
            // Move data to archive table
            String archiveQuery = String.format("""
                INSERT INTO %s
                SELECT *
                FROM %s
                WHERE timestamp < ?
                """,
                archiveTable,
                sourceTable
            );
            
            clickHouseTemplate.update(archiveQuery, cutoffDate);
            
            // Delete archived data from source table
            String cleanupQuery = String.format("""
                ALTER TABLE %s
                DELETE WHERE timestamp < ?
                """,
                sourceTable
            );
            
            clickHouseTemplate.update# ClickHouse Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ClickHouse Architecture

Before diving into configuration details, let's understand why ClickHouse is particularly well-suited for analytical workloads and how its architecture supports high-performance analytics.

ClickHouse is a column-oriented database management system designed for online analytical processing (OLAP). Its architecture is optimized for:
- Fast query processing on large datasets
- Real-time data ingestion
- Efficient compression
- Distributed query execution
- Fault tolerance and high availability

### 1.1 Key Concepts

Several fundamental concepts make ClickHouse powerful:

1. Column-oriented Storage: Data is stored by columns rather than rows, enabling:
   - Better compression ratios
   - Faster analytical queries
   - Efficient scanning of specific columns

2. Data Types and Encodings:
   - Specialized data types for analytics
   - Custom compression methods per column
   - Efficient numeric and string encodings

3. Distributed Architecture:
   - Sharding for horizontal scaling
   - Replication for fault tolerance
   - Distributed query execution

## 2. Single Node Setup

Let's start with a basic single-node ClickHouse installation using Docker, which is perfect for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
      - "9009:9009"   # Internal port for replication
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config:/etc/clickhouse-server/config.d
      - ./users:/etc/clickhouse-server/users.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

volumes:
  clickhouse_data:
```

### 2.2 Basic Configuration

The configuration consists of several XML files. Here's the basic setup:

```xml
<!-- config.xml -->
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>

    <memory>
        <!-- Recommended to set to 80% of available RAM -->
        <max_server_memory_usage>8589934592</max_server_memory_usage>
        <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
    </memory>

    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                </volumes>
            </default>
        </policies>
    </storage_configuration>

    <!-- Performance settings -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <default_max_threads>8</default_max_threads>
</clickhouse>

<!-- users.xml -->
<?xml version="1.0"?>
<clickhouse>
    <users>
        <default>
            <password>default_password</password>
            <networks>
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
        </default>
    </users>

    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by>
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort>
        </default>
    </profiles>

    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

## 3. Distributed Setup

For production environments, we'll configure a ClickHouse cluster with multiple nodes for high availability and better performance.

### 3.1 Multi-node Cluster Configuration

```yaml
version: '3.8'
services:
  clickhouse1:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse1_data:/var/lib/clickhouse
      - ./config/node1:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=1

  clickhouse2:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - clickhouse2_data:/var/lib/clickhouse
      - ./config/node2:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=2

  clickhouse3:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - clickhouse3_data:/var/lib/clickhouse
      - ./config/node3:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=1

  clickhouse4:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse4
    ports:
      - "8126:8123"
      - "9003:9000"
    volumes:
      - clickhouse4_data:/var/lib/clickhouse
      - ./config/node4:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=2

volumes:
  clickhouse1_data:
  clickhouse2_data:
  clickhouse3_data:
  clickhouse4_data:
```

### 3.2 Cluster Configuration

Let's configure the cluster with two shards and two replicas per shard:

```xml
<!-- metrika.xml -->
<?xml version="1.0"?>
<clickhouse>
    <remote_servers>
        <analytics_cluster>
            <!-- First shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Second shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </analytics_cluster>
    </remote_servers>

    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <macros>
        <shard>{CLICKHOUSE_SHARD}</shard>
        <replica>{CLICKHOUSE_REPLICA}</replica>
    </macros>
</clickhouse>
```

## 4. Spring Boot Integration

Now let's integrate ClickHouse with Spring Boot for seamless data access and management.

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    
    <!-- ClickHouse JDBC Driver -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ClickHouse Configuration

Let's create a comprehensive configuration that supports both single-node and cluster setups:

```java
@Configuration
@EnableTransactionManagement
public class ClickHouseConfig {
    
    @Bean
    @ConfigurationProperties(prefix = "clickhouse.datasource")
    public DataSourceProperties clickHouseDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    public DataSource clickHouseDataSource() {
        ClickHouseDataSource dataSource = new ClickHouseDataSource(
            createClickHouseProperties());
            
        // Configure connection pooling
        HikariConfig config = new HikariConfig();
        config.setDataSource(dataSource);
        config.setMaximumPoolSize(10);
        config.setMinimumIdle(5);
        config.setIdleTimeout(300000);
        config.setConnectionTimeout(20000);
        
        return new HikariDataSource(config);
    }
    
    private ClickHouseProperties createClickHouseProperties() {
        ClickHouseProperties properties = new ClickHouseProperties();
        properties.setUser("default");
        properties.setPassword("default_password");
        properties.setDatabase("default");
        
        // Performance settings
        properties.setUseServerTimeZone(true);
        properties.setUseTimeZone("UTC");
        properties.setUseServerTimeZoneForDates(true);
        
        // Connection settings
        properties.setSocketTimeout(30000);
        properties.setConnectionTimeout(10000);
        
        // Buffer settings
        properties.setBufferSize(65536);
        
        return properties;
    }
    
    @Bean
    public ClickHouseTemplate clickHouseTemplate(
            DataSource clickHouseDataSource) {
        return new ClickHouseTemplate(clickHouseDataSource);
    }
}
```

### 4.3 ClickHouse Template

Let's create a template class for executing queries:

```java
@Component
@Slf4j
public class ClickHouseTemplate {
    
    private final JdbcTemplate jdbcTemplate;
    
    public ClickHouseTemplate(DataSource dataSource) {
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    /**
     * Execute a batch insert operation
     */
    public <T> void batchInsert(String tableName, 
            List<T> records, 
            ClickHouseInsertMapper<T> mapper) {
        String insertSql = createInsertSql(tableName, mapper.getColumns());
        
        jdbcTemplate.batchUpdate(insertSql, 
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                        throws SQLException {
                    mapper.mapRow(ps, records.get(i));
                }
                
                @Override
                public int getBatchSize() {
                    return records.size();
                }
            });
    }
    
    /**
     * Execute analytical query
     */
    public <T> List<T> query(String sql, 
            Class<T> resultType, 
            Object... params) {
        try {
            return jdbcTemplate.query(
                sql, 
                new BeanPropertyRowMapper<>(resultType), 
                params
            );
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute analytical query with custom row mapper
     */
    public <T> List<T> query(String sql, 
            RowMapper<T> rowMapper, 
            Object... params) {
        try {
            return jdbcTemplate.query(sql, rowMapper, params);
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute mutation query (INSERT, UPDATE, DELETE)
     */
    public int update(String sql, Object... params) {
        try {
            return jdbcTemplate.update(sql, params);
        } catch (DataAccessException e) {
            log.error("Error executing update: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute update", e);
        }
    }
    
    private String createInsertSql(String tableName, String[] columns) {
        StringBuilder sql = new StringBuilder()
            .append("INSERT INTO ")
            .append(tableName)
            .append(" (")
            .append(String.join(", ", columns))
            .append(") VALUES (")
            .append(String.join(", ", 
                Collections.nCopies(columns.length, "?")))
            .append(")");
            
        return sql.toString();
    }
}
```

### 4.4 Data Models and Repositories

Example of working with ClickHouse tables:

```java
@Data
@AllArgsConstructor
public class ClickHouseEvent {
    private LocalDateTime timestamp;
    private String eventType;
    private String userId;
    private Map<String, String> properties;
    private Long value;
}

@Repository
@Slf4j
public class ClickHouseEventRepository {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    private static final String CREATE_TABLE_SQL = """
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id String,
            properties Map(String, String),
            value Int64
        )
        ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (timestamp, event_type)
        """;
        
    private static final String INSERT_EVENT_SQL = """
        INSERT INTO events (
            timestamp, event_type, user_id, properties, value
        ) VALUES (?, ?, ?, ?, ?)
        """;
        
    public ClickHouseEventRepository(ClickHouseTemplate clickHouseTemplate) {
        this.clickHouseTemplate = clickHouseTemplate;
        initializeTable();
    }
    
    private void initializeTable() {
        try {
            clickHouseTemplate.update(CREATE_TABLE_SQL);
        } catch (clickHouseTemplate.update(cleanupQuery, cutoffDate);
            
            log.info("Archived data from {} to {} before {}", 
                sourceTable, archiveTable, cutoffDate);
                
        } catch (Exception e) {
            log.error("Failed to archive data for table: {}", 
                sourceTable, e);
            throw new ClickHouseOperationException(
                "Data archiving failed", e);
        }
    }
}

## 6. Production Best Practices

### 6.1 Performance Optimization Strategies

When deploying ClickHouse in production, several key optimization strategies can significantly improve performance. Let's implement a comprehensive optimization service:

```java
@Service
@Slf4j
public class ClickHouseOptimizationService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final ClickHouseMetricsCollector metricsCollector;
    
    /**
     * Analyzes and optimizes table structure based on query patterns and data volume.
     * This helps maintain high query performance as data grows.
     */
    public TableOptimizationPlan analyzeTablePerformance(String tableName) {
        TableOptimizationPlan plan = new TableOptimizationPlan();
        
        // Analyze table statistics
        String statsQuery = """
            SELECT
                table,
                total_rows,
                total_bytes,
                primary_key_bytes_in_memory,
                bytes_on_disk
            FROM system.tables
            WHERE name = ?
            """;
            
        clickHouseTemplate.query(
            statsQuery,
            (rs) -> {
                plan.setTotalRows(rs.getLong("total_rows"));
                plan.setTotalBytes(rs.getLong("total_bytes"));
                plan.setPrimaryKeyMemory(
                    rs.getLong("primary_key_bytes_in_memory"));
                plan.setDiskUsage(rs.getLong("bytes_on_disk"));
                return null;
            },
            tableName
        );
        
        // Analyze query patterns
        String queryPatternsQuery = """
            SELECT
                query_kind,
                count() as query_count,
                avg(query_duration_ms) as avg_duration,
                max(read_rows) as max_rows_read,
                max(memory_usage) as max_memory
            FROM system.query_log
            WHERE table = ?
                AND event_time >= now() - INTERVAL 1 DAY
            GROUP BY query_kind
            """;
            
        clickHouseTemplate.query(
            queryPatternsQuery,
            (rs) -> {
                QueryPattern pattern = new QueryPattern(
                    rs.getString("query_kind"),
                    rs.getLong("query_count"),
                    rs.getDouble("avg_duration"),
                    rs.getLong("max_rows_read"),
                    rs.getLong("max_memory")
                );
                plan.addQueryPattern(pattern);
                return null;
            },
            tableName
        );
        
        // Generate optimization recommendations
        plan.generateRecommendations();
        
        return plan;
    }
    
    /**
     * Implements the recommended optimizations for a table.
     * This method carefully applies changes while ensuring data consistency.
     */
    public void applyOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        try {
            // Apply index optimizations
            if (plan.isIndexOptimizationNeeded()) {
                applyIndexOptimizations(tableName, plan);
            }
            
            // Optimize partitioning
            if (plan.isPartitioningOptimizationNeeded()) {
                optimizePartitioning(tableName, plan);
            }
            
            // Adjust compression settings
            if (plan.isCompressionOptimizationNeeded()) {
                optimizeCompression(tableName, plan);
            }
            
            log.info("Successfully applied optimizations for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to apply optimizations for table: {}", 
                tableName, e);
            throw new ClickHouseOptimizationException(
                "Optimization application failed", e);
        }
    }
    
    private void applyIndexOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply skip indexes for frequently filtered columns
        for (String column : plan.getFrequentlyFilteredColumns()) {
            String indexQuery = String.format("""
                ALTER TABLE %s
                ADD INDEX %s_idx %s TYPE minmax GRANULARITY 3
                """,
                tableName,
                column,
                column
            );
            
            clickHouseTemplate.update(indexQuery);
        }
        
        // Optimize primary key if needed
        if (plan.isPrimaryKeyOptimizationNeeded()) {
            String primaryKeyQuery = String.format("""
                ALTER TABLE %s
                MODIFY ORDER BY %s
                """,
                tableName,
                String.join(", ", plan.getRecommendedPrimaryKeyColumns())
            );
            
            clickHouseTemplate.update(primaryKeyQuery);
        }
    }
    
    private void optimizePartitioning(
            String tableName, 
            TableOptimizationPlan plan) {
        if (plan.getTotalRows() > 100_000_000) {
            // For large tables, implement more granular partitioning
            String partitioningQuery = String.format("""
                ALTER TABLE %s
                MODIFY PARTITION BY toYYYYMM(timestamp)
                """,
                tableName
            );
            
            clickHouseTemplate.update(partitioningQuery);
        }
    }
    
    private void optimizeCompression(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply column-specific compression settings
        for (Map.Entry<String, String> entry : 
                plan.getCompressionRecommendations().entrySet()) {
            String compressionQuery = String.format("""
                ALTER TABLE %s
                MODIFY COLUMN %s %s CODEC(%s)
                """,
                tableName,
                entry.getKey(),
                getColumnType(tableName, entry.getKey()),
                entry.getValue()
            );
            
            clickHouseTemplate.update(compressionQuery);
        }
    }
    
    private String getColumnType(String tableName, String columnName) {
        String query = String.format("""
            SELECT type
            FROM system.columns
            WHERE table = ? AND name = ?
            """);
            
        return clickHouseTemplate.query(
            query,
            (rs, rowNum) -> rs.getString("type"),
            tableName,
            columnName
        ).get(0);
    }
}

@Data
public class TableOptimizationPlan {
    private long totalRows;
    private long totalBytes;
    private long primaryKeyMemory;
    private long diskUsage;
    private List<QueryPattern> queryPatterns = new ArrayList<>();
    private List<String> recommendedPrimaryKeyColumns = new ArrayList<>();
    private Map<String, String> compressionRecommendations = new HashMap<>();
    private Set<String> frequentlyFilteredColumns = new HashSet<>();
    
    public void generateRecommendations() {
        analyzeQueryPatterns();
        generateCompressionRecommendations();
        optimizePrimaryKey();
    }
    
    private void analyzeQueryPatterns() {
        // Analyze query patterns to identify optimization opportunities
        queryPatterns.forEach(pattern -> {
            if (pattern.getQueryCount() > 1000 && 
                    pattern.getAvgDuration() > 1000) {
                // High-frequency, slow queries need optimization
                analyzeQueryForOptimization(pattern);
            }
        });
    }
    
    private void analyzeQueryForOptimization(QueryPattern pattern) {
        // Extract frequently filtered columns from query pattern
        // This is a simplified example - in practice, you would
        // implement more sophisticated query analysis
        if (pattern.getMaxRowsRead() > totalRows * 0.5) {
            // Query is scanning too much data, might need better indexing
            extractFilterColumns(pattern).forEach(column ->
                frequentlyFilteredColumns.add(column)
            );
        }
    }
    
    private Set<String> extractFilterColumns(QueryPattern pattern) {
        // In practice, implement query parsing to extract filter columns
        return new HashSet<>();
    }
    
    private void generateCompressionRecommendations() {
        // Generate compression recommendations based on data patterns
        // This is a simplified example - in practice, you would analyze
        // actual data patterns and column value distributions
        queryPatterns.forEach(pattern -> {
            if (pattern.getMaxMemory() > 1024 * 1024 * 1024) {
                // High memory usage queries might benefit from better compression
                recommendHighCompressionForLargeColumns(pattern);
            }
        });
    }
    
    private void recommendHighCompressionForLargeColumns(QueryPattern pattern) {
        // Add compression recommendations for columns with high memory usage
        // In practice, you would analyze column-specific characteristics
        compressionRecommendations.put(
            "high_cardinality_column", 
            "ZSTD(3)"
        );
        compressionRecommendations.put(
            "low_cardinality_column",
            "LowCardinality"
        );
    }
    
    private void optimizePrimaryKey() {
        // Analyze query patterns to recommend optimal primary key order
        // This is a simplified example - in practice, you would analyze
        // actual query access patterns and data distribution
        if (!frequentlyFilteredColumns.isEmpty()) {
            recommendedPrimaryKeyColumns.addAll(frequentlyFilteredColumns);
        }
    }
}

@Data
@AllArgsConstructor
public class QueryPattern {
    private String queryKind;
    private long queryCount;
    private double avgDuration;
    private long maxRowsRead;
    private long maxMemory;
}
```

### 6.2 Fault Tolerance and High Availability

Implementing robust fault tolerance mechanisms for ClickHouse deployments:

```java
@Service
@Slf4j
public class ClickHouseHighAvailabilityService {
    
    private final List<ClickHouseTemplate> clickHouseTemplates;
    private final LoadBalancer loadBalancer;
    private final CircuitBreaker circuitBreaker;
    
    /**
     * Executes queries with automatic failover and retry mechanisms.
     * This ensures continuous operation even when some nodes are unavailable.
     */
    public <T> T executeWithFailover(ClickHouseOperation<T> operation) {
        Exception lastException = null;
        
        for (int attempt = 0; attempt < 3; attempt++) {
            ClickHouseTemplate template = loadBalancer.getNextAvailable();
            
            if (template == null) {
                throw new NoAvailableNodesException(
                    "No ClickHouse nodes available");
            }
            
            try {
                if (!circuitBreaker.isAllowed()) {
                    continue;
                }
                
                T result = operation.execute(template);
                circuitBreaker.recordSuccess();
                return result;
                
            } catch (Exception e) {
                lastException = e;
                circuitBreaker.recordFailure();
                loadBalancer.markNodeAsFailed(template);
                
                log.warn("Query execution failed on attempt {}: {}", 
                    attempt + 1, e.getMessage());
            }
        }
        
        throw new ClickHouseFailoverException(
            "All failover attempts exhausted", lastException);
    }
}

@Component
@Slf4j
public class LoadBalancer {
    private final List<ClickHouseTemplate> availableNodes;
    private final AtomicInteger currentIndex = new AtomicInteger(0);
    private final Map<ClickHouseTemplate, HealthStatus> nodeStatus;
    
    public LoadBalancer(List<ClickHouseTemplate> nodes) {
        this.availableNodes = new CopyOnWriteArrayList<>(nodes);
        this.nodeStatus = new ConcurrentHashMap<>();
        
        nodes.forEach(node -> 
            nodeStatus.put(node, new HealthStatus()));
            
        // Start health check schedule
        startHealthChecks();
    }
    
    public ClickHouseTemplate getNextAvailable() {
        int attempts = availableNodes.size();
        
        while (attempts > 0) {
            ClickHouseTemplate node = availableNodes.get(
                currentIndex.getAndIncrement() % availableNodes.size()
            );
            
            if (isNodeHealthy(node)) {
                return node;
            }
            
            attempts--;
        }
        
        return null;
    }
    
    public void markNodeAsFailed(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        status.recordFailure();
        
        if (status.getConsecutiveFailures() >= 3) {
            availableNodes.remove(node);
            log.warn("Removed unhealthy node from rotation");
        }
    }
    
    private boolean isNodeHealthy(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        return status.isHealthy() && checkNodeConnection(node);
    }
    
    private boolean checkNodeConnection(ClickHouseTemplate node) {
        try {
            node.query("SELECT 1", (rs, rowNum) -> 1);
            return true;
        } catch (Exception e) {
            log.warn("Node health check failed: {}", e.getMessage());
            return false;
        }
    }
    
    private void startHealthChecks() {
        ScheduledExecutorService executor = 
            Executors.newSingleThreadScheduledExecutor();
            
        executor.scheduleAtFixedRate(
            this::performHealthChecks,
            0,
            30,
            TimeUnit.SECONDS
        );
    }
    
    private void performHealthChecks() {
        nodeStatus.forEach((node, status) -> {
            if (checkNodeConnection(node)) {
                status.recordSuccess();
                if (!availableNodes.contains(node)) {
                    availableNodes.add(node);
                    log.info("Restored healthy node to rotation");
                }
            } else {
                markNodeAsFailed(node);
            }
        });
    }
}

@Data
public class HealthStatus {
    private AtomicInteger consecutiveFailures = new AtomicInteger(0);
    private AtomicLong lastSuccess = new AtomicLong(0);
    
    public void recordSuccess() {
        consecutiveFailures.set(0);
        lastSuccess.set(System.currentTimeMillis());
    }
    
    public void recordFailure() {
        consecutiveFailures.incrementAndGet();
    }
    
    public boolean isHealthy() {
        return consecutiveFailures.get() < 3 &&
            (System.currentTimeMillis() - lastSuccess.get()) < 60000;
    }
}
```

### 6.3 Backup and Recovery

Implementing comprehensive backup and recovery mechanisms:

```java
@Service
@Slf4j
public class ClickHouseBackupService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final Path backupDirectory;
    
    /**
     * Creates a consistent backup of a ClickHouse table or database.
     * This ensures data can be recovered in case of failures.
     */
    public void createBackup(String database, String table) {
        String backupPath = generateBackupPath(database, table);
        
        try {
            // Freeze        } catch (Exception e) {
            log.error("Failed to initialize events table", e);
            throw new ClickHouseInitializationException(
                "Could not create events table", e);
        }
    }
    
    /**
     * Stores a single event in ClickHouse. The method ensures proper handling of
     * the Map type for properties and uses prepared statements for efficiency.
     */
    public void storeEvent(ClickHouseEvent event) {
        try {
            clickHouseTemplate.update(
                INSERT_EVENT_SQL,
                event.getTimestamp(),
                event.getEventType(),
                event.getUserId(),
                convertPropertiesToClickHouseFormat(event.getProperties()),
                event.getValue()
            );
        } catch (Exception e) {
            log.error("Failed to store event: {}", event, e);
            throw new ClickHouseOperationException(
                "Could not store event", e);
        }
    }
    
    /**
     * Performs batch insertion of events for better performance when dealing with
     * multiple records. This method uses the ClickHouse native format for efficient
     * data loading.
     */
    public void storeBatchEvents(List<ClickHouseEvent> events) {
        try {
            clickHouseTemplate.batchInsert(
                "events",
                events,
                new ClickHouseInsertMapper<ClickHouseEvent>() {
                    @Override
                    public String[] getColumns() {
                        return new String[] {
                            "timestamp", "event_type", "user_id", 
                            "properties", "value"
                        };
                    }
                    
                    @Override
                    public void mapRow(PreparedStatement ps, ClickHouseEvent event) 
                            throws SQLException {
                        ps.setObject(1, event.getTimestamp());
                        ps.setString(2, event.getEventType());
                        ps.setString(3, event.getUserId());
                        ps.setObject(4, convertPropertiesToClickHouseFormat(
                            event.getProperties()));
                        ps.setLong(5, event.getValue());
                    }
                }
            );
        } catch (Exception e) {
            log.error("Failed to store batch events", e);
            throw new ClickHouseOperationException(
                "Could not store batch events", e);
        }
    }
    
    /**
     * Queries events within a specific time range. This method demonstrates
     * how to work with ClickHouse's powerful date and time functions.
     */
    public List<ClickHouseEvent> queryEventsByTimeRange(
            LocalDateTime startTime,
            LocalDateTime endTime,
            String eventType) {
        String sql = """
            SELECT
                timestamp,
                event_type,
                user_id,
                properties,
                value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
                AND event_type = ?
            ORDER BY timestamp
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new ClickHouseEvent(
                rs.getObject("timestamp", LocalDateTime.class),
                rs.getString("event_type"),
                rs.getString("user_id"),
                convertClickHouseMapToJava(rs.getString("properties")),
                rs.getLong("value")
            ),
            startTime,
            endTime,
            eventType
        );
    }
    
    /**
     * Performs an analytical query to aggregate events. This demonstrates
     * ClickHouse's analytical capabilities and performance optimizations.
     */
    public Map<String, Long> aggregateEventsByType(
            LocalDateTime startTime,
            LocalDateTime endTime) {
        String sql = """
            SELECT
                event_type,
                count() as event_count,
                sum(value) as total_value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY event_type
            WITH TOTALS
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new AbstractMap.SimpleEntry<>(
                rs.getString("event_type"),
                rs.getLong("event_count")
            ),
            startTime,
            endTime
        ).stream().collect(Collectors.toMap(
            Map.Entry::getKey,
            Map.Entry::getValue
        ));
    }
    
    private String convertPropertiesToClickHouseFormat(
            Map<String, String> properties) {
        // Convert Java Map to ClickHouse Map format
        return properties.entrySet().stream()
            .map(e -> String.format("'%s':'%s'",
                e.getKey().replace("'", "\\'"),
                e.getValue().replace("'", "\\'"))
            )
            .collect(Collectors.joining(",", "{", "}"));
    }
    
    private Map<String, String> convertClickHouseMapToJava(String mapStr) {
        // Convert ClickHouse Map string format to Java Map
        if (mapStr == null || mapStr.length() < 2) {
            return Collections.emptyMap();
        }
        
        String content = mapStr.substring(1, mapStr.length() - 1);
        Map<String, String> result = new HashMap<>();
        
        Pattern pattern = Pattern.compile("'([^']*)'\\s*:\\s*'([^']*)'");
        Matcher matcher = pattern.matcher(content);
        
        while (matcher.find()) {
            result.put(
                matcher.group(1).replace("\\'", "'"),
                matcher.group(2).replace("\\'", "'")
            );
        }
        
        return result;
    }
}
```

## 5. Advanced Features and Optimizations

### 5.1 Query Optimization Service

Let's create a service that helps optimize ClickHouse queries for better performance:

```java
@Service
@Slf4j
public class ClickHouseQueryOptimizer {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Analyzes and optimizes a given query using ClickHouse's explain functionality.
     * This helps understand query execution plans and potential optimizations.
     */
    public String analyzeQuery(String query) {
        String explainQuery = String.format(
            "EXPLAIN indexes = 1, actions = 1, description = 1 %s",
            query
        );
        
        try {
            return clickHouseTemplate.query(
                explainQuery,
                (rs, rowNum) -> rs.getString(1)
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze query: {}", query, e);
            throw new ClickHouseQueryException(
                "Query analysis failed", e);
        }
    }
    
    /**
     * Suggests optimal table settings based on data patterns and query types.
     * This helps improve query performance through better table design.
     */
    public TableOptimizationRecommendations analyzeTableStructure(
            String tableName) {
        String analyzeQuery = """
            SELECT
                count() as total_rows,
                max(bytes_on_disk) as disk_usage,
                max(compression_ratio) as compression_ratio
            FROM system.parts
            WHERE table = ?
            """;
            
        try {
            return clickHouseTemplate.query(
                analyzeQuery,
                (rs, rowNum) -> new TableOptimizationRecommendations(
                    rs.getLong("total_rows"),
                    rs.getLong("disk_usage"),
                    rs.getDouble("compression_ratio")
                ),
                tableName
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze table: {}", tableName, e);
            throw new ClickHouseQueryException(
                "Table analysis failed", e);
        }
    }
}

@Data
@AllArgsConstructor
public class TableOptimizationRecommendations {
    private long totalRows;
    private long diskUsage;
    private double compressionRatio;
    
    public String getPartitioningRecommendation() {
        if (totalRows > 100_000_000) {
            return "Consider partitioning by date for better query performance";
        }
        return "Current partitioning scheme appears adequate";
    }
    
    public String getCompressionRecommendation() {
        if (compressionRatio < 3.0) {
            return "Consider using more aggressive compression settings";
        }
        return "Compression ratio is within acceptable range";
    }
}
```

### 5.2 Monitoring and Metrics

Implementing comprehensive monitoring for ClickHouse:

```java
@Configuration
public class ClickHouseMonitoringConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public ClickHouseMetricsCollector metricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        return new ClickHouseMetricsCollector(registry, clickHouseTemplate);
    }
}

@Component
@Slf4j
public class ClickHouseMetricsCollector {
    
    private final MeterRegistry registry;
    private final ClickHouseTemplate clickHouseTemplate;
    private final Timer queryTimer;
    private final Timer insertTimer;
    private final Counter errorCounter;
    
    public ClickHouseMetricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        this.registry = registry;
        this.clickHouseTemplate = clickHouseTemplate;
        
        // Initialize metrics
        this.queryTimer = Timer.builder("clickhouse.query.duration")
            .description("Time taken for ClickHouse queries")
            .register(registry);
            
        this.insertTimer = Timer.builder("clickhouse.insert.duration")
            .description("Time taken for ClickHouse inserts")
            .register(registry);
            
        this.errorCounter = Counter.builder("clickhouse.errors")
            .description("Number of ClickHouse errors")
            .register(registry);
            
        // Start collecting system metrics
        scheduleMetricsCollection();
    }
    
    private void scheduleMetricsCollection() {
        Executors.newSingleThreadScheduledExecutor()
            .scheduleAtFixedRate(
                this::collectSystemMetrics,
                0,
                1,
                TimeUnit.MINUTES
            );
    }
    
    private void collectSystemMetrics() {
        try {
            // Collect memory metrics
            String memoryQuery = """
                SELECT
                    metric,
                    value
                FROM system.metrics
                WHERE metric IN ('MemoryTracking', 'MemoryTrackingInBackgroundProcesses')
                """;
                
            clickHouseTemplate.query(
                memoryQuery,
                (rs, rowNum) -> {
                    String metric = rs.getString("metric");
                    double value = rs.getDouble("value");
                    registry.gauge(
                        "clickhouse.memory." + metric.toLowerCase(),
                        value
                    );
                    return null;
                }
            );
            
            // Collect query metrics
            String queryMetricsQuery = """
                SELECT
                    event_type,
                    count() as count
                FROM system.query_log
                WHERE event_time >= now() - INTERVAL 1 MINUTE
                GROUP BY event_type
                """;
                
            clickHouseTemplate.query(
                queryMetricsQuery,
                (rs, rowNum) -> {
                    String eventType = rs.getString("event_type");
                    long count = rs.getLong("count");
                    registry.counter(
                        "clickhouse.queries",
                        "type",
                        eventType
                    ).increment(count);
                    return null;
                }
            );
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            errorCounter.increment();
        }
    }
    
    public Timer.Sample startQueryTimer() {
        return Timer.start(registry);
    }
    
    public void stopQueryTimer(Timer.Sample sample) {
        sample.stop(queryTimer);
    }
    
    public Timer.Sample startInsertTimer() {
        return Timer.start(registry);
    }
    
    public void stopInsertTimer(Timer.Sample sample) {
        sample.stop(insertTimer);
    }
    
    public void recordError(String errorType) {
        errorCounter.increment();
        registry.counter(
            "clickhouse.errors",
            "type",
            errorType
        ).increment();
    }
}
```

### 5.3 Data Lifecycle Management

Implementing data lifecycle management for ClickHouse tables:

```java
@Service
@Slf4j
public class ClickHouseDataLifecycleManager {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Manages the lifecycle of data in ClickHouse tables, including
     * partitioning, TTL, and data archiving strategies.
     */
    public void configureTableLifecycle(
            String tableName,
            Period retentionPeriod,
            String partitioningColumn) {
        try {
            // Configure TTL for data retention
            String ttlQuery = String.format("""
                ALTER TABLE %s
                MODIFY TTL
                    %s + INTERVAL %d MONTH
                    DELETE
                """,
                tableName,
                partitioningColumn,
                retentionPeriod.toTotalMonths()
            );
            
            clickHouseTemplate.update(ttlQuery);
            
            // Configure storage policy for cold data
            String storagePolicyQuery = String.format("""
                ALTER TABLE %s
                MODIFY SETTING storage_policy = 'tiered'
                """,
                tableName
            );
            
            clickHouseTemplate.update(storagePolicyQuery);
            
            log.info("Configured lifecycle management for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to configure table lifecycle: {}", 
                tableName, e);
            throw new ClickHouseOperationException(
                "Lifecycle configuration failed", e);
        }
    }
    
    /**
     * Optimizes table storage by merging partitions and reorganizing data.
     * This helps maintain query performance over time.
     */
    public void optimizeTable(String tableName) {
        try {
            String optimizeQuery = String.format(
                "OPTIMIZE TABLE %s FINAL",
                tableName
            );
            
            clickHouseTemplate.update(optimizeQuery);
            log.info("Optimized table: {}", tableName);
            
        } catch (Exception e) {
            log.error("Failed to optimize table: {}", tableName, e);
            throw new ClickHouseOperationException(
                "Table optimization failed", e);
        }
    }
    
    /**
     * Archives old data to a different storage tier while maintaining
     * accessibility for historical analysis.
     */
    public void archiveOldData(
            String sourceTable,
            String archiveTable,
            LocalDateTime cutoffDate) {
        try {
            // Move data to archive table
            String archiveQuery = String.format("""
                INSERT INTO %s
                SELECT *
                FROM %s
                WHERE timestamp < ?
                """,
                archiveTable,
                sourceTable
            );
            
            clickHouseTemplate.update(archiveQuery, cutoffDate);
            
            // Delete archived data from source table
            String cleanupQuery = String.format("""
                ALTER TABLE %s
                DELETE WHERE timestamp < ?
                """,
                sourceTable
            );
            
            clickHouseTemplate.update# ClickHouse Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ClickHouse Architecture

Before diving into configuration details, let's understand why ClickHouse is particularly well-suited for analytical workloads and how its architecture supports high-performance analytics.

ClickHouse is a column-oriented database management system designed for online analytical processing (OLAP). Its architecture is optimized for:
- Fast query processing on large datasets
- Real-time data ingestion
- Efficient compression
- Distributed query execution
- Fault tolerance and high availability

### 1.1 Key Concepts

Several fundamental concepts make ClickHouse powerful:

1. Column-oriented Storage: Data is stored by columns rather than rows, enabling:
   - Better compression ratios
   - Faster analytical queries
   - Efficient scanning of specific columns

2. Data Types and Encodings:
   - Specialized data types for analytics
   - Custom compression methods per column
   - Efficient numeric and string encodings

3. Distributed Architecture:
   - Sharding for horizontal scaling
   - Replication for fault tolerance
   - Distributed query execution

## 2. Single Node Setup

Let's start with a basic single-node ClickHouse installation using Docker, which is perfect for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
      - "9009:9009"   # Internal port for replication
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config:/etc/clickhouse-server/config.d
      - ./users:/etc/clickhouse-server/users.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

volumes:
  clickhouse_data:
```

### 2.2 Basic Configuration

The configuration consists of several XML files. Here's the basic setup:

```xml
<!-- config.xml -->
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>

    <memory>
        <!-- Recommended to set to 80% of available RAM -->
        <max_server_memory_usage>8589934592</max_server_memory_usage>
        <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
    </memory>

    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                </volumes>
            </default>
        </policies>
    </storage_configuration>

    <!-- Performance settings -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <default_max_threads>8</default_max_threads>
</clickhouse>

<!-- users.xml -->
<?xml version="1.0"?>
<clickhouse>
    <users>
        <default>
            <password>default_password</password>
            <networks>
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
        </default>
    </users>

    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by>
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort>
        </default>
    </profiles>

    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

## 3. Distributed Setup

For production environments, we'll configure a ClickHouse cluster with multiple nodes for high availability and better performance.

### 3.1 Multi-node Cluster Configuration

```yaml
version: '3.8'
services:
  clickhouse1:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse1_data:/var/lib/clickhouse
      - ./config/node1:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=1

  clickhouse2:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - clickhouse2_data:/var/lib/clickhouse
      - ./config/node2:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=2

  clickhouse3:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - clickhouse3_data:/var/lib/clickhouse
      - ./config/node3:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=1

  clickhouse4:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse4
    ports:
      - "8126:8123"
      - "9003:9000"
    volumes:
      - clickhouse4_data:/var/lib/clickhouse
      - ./config/node4:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=2

volumes:
  clickhouse1_data:
  clickhouse2_data:
  clickhouse3_data:
  clickhouse4_data:
```

### 3.2 Cluster Configuration

Let's configure the cluster with two shards and two replicas per shard:

```xml
<!-- metrika.xml -->
<?xml version="1.0"?>
<clickhouse>
    <remote_servers>
        <analytics_cluster>
            <!-- First shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Second shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </analytics_cluster>
    </remote_servers>

    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <macros>
        <shard>{CLICKHOUSE_SHARD}</shard>
        <replica>{CLICKHOUSE_REPLICA}</replica>
    </macros>
</clickhouse>
```

## 4. Spring Boot Integration

Now let's integrate ClickHouse with Spring Boot for seamless data access and management.

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    
    <!-- ClickHouse JDBC Driver -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ClickHouse Configuration

Let's create a comprehensive configuration that supports both single-node and cluster setups:

```java
@Configuration
@EnableTransactionManagement
public class ClickHouseConfig {
    
    @Bean
    @ConfigurationProperties(prefix = "clickhouse.datasource")
    public DataSourceProperties clickHouseDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    public DataSource clickHouseDataSource() {
        ClickHouseDataSource dataSource = new ClickHouseDataSource(
            createClickHouseProperties());
            
        // Configure connection pooling
        HikariConfig config = new HikariConfig();
        config.setDataSource(dataSource);
        config.setMaximumPoolSize(10);
        config.setMinimumIdle(5);
        config.setIdleTimeout(300000);
        config.setConnectionTimeout(20000);
        
        return new HikariDataSource(config);
    }
    
    private ClickHouseProperties createClickHouseProperties() {
        ClickHouseProperties properties = new ClickHouseProperties();
        properties.setUser("default");
        properties.setPassword("default_password");
        properties.setDatabase("default");
        
        // Performance settings
        properties.setUseServerTimeZone(true);
        properties.setUseTimeZone("UTC");
        properties.setUseServerTimeZoneForDates(true);
        
        // Connection settings
        properties.setSocketTimeout(30000);
        properties.setConnectionTimeout(10000);
        
        // Buffer settings
        properties.setBufferSize(65536);
        
        return properties;
    }
    
    @Bean
    public ClickHouseTemplate clickHouseTemplate(
            DataSource clickHouseDataSource) {
        return new ClickHouseTemplate(clickHouseDataSource);
    }
}
```

### 4.3 ClickHouse Template

Let's create a template class for executing queries:

```java
@Component
@Slf4j
public class ClickHouseTemplate {
    
    private final JdbcTemplate jdbcTemplate;
    
    public ClickHouseTemplate(DataSource dataSource) {
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    /**
     * Execute a batch insert operation
     */
    public <T> void batchInsert(String tableName, 
            List<T> records, 
            ClickHouseInsertMapper<T> mapper) {
        String insertSql = createInsertSql(tableName, mapper.getColumns());
        
        jdbcTemplate.batchUpdate(insertSql, 
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                        throws SQLException {
                    mapper.mapRow(ps, records.get(i));
                }
                
                @Override
                public int getBatchSize() {
                    return records.size();
                }
            });
    }
    
    /**
     * Execute analytical query
     */
    public <T> List<T> query(String sql, 
            Class<T> resultType, 
            Object... params) {
        try {
            return jdbcTemplate.query(
                sql, 
                new BeanPropertyRowMapper<>(resultType), 
                params
            );
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute analytical query with custom row mapper
     */
    public <T> List<T> query(String sql, 
            RowMapper<T> rowMapper, 
            Object... params) {
        try {
            return jdbcTemplate.query(sql, rowMapper, params);
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute mutation query (INSERT, UPDATE, DELETE)
     */
    public int update(String sql, Object... params) {
        try {
            return jdbcTemplate.update(sql, params);
        } catch (DataAccessException e) {
            log.error("Error executing update: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute update", e);
        }
    }
    
    private String createInsertSql(String tableName, String[] columns) {
        StringBuilder sql = new StringBuilder()
            .append("INSERT INTO ")
            .append(tableName)
            .append(" (")
            .append(String.join(", ", columns))
            .append(") VALUES (")
            .append(String.join(", ", 
                Collections.nCopies(columns.length, "?")))
            .append(")");
            
        return sql.toString();
    }
}
```

### 4.4 Data Models and Repositories

Example of working with ClickHouse tables:

```java
@Data
@AllArgsConstructor
public class ClickHouseEvent {
    private LocalDateTime timestamp;
    private String eventType;
    private String userId;
    private Map<String, String> properties;
    private Long value;
}

@Repository
@Slf4j
public class ClickHouseEventRepository {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    private static final String CREATE_TABLE_SQL = """
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id String,
            properties Map(String, String),
            value Int64
        )
        ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (timestamp, event_type)
        """;
        
    private static final String INSERT_EVENT_SQL = """
        INSERT INTO events (
            timestamp, event_type, user_id, properties, value
        ) VALUES (?, ?, ?, ?, ?)
        """;
        
    public ClickHouseEventRepository(ClickHouseTemplate clickHouseTemplate) {
        this.clickHouseTemplate = clickHouseTemplate;
        initializeTable();
    }
    
    private void initializeTable() {
        try {
            clickHouseTemplate.update(CREATE_TABLE_SQL);
        } catch (// Freeze table to create a consistent point-in-time backup
            String freezeQuery = String.format("""
                ALTER TABLE %s.%s FREEZE
                """, 
                database, 
                table
            );
            
            clickHouseTemplate.update(freezeQuery);
            
            // Copy shadow directory to backup location
            Path shadowPath = Paths.get("/var/lib/clickhouse/shadow");
            Path backupPath = Paths.get(backupDirectory.toString(), 
                generateBackupName(database, table));
                
            Files.walk(shadowPath)
                .filter(path -> !Files.isDirectory(path))
                .forEach(source -> {
                    try {
                        Path relativePath = shadowPath.relativize(source);
                        Path targetPath = backupPath.resolve(relativePath);
                        
                        Files.createDirectories(targetPath.getParent());
                        Files.copy(source, targetPath, 
                            StandardCopyOption.REPLACE_EXISTING);
                            
                    } catch (IOException e) {
                        throw new ClickHouseBackupException(
                            "Failed to copy backup files", e);
                    }
                });
                
            // Create metadata backup
            backupMetadata(database, table, backupPath);
            
            log.info("Successfully created backup for {}.{} at {}", 
                database, table, backupPath);
                
        } catch (Exception e) {
            log.error("Backup creation failed for {}.{}", 
                database, table, e);
            throw new ClickHouseBackupException(
                "Failed to create backup", e);
        } finally {
            // Clean up shadow directory
            cleanupShadow();
        }
    }
    
    /**
     * Restores a table or database from a previously created backup.
     * This method ensures data consistency during the restore process.
     */
    public void restoreBackup(String database, 
            String table, 
            String backupName) {
        try {
            // Verify backup exists and is valid
            Path backupPath = Paths.get(backupDirectory.toString(), 
                backupName);
                
            if (!isValidBackup(backupPath)) {
                throw new InvalidBackupException(
                    "Backup validation failed");
            }
            
            // Restore metadata first
            restoreMetadata(database, table, backupPath);
            
            // Stop inserts to ensure consistency
            String readOnlyQuery = String.format("""
                ALTER TABLE %s.%s MODIFY SETTING read_only=1
                """,
                database,
                table
            );
            
            clickHouseTemplate.update(readOnlyQuery);
            
            try {
                // Detach current table
                String detachQuery = String.format("""
                    ALTER TABLE %s.%s DETACH
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(detachQuery);
                
                // Copy backup files to data directory
                Path dataPath = Paths.get("/var/lib/clickhouse/data", 
                    database, 
                    table);
                    
                Files.walk(backupPath)
                    .filter(path -> !Files.isDirectory(path))
                    .forEach(source -> {
                        try {
                            Path relativePath = backupPath
                                .relativize(source);
                            Path targetPath = dataPath
                                .resolve(relativePath);
                            
                            Files.createDirectories(targetPath
                                .getParent());
                            Files.copy(source, targetPath, 
                                StandardCopyOption.REPLACE_EXISTING);
                                
                        } catch (IOException e) {
                            throw new ClickHouseRestoreException(
                                "Failed to copy restore files", e);
                        }
                    });
                
                // Attach table back
                String attachQuery = String.format("""
                    ALTER TABLE %s.%s ATTACH
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(attachQuery);
                
            } finally {
                // Re-enable writes
                String writeEnableQuery = String.format("""
                    ALTER TABLE %s.%s MODIFY SETTING read_only=0
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(writeEnableQuery);
            }
            
            log.info("Successfully restored {}.{} from backup {}", 
                database, table, backupName);
                
        } catch (Exception e) {
            log.error("Restore failed for {}.{} from backup {}", 
                database, table, backupName, e);
            throw new ClickHouseRestoreException(
                "Failed to restore from backup", e);
        }
    }
    
    /**
     * Backs up table metadata including schema, settings, and configurations.
     * This ensures the table can be properly reconstructed during restore.
     */
    private void backupMetadata(String database, 
            String table, 
            Path backupPath) {
        try {
            // Get table metadata
            String metadataQuery = String.format("""
                SELECT create_table_query, 
                       engine,
                       partition_key,
                       sorting_key,
                       primary_key,
                       sampling_key
                FROM system.tables
                WHERE database = ? AND name = ?
                """);
                
            TableMetadata metadata = clickHouseTemplate.query(
                metadataQuery,
                (rs, rowNum) -> new TableMetadata(
                    rs.getString("create_table_query"),
                    rs.getString("engine"),
                    rs.getString("partition_key"),
                    rs.getString("sorting_key"),
                    rs.getString("primary_key"),
                    rs.getString("sampling_key")
                ),
                database,
                table
            ).get(0);
            
            // Save metadata to backup location
            Path metadataPath = backupPath.resolve("metadata.json");
            ObjectMapper mapper = new ObjectMapper();
            mapper.writeValue(metadataPath.toFile(), metadata);
            
        } catch (Exception e) {
            throw new ClickHouseBackupException(
                "Failed to backup metadata", e);
        }
    }
    
    /**
     * Restores table metadata before data restoration.
     * This ensures the table is properly configured for the restored data.
     */
    private void restoreMetadata(String database, 
            String table, 
            Path backupPath) {
        try {
            // Read metadata from backup
            Path metadataPath = backupPath.resolve("metadata.json");
            ObjectMapper mapper = new ObjectMapper();
            TableMetadata metadata = mapper.readValue(
                metadataPath.toFile(), 
                TableMetadata.class
            );
            
            // Drop existing table if exists
            String dropQuery = String.format("""
                DROP TABLE IF EXISTS %s.%s
                """,
                database,
                table
            );
            
            clickHouseTemplate.update(dropQuery);
            
            // Recreate table using backed up metadata
            clickHouseTemplate.update(metadata.getCreateTableQuery());
            
        } catch (Exception e) {
            throw new ClickHouseRestoreException(
                "Failed to restore metadata", e);
        }
    }
    
    private String generateBackupPath(String database, String table) {
        return Paths.get(
            backupDirectory.toString(),
            generateBackupName(database, table)
        ).toString();
    }
    
    private String generateBackupName(String database, String table) {
        return String.format("%s_%s_%s", 
            database,
            table,
            LocalDateTime.now().format(
                DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss")
            )
        );
    }
    
    private boolean isValidBackup(Path backupPath) {
        // Verify backup structure and integrity
        if (!Files.exists(backupPath)) {
            return false;
        }
        
        // Check metadata file exists
        if (!Files.exists(backupPath.resolve("metadata.json"))) {
            return false;
        }
        
        // Verify backup files
        try {
            return Files.walk(backupPath)
                .filter(path -> !Files.isDirectory(path))
                .allMatch(this::verifyBackupFile);
        } catch (IOException e) {
            return false;
        }
    }
    
    private boolean verifyBackupFile(Path file) {
        try {
            // Verify file integrity
            return Files.size(file) > 0;
        } catch (IOException e) {
            return false;
        }
    }
    
    private void cleanupShadow() {
        try {
            Path shadowPath = Paths.get("/var/lib/clickhouse/shadow");
            if (Files.exists(shadowPath)) {
                Files.walk(shadowPath)
                    .sorted(Comparator.reverseOrder())
                    .forEach(path -> {
                        try {
                            Files.delete(path);
                        } catch (IOException e) {
                            log.warn("Failed to cleanup shadow file: {}", 
                                path, e);
                        }
                    });
            }
        } catch (IOException e) {
            log.warn("Failed to cleanup shadow directory", e);
        }
    }
}

@Data
@AllArgsConstructor
@NoArgsConstructor
public class TableMetadata {
    private String createTableQuery;
    private String engine;
    private String partitionKey;
    private String sortingKey;
    private String primaryKey;
    private String samplingKey;
}
```

### 6.4 Monitoring and Alerting

Let's implement a comprehensive monitoring system that tracks key ClickHouse metrics and alerts on potential issues:

```java
@Service
@Slf4j
public class ClickHouseMonitoringService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final AlertService alertService;
    private final MeterRegistry meterRegistry;
    
    /**
     * Collects and analyzes system metrics to identify potential issues
     * before they affect production workloads.
     */
    @Scheduled(fixedRate = 60000) // Every minute
    public void collectSystemMetrics() {
        try {
            // Collect memory metrics
            collectMemoryMetrics();
            
            // Monitor query performance
            monitorQueryPerformance();
            
            // Check disk usage
            checkDiskUsage();
            
            // Monitor replication status
            checkReplicationStatus();
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            alertService.sendAlert(
                AlertLevel.ERROR,
                "Monitoring failed",
                "System metrics collection failed: " + e.getMessage()
            );
        }
    }
    
    private void collectMemoryMetrics() {
        String memoryQuery = """
            SELECT metric, value
            FROM system.metrics
            WHERE metric LIKE '%Memory%'
            """;
            
        clickHouseTemplate.query(
            memoryQuery,
            (rs, rowNum) -> {
                String metric = rs.getString("metric");
                double value = rs.getDouble("value");
                
                // Record metric
                meterRegistry.gauge(
                    "clickhouse.memory." + metric.toLowerCase(),
                    value
                );
                
                // Check thresholds
                if (metric.equals("MemoryTracking") && 
                        value > 0.9 * getTotalMemory()) {
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Memory Usage",
                        String.format(
                            "Memory usage is at %.2f%% of total", 
                            (value / getTotalMemory()) * 100
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void monitorQueryPerformance() {
        String queryMetricsQuery = """
            SELECT
                type,
                query,
                query_duration_ms,
                memory_usage,
                read_rows,
                written_rows
            FROM system.query_log
            WHERE event_time >= now() - INTERVAL 1 MINUTE
                AND type = 'QueryFinish'
            """;
            
        clickHouseTemplate.query(
            queryMetricsQuery,
            (rs, rowNum) -> {
                long duration = rs.getLong("query_duration_ms");
                long memoryUsage = rs.getLong("memory_usage");
                
                // Record metrics
                meterRegistry.timer("clickhouse.query.duration")
                    .record(duration, TimeUnit.MILLISECONDS);
                meterRegistry.gauge("clickhouse.query.memory", 
                    memoryUsage);
                
                // Alert on slow queries
                if (duration > 10000) { // 10 seconds
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "Slow Query Detected",
                        String.format(
                            "Query took %dms: %s",
                            duration,
                            rs.getString("query")
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void checkDiskUsage() {
        String diskQuery = """
            SELECT
                name,
                free_space,
                total_space
            FROM system.disks
            """;
            
        clickHouseTemplate.query(
            diskQuery,
            (rs, rowNum) -> {
                String name = rs.getString("name");
                long freeSpace = rs.getLong("free_space");
                long totalSpace = rs.getLong("total_space");
                double usagePercent = (1 - (double)freeSpace/totalSpace) 
                    * 100;
                
                // Record metrics
                meterRegistry.gauge(
                    "clickhouse.disk.usage.percent", 
                    usagePercent
                );
                
                // Alert on high disk usage
                if (usagePercent > 85) {
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Disk Usage",
                        String.format(
                            "Disk %s usage is at %.2f%%",
                            name,
                            usagePercent
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void checkReplicationStatus() {
        String replicationQuery = """
            SELECT
                database,
                table,
                absolute_delay,
                total_replicas,
                active_replicas
            FROM system.replicas
            """;
            
        clickHouseTemplate.query(
            replicationQuery,
            (rs, rowNum) -> {
                String database = rs.getString("database");
                String table = rs.getString("table");
                long delay = rs.getLong("absolute_delay");
                int totalReplicas = rs.getInt("total_replicas");
                int activeReplicas = rs.getInt("active_replicas");
                
                // Record metrics
                meterRegistry.gauge(
                    "clickhouse.replication.delay",
                    delay
                );
                meterRegistry.gauge(
                    "clickhouse.replication.active_replicas",
                    activeReplicas
                );
                
                // Alert on replication issues
                if (delay > 300) { // 5 minutes
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Replication Delay",
                        String.format(
                            "Table %s.%s has %d seconds replication delay",
                            database,
                            table,
                            delay
                        )
                    );
                }            clickHouseTemplate.update(cleanupQuery, cutoffDate);
            
            log.info("Archived data from {} to {} before {}", 
                sourceTable, archiveTable, cutoffDate);
                
        } catch (Exception e) {
            log.error("Failed to archive data for table: {}", 
                sourceTable, e);
            throw new ClickHouseOperationException(
                "Data archiving failed", e);
        }
    }
}

## 6. Production Best Practices

### 6.1 Performance Optimization Strategies

When deploying ClickHouse in production, several key optimization strategies can significantly improve performance. Let's implement a comprehensive optimization service:

```java
@Service
@Slf4j
public class ClickHouseOptimizationService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final ClickHouseMetricsCollector metricsCollector;
    
    /**
     * Analyzes and optimizes table structure based on query patterns and data volume.
     * This helps maintain high query performance as data grows.
     */
    public TableOptimizationPlan analyzeTablePerformance(String tableName) {
        TableOptimizationPlan plan = new TableOptimizationPlan();
        
        // Analyze table statistics
        String statsQuery = """
            SELECT
                table,
                total_rows,
                total_bytes,
                primary_key_bytes_in_memory,
                bytes_on_disk
            FROM system.tables
            WHERE name = ?
            """;
            
        clickHouseTemplate.query(
            statsQuery,
            (rs) -> {
                plan.setTotalRows(rs.getLong("total_rows"));
                plan.setTotalBytes(rs.getLong("total_bytes"));
                plan.setPrimaryKeyMemory(
                    rs.getLong("primary_key_bytes_in_memory"));
                plan.setDiskUsage(rs.getLong("bytes_on_disk"));
                return null;
            },
            tableName
        );
        
        // Analyze query patterns
        String queryPatternsQuery = """
            SELECT
                query_kind,
                count() as query_count,
                avg(query_duration_ms) as avg_duration,
                max(read_rows) as max_rows_read,
                max(memory_usage) as max_memory
            FROM system.query_log
            WHERE table = ?
                AND event_time >= now() - INTERVAL 1 DAY
            GROUP BY query_kind
            """;
            
        clickHouseTemplate.query(
            queryPatternsQuery,
            (rs) -> {
                QueryPattern pattern = new QueryPattern(
                    rs.getString("query_kind"),
                    rs.getLong("query_count"),
                    rs.getDouble("avg_duration"),
                    rs.getLong("max_rows_read"),
                    rs.getLong("max_memory")
                );
                plan.addQueryPattern(pattern);
                return null;
            },
            tableName
        );
        
        // Generate optimization recommendations
        plan.generateRecommendations();
        
        return plan;
    }
    
    /**
     * Implements the recommended optimizations for a table.
     * This method carefully applies changes while ensuring data consistency.
     */
    public void applyOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        try {
            // Apply index optimizations
            if (plan.isIndexOptimizationNeeded()) {
                applyIndexOptimizations(tableName, plan);
            }
            
            // Optimize partitioning
            if (plan.isPartitioningOptimizationNeeded()) {
                optimizePartitioning(tableName, plan);
            }
            
            // Adjust compression settings
            if (plan.isCompressionOptimizationNeeded()) {
                optimizeCompression(tableName, plan);
            }
            
            log.info("Successfully applied optimizations for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to apply optimizations for table: {}", 
                tableName, e);
            throw new ClickHouseOptimizationException(
                "Optimization application failed", e);
        }
    }
    
    private void applyIndexOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply skip indexes for frequently filtered columns
        for (String column : plan.getFrequentlyFilteredColumns()) {
            String indexQuery = String.format("""
                ALTER TABLE %s
                ADD INDEX %s_idx %s TYPE minmax GRANULARITY 3
                """,
                tableName,
                column,
                column
            );
            
            clickHouseTemplate.update(indexQuery);
        }
        
        // Optimize primary key if needed
        if (plan.isPrimaryKeyOptimizationNeeded()) {
            String primaryKeyQuery = String.format("""
                ALTER TABLE %s
                MODIFY ORDER BY %s
                """,
                tableName,
                String.join(", ", plan.getRecommendedPrimaryKeyColumns())
            );
            
            clickHouseTemplate.update(primaryKeyQuery);
        }
    }
    
    private void optimizePartitioning(
            String tableName, 
            TableOptimizationPlan plan) {
        if (plan.getTotalRows() > 100_000_000) {
            // For large tables, implement more granular partitioning
            String partitioningQuery = String.format("""
                ALTER TABLE %s
                MODIFY PARTITION BY toYYYYMM(timestamp)
                """,
                tableName
            );
            
            clickHouseTemplate.update(partitioningQuery);
        }
    }
    
    private void optimizeCompression(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply column-specific compression settings
        for (Map.Entry<String, String> entry : 
                plan.getCompressionRecommendations().entrySet()) {
            String compressionQuery = String.format("""
                ALTER TABLE %s
                MODIFY COLUMN %s %s CODEC(%s)
                """,
                tableName,
                entry.getKey(),
                getColumnType(tableName, entry.getKey()),
                entry.getValue()
            );
            
            clickHouseTemplate.update(compressionQuery);
        }
    }
    
    private String getColumnType(String tableName, String columnName) {
        String query = String.format("""
            SELECT type
            FROM system.columns
            WHERE table = ? AND name = ?
            """);
            
        return clickHouseTemplate.query(
            query,
            (rs, rowNum) -> rs.getString("type"),
            tableName,
            columnName
        ).get(0);
    }
}

@Data
public class TableOptimizationPlan {
    private long totalRows;
    private long totalBytes;
    private long primaryKeyMemory;
    private long diskUsage;
    private List<QueryPattern> queryPatterns = new ArrayList<>();
    private List<String> recommendedPrimaryKeyColumns = new ArrayList<>();
    private Map<String, String> compressionRecommendations = new HashMap<>();
    private Set<String> frequentlyFilteredColumns = new HashSet<>();
    
    public void generateRecommendations() {
        analyzeQueryPatterns();
        generateCompressionRecommendations();
        optimizePrimaryKey();
    }
    
    private void analyzeQueryPatterns() {
        // Analyze query patterns to identify optimization opportunities
        queryPatterns.forEach(pattern -> {
            if (pattern.getQueryCount() > 1000 && 
                    pattern.getAvgDuration() > 1000) {
                // High-frequency, slow queries need optimization
                analyzeQueryForOptimization(pattern);
            }
        });
    }
    
    private void analyzeQueryForOptimization(QueryPattern pattern) {
        // Extract frequently filtered columns from query pattern
        // This is a simplified example - in practice, you would
        // implement more sophisticated query analysis
        if (pattern.getMaxRowsRead() > totalRows * 0.5) {
            // Query is scanning too much data, might need better indexing
            extractFilterColumns(pattern).forEach(column ->
                frequentlyFilteredColumns.add(column)
            );
        }
    }
    
    private Set<String> extractFilterColumns(QueryPattern pattern) {
        // In practice, implement query parsing to extract filter columns
        return new HashSet<>();
    }
    
    private void generateCompressionRecommendations() {
        // Generate compression recommendations based on data patterns
        // This is a simplified example - in practice, you would analyze
        // actual data patterns and column value distributions
        queryPatterns.forEach(pattern -> {
            if (pattern.getMaxMemory() > 1024 * 1024 * 1024) {
                // High memory usage queries might benefit from better compression
                recommendHighCompressionForLargeColumns(pattern);
            }
        });
    }
    
    private void recommendHighCompressionForLargeColumns(QueryPattern pattern) {
        // Add compression recommendations for columns with high memory usage
        // In practice, you would analyze column-specific characteristics
        compressionRecommendations.put(
            "high_cardinality_column", 
            "ZSTD(3)"
        );
        compressionRecommendations.put(
            "low_cardinality_column",
            "LowCardinality"
        );
    }
    
    private void optimizePrimaryKey() {
        // Analyze query patterns to recommend optimal primary key order
        // This is a simplified example - in practice, you would analyze
        // actual query access patterns and data distribution
        if (!frequentlyFilteredColumns.isEmpty()) {
            recommendedPrimaryKeyColumns.addAll(frequentlyFilteredColumns);
        }
    }
}

@Data
@AllArgsConstructor
public class QueryPattern {
    private String queryKind;
    private long queryCount;
    private double avgDuration;
    private long maxRowsRead;
    private long maxMemory;
}
```

### 6.2 Fault Tolerance and High Availability

Implementing robust fault tolerance mechanisms for ClickHouse deployments:

```java
@Service
@Slf4j
public class ClickHouseHighAvailabilityService {
    
    private final List<ClickHouseTemplate> clickHouseTemplates;
    private final LoadBalancer loadBalancer;
    private final CircuitBreaker circuitBreaker;
    
    /**
     * Executes queries with automatic failover and retry mechanisms.
     * This ensures continuous operation even when some nodes are unavailable.
     */
    public <T> T executeWithFailover(ClickHouseOperation<T> operation) {
        Exception lastException = null;
        
        for (int attempt = 0; attempt < 3; attempt++) {
            ClickHouseTemplate template = loadBalancer.getNextAvailable();
            
            if (template == null) {
                throw new NoAvailableNodesException(
                    "No ClickHouse nodes available");
            }
            
            try {
                if (!circuitBreaker.isAllowed()) {
                    continue;
                }
                
                T result = operation.execute(template);
                circuitBreaker.recordSuccess();
                return result;
                
            } catch (Exception e) {
                lastException = e;
                circuitBreaker.recordFailure();
                loadBalancer.markNodeAsFailed(template);
                
                log.warn("Query execution failed on attempt {}: {}", 
                    attempt + 1, e.getMessage());
            }
        }
        
        throw new ClickHouseFailoverException(
            "All failover attempts exhausted", lastException);
    }
}

@Component
@Slf4j
public class LoadBalancer {
    private final List<ClickHouseTemplate> availableNodes;
    private final AtomicInteger currentIndex = new AtomicInteger(0);
    private final Map<ClickHouseTemplate, HealthStatus> nodeStatus;
    
    public LoadBalancer(List<ClickHouseTemplate> nodes) {
        this.availableNodes = new CopyOnWriteArrayList<>(nodes);
        this.nodeStatus = new ConcurrentHashMap<>();
        
        nodes.forEach(node -> 
            nodeStatus.put(node, new HealthStatus()));
            
        // Start health check schedule
        startHealthChecks();
    }
    
    public ClickHouseTemplate getNextAvailable() {
        int attempts = availableNodes.size();
        
        while (attempts > 0) {
            ClickHouseTemplate node = availableNodes.get(
                currentIndex.getAndIncrement() % availableNodes.size()
            );
            
            if (isNodeHealthy(node)) {
                return node;
            }
            
            attempts--;
        }
        
        return null;
    }
    
    public void markNodeAsFailed(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        status.recordFailure();
        
        if (status.getConsecutiveFailures() >= 3) {
            availableNodes.remove(node);
            log.warn("Removed unhealthy node from rotation");
        }
    }
    
    private boolean isNodeHealthy(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        return status.isHealthy() && checkNodeConnection(node);
    }
    
    private boolean checkNodeConnection(ClickHouseTemplate node) {
        try {
            node.query("SELECT 1", (rs, rowNum) -> 1);
            return true;
        } catch (Exception e) {
            log.warn("Node health check failed: {}", e.getMessage());
            return false;
        }
    }
    
    private void startHealthChecks() {
        ScheduledExecutorService executor = 
            Executors.newSingleThreadScheduledExecutor();
            
        executor.scheduleAtFixedRate(
            this::performHealthChecks,
            0,
            30,
            TimeUnit.SECONDS
        );
    }
    
    private void performHealthChecks() {
        nodeStatus.forEach((node, status) -> {
            if (checkNodeConnection(node)) {
                status.recordSuccess();
                if (!availableNodes.contains(node)) {
                    availableNodes.add(node);
                    log.info("Restored healthy node to rotation");
                }
            } else {
                markNodeAsFailed(node);
            }
        });
    }
}

@Data
public class HealthStatus {
    private AtomicInteger consecutiveFailures = new AtomicInteger(0);
    private AtomicLong lastSuccess = new AtomicLong(0);
    
    public void recordSuccess() {
        consecutiveFailures.set(0);
        lastSuccess.set(System.currentTimeMillis());
    }
    
    public void recordFailure() {
        consecutiveFailures.incrementAndGet();
    }
    
    public boolean isHealthy() {
        return consecutiveFailures.get() < 3 &&
            (System.currentTimeMillis() - lastSuccess.get()) < 60000;
    }
}
```

### 6.3 Backup and Recovery

Implementing comprehensive backup and recovery mechanisms:

```java
@Service
@Slf4j
public class ClickHouseBackupService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final Path backupDirectory;
    
    /**
     * Creates a consistent backup of a ClickHouse table or database.
     * This ensures data can be recovered in case of failures.
     */
    public void createBackup(String database, String table) {
        String backupPath = generateBackupPath(database, table);
        
        try {
            // Freeze        } catch (Exception e) {
            log.error("Failed to initialize events table", e);
            throw new ClickHouseInitializationException(
                "Could not create events table", e);
        }
    }
    
    /**
     * Stores a single event in ClickHouse. The method ensures proper handling of
     * the Map type for properties and uses prepared statements for efficiency.
     */
    public void storeEvent(ClickHouseEvent event) {
        try {
            clickHouseTemplate.update(
                INSERT_EVENT_SQL,
                event.getTimestamp(),
                event.getEventType(),
                event.getUserId(),
                convertPropertiesToClickHouseFormat(event.getProperties()),
                event.getValue()
            );
        } catch (Exception e) {
            log.error("Failed to store event: {}", event, e);
            throw new ClickHouseOperationException(
                "Could not store event", e);
        }
    }
    
    /**
     * Performs batch insertion of events for better performance when dealing with
     * multiple records. This method uses the ClickHouse native format for efficient
     * data loading.
     */
    public void storeBatchEvents(List<ClickHouseEvent> events) {
        try {
            clickHouseTemplate.batchInsert(
                "events",
                events,
                new ClickHouseInsertMapper<ClickHouseEvent>() {
                    @Override
                    public String[] getColumns() {
                        return new String[] {
                            "timestamp", "event_type", "user_id", 
                            "properties", "value"
                        };
                    }
                    
                    @Override
                    public void mapRow(PreparedStatement ps, ClickHouseEvent event) 
                            throws SQLException {
                        ps.setObject(1, event.getTimestamp());
                        ps.setString(2, event.getEventType());
                        ps.setString(3, event.getUserId());
                        ps.setObject(4, convertPropertiesToClickHouseFormat(
                            event.getProperties()));
                        ps.setLong(5, event.getValue());
                    }
                }
            );
        } catch (Exception e) {
            log.error("Failed to store batch events", e);
            throw new ClickHouseOperationException(
                "Could not store batch events", e);
        }
    }
    
    /**
     * Queries events within a specific time range. This method demonstrates
     * how to work with ClickHouse's powerful date and time functions.
     */
    public List<ClickHouseEvent> queryEventsByTimeRange(
            LocalDateTime startTime,
            LocalDateTime endTime,
            String eventType) {
        String sql = """
            SELECT
                timestamp,
                event_type,
                user_id,
                properties,
                value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
                AND event_type = ?
            ORDER BY timestamp
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new ClickHouseEvent(
                rs.getObject("timestamp", LocalDateTime.class),
                rs.getString("event_type"),
                rs.getString("user_id"),
                convertClickHouseMapToJava(rs.getString("properties")),
                rs.getLong("value")
            ),
            startTime,
            endTime,
            eventType
        );
    }
    
    /**
     * Performs an analytical query to aggregate events. This demonstrates
     * ClickHouse's analytical capabilities and performance optimizations.
     */
    public Map<String, Long> aggregateEventsByType(
            LocalDateTime startTime,
            LocalDateTime endTime) {
        String sql = """
            SELECT
                event_type,
                count() as event_count,
                sum(value) as total_value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY event_type
            WITH TOTALS
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new AbstractMap.SimpleEntry<>(
                rs.getString("event_type"),
                rs.getLong("event_count")
            ),
            startTime,
            endTime
        ).stream().collect(Collectors.toMap(
            Map.Entry::getKey,
            Map.Entry::getValue
        ));
    }
    
    private String convertPropertiesToClickHouseFormat(
            Map<String, String> properties) {
        // Convert Java Map to ClickHouse Map format
        return properties.entrySet().stream()
            .map(e -> String.format("'%s':'%s'",
                e.getKey().replace("'", "\\'"),
                e.getValue().replace("'", "\\'"))
            )
            .collect(Collectors.joining(",", "{", "}"));
    }
    
    private Map<String, String> convertClickHouseMapToJava(String mapStr) {
        // Convert ClickHouse Map string format to Java Map
        if (mapStr == null || mapStr.length() < 2) {
            return Collections.emptyMap();
        }
        
        String content = mapStr.substring(1, mapStr.length() - 1);
        Map<String, String> result = new HashMap<>();
        
        Pattern pattern = Pattern.compile("'([^']*)'\\s*:\\s*'([^']*)'");
        Matcher matcher = pattern.matcher(content);
        
        while (matcher.find()) {
            result.put(
                matcher.group(1).replace("\\'", "'"),
                matcher.group(2).replace("\\'", "'")
            );
        }
        
        return result;
    }
}
```

## 5. Advanced Features and Optimizations

### 5.1 Query Optimization Service

Let's create a service that helps optimize ClickHouse queries for better performance:

```java
@Service
@Slf4j
public class ClickHouseQueryOptimizer {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Analyzes and optimizes a given query using ClickHouse's explain functionality.
     * This helps understand query execution plans and potential optimizations.
     */
    public String analyzeQuery(String query) {
        String explainQuery = String.format(
            "EXPLAIN indexes = 1, actions = 1, description = 1 %s",
            query
        );
        
        try {
            return clickHouseTemplate.query(
                explainQuery,
                (rs, rowNum) -> rs.getString(1)
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze query: {}", query, e);
            throw new ClickHouseQueryException(
                "Query analysis failed", e);
        }
    }
    
    /**
     * Suggests optimal table settings based on data patterns and query types.
     * This helps improve query performance through better table design.
     */
    public TableOptimizationRecommendations analyzeTableStructure(
            String tableName) {
        String analyzeQuery = """
            SELECT
                count() as total_rows,
                max(bytes_on_disk) as disk_usage,
                max(compression_ratio) as compression_ratio
            FROM system.parts
            WHERE table = ?
            """;
            
        try {
            return clickHouseTemplate.query(
                analyzeQuery,
                (rs, rowNum) -> new TableOptimizationRecommendations(
                    rs.getLong("total_rows"),
                    rs.getLong("disk_usage"),
                    rs.getDouble("compression_ratio")
                ),
                tableName
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze table: {}", tableName, e);
            throw new ClickHouseQueryException(
                "Table analysis failed", e);
        }
    }
}

@Data
@AllArgsConstructor
public class TableOptimizationRecommendations {
    private long totalRows;
    private long diskUsage;
    private double compressionRatio;
    
    public String getPartitioningRecommendation() {
        if (totalRows > 100_000_000) {
            return "Consider partitioning by date for better query performance";
        }
        return "Current partitioning scheme appears adequate";
    }
    
    public String getCompressionRecommendation() {
        if (compressionRatio < 3.0) {
            return "Consider using more aggressive compression settings";
        }
        return "Compression ratio is within acceptable range";
    }
}
```

### 5.2 Monitoring and Metrics

Implementing comprehensive monitoring for ClickHouse:

```java
@Configuration
public class ClickHouseMonitoringConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public ClickHouseMetricsCollector metricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        return new ClickHouseMetricsCollector(registry, clickHouseTemplate);
    }
}

@Component
@Slf4j
public class ClickHouseMetricsCollector {
    
    private final MeterRegistry registry;
    private final ClickHouseTemplate clickHouseTemplate;
    private final Timer queryTimer;
    private final Timer insertTimer;
    private final Counter errorCounter;
    
    public ClickHouseMetricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        this.registry = registry;
        this.clickHouseTemplate = clickHouseTemplate;
        
        // Initialize metrics
        this.queryTimer = Timer.builder("clickhouse.query.duration")
            .description("Time taken for ClickHouse queries")
            .register(registry);
            
        this.insertTimer = Timer.builder("clickhouse.insert.duration")
            .description("Time taken for ClickHouse inserts")
            .register(registry);
            
        this.errorCounter = Counter.builder("clickhouse.errors")
            .description("Number of ClickHouse errors")
            .register(registry);
            
        // Start collecting system metrics
        scheduleMetricsCollection();
    }
    
    private void scheduleMetricsCollection() {
        Executors.newSingleThreadScheduledExecutor()
            .scheduleAtFixedRate(
                this::collectSystemMetrics,
                0,
                1,
                TimeUnit.MINUTES
            );
    }
    
    private void collectSystemMetrics() {
        try {
            // Collect memory metrics
            String memoryQuery = """
                SELECT
                    metric,
                    value
                FROM system.metrics
                WHERE metric IN ('MemoryTracking', 'MemoryTrackingInBackgroundProcesses')
                """;
                
            clickHouseTemplate.query(
                memoryQuery,
                (rs, rowNum) -> {
                    String metric = rs.getString("metric");
                    double value = rs.getDouble("value");
                    registry.gauge(
                        "clickhouse.memory." + metric.toLowerCase(),
                        value
                    );
                    return null;
                }
            );
            
            // Collect query metrics
            String queryMetricsQuery = """
                SELECT
                    event_type,
                    count() as count
                FROM system.query_log
                WHERE event_time >= now() - INTERVAL 1 MINUTE
                GROUP BY event_type
                """;
                
            clickHouseTemplate.query(
                queryMetricsQuery,
                (rs, rowNum) -> {
                    String eventType = rs.getString("event_type");
                    long count = rs.getLong("count");
                    registry.counter(
                        "clickhouse.queries",
                        "type",
                        eventType
                    ).increment(count);
                    return null;
                }
            );
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            errorCounter.increment();
        }
    }
    
    public Timer.Sample startQueryTimer() {
        return Timer.start(registry);
    }
    
    public void stopQueryTimer(Timer.Sample sample) {
        sample.stop(queryTimer);
    }
    
    public Timer.Sample startInsertTimer() {
        return Timer.start(registry);
    }
    
    public void stopInsertTimer(Timer.Sample sample) {
        sample.stop(insertTimer);
    }
    
    public void recordError(String errorType) {
        errorCounter.increment();
        registry.counter(
            "clickhouse.errors",
            "type",
            errorType
        ).increment();
    }
}
```

### 5.3 Data Lifecycle Management

Implementing data lifecycle management for ClickHouse tables:

```java
@Service
@Slf4j
public class ClickHouseDataLifecycleManager {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Manages the lifecycle of data in ClickHouse tables, including
     * partitioning, TTL, and data archiving strategies.
     */
    public void configureTableLifecycle(
            String tableName,
            Period retentionPeriod,
            String partitioningColumn) {
        try {
            // Configure TTL for data retention
            String ttlQuery = String.format("""
                ALTER TABLE %s
                MODIFY TTL
                    %s + INTERVAL %d MONTH
                    DELETE
                """,
                tableName,
                partitioningColumn,
                retentionPeriod.toTotalMonths()
            );
            
            clickHouseTemplate.update(ttlQuery);
            
            // Configure storage policy for cold data
            String storagePolicyQuery = String.format("""
                ALTER TABLE %s
                MODIFY SETTING storage_policy = 'tiered'
                """,
                tableName
            );
            
            clickHouseTemplate.update(storagePolicyQuery);
            
            log.info("Configured lifecycle management for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to configure table lifecycle: {}", 
                tableName, e);
            throw new ClickHouseOperationException(
                "Lifecycle configuration failed", e);
        }
    }
    
    /**
     * Optimizes table storage by merging partitions and reorganizing data.
     * This helps maintain query performance over time.
     */
    public void optimizeTable(String tableName) {
        try {
            String optimizeQuery = String.format(
                "OPTIMIZE TABLE %s FINAL",
                tableName
            );
            
            clickHouseTemplate.update(optimizeQuery);
            log.info("Optimized table: {}", tableName);
            
        } catch (Exception e) {
            log.error("Failed to optimize table: {}", tableName, e);
            throw new ClickHouseOperationException(
                "Table optimization failed", e);
        }
    }
    
    /**
     * Archives old data to a different storage tier while maintaining
     * accessibility for historical analysis.
     */
    public void archiveOldData(
            String sourceTable,
            String archiveTable,
            LocalDateTime cutoffDate) {
        try {
            // Move data to archive table
            String archiveQuery = String.format("""
                INSERT INTO %s
                SELECT *
                FROM %s
                WHERE timestamp < ?
                """,
                archiveTable,
                sourceTable
            );
            
            clickHouseTemplate.update(archiveQuery, cutoffDate);
            
            // Delete archived data from source table
            String cleanupQuery = String.format("""
                ALTER TABLE %s
                DELETE WHERE timestamp < ?
                """,
                sourceTable
            );
            
            clickHouseTemplate.update# ClickHouse Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ClickHouse Architecture

Before diving into configuration details, let's understand why ClickHouse is particularly well-suited for analytical workloads and how its architecture supports high-performance analytics.

ClickHouse is a column-oriented database management system designed for online analytical processing (OLAP). Its architecture is optimized for:
- Fast query processing on large datasets
- Real-time data ingestion
- Efficient compression
- Distributed query execution
- Fault tolerance and high availability

### 1.1 Key Concepts

Several fundamental concepts make ClickHouse powerful:

1. Column-oriented Storage: Data is stored by columns rather than rows, enabling:
   - Better compression ratios
   - Faster analytical queries
   - Efficient scanning of specific columns

2. Data Types and Encodings:
   - Specialized data types for analytics
   - Custom compression methods per column
   - Efficient numeric and string encodings

3. Distributed Architecture:
   - Sharding for horizontal scaling
   - Replication for fault tolerance
   - Distributed query execution

## 2. Single Node Setup

Let's start with a basic single-node ClickHouse installation using Docker, which is perfect for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
      - "9009:9009"   # Internal port for replication
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config:/etc/clickhouse-server/config.d
      - ./users:/etc/clickhouse-server/users.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

volumes:
  clickhouse_data:
```

### 2.2 Basic Configuration

The configuration consists of several XML files. Here's the basic setup:

```xml
<!-- config.xml -->
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>

    <memory>
        <!-- Recommended to set to 80% of available RAM -->
        <max_server_memory_usage>8589934592</max_server_memory_usage>
        <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
    </memory>

    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                </volumes>
            </default>
        </policies>
    </storage_configuration>

    <!-- Performance settings -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <default_max_threads>8</default_max_threads>
</clickhouse>

<!-- users.xml -->
<?xml version="1.0"?>
<clickhouse>
    <users>
        <default>
            <password>default_password</password>
            <networks>
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
        </default>
    </users>

    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by>
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort>
        </default>
    </profiles>

    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

## 3. Distributed Setup

For production environments, we'll configure a ClickHouse cluster with multiple nodes for high availability and better performance.

### 3.1 Multi-node Cluster Configuration

```yaml
version: '3.8'
services:
  clickhouse1:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse1_data:/var/lib/clickhouse
      - ./config/node1:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=1

  clickhouse2:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - clickhouse2_data:/var/lib/clickhouse
      - ./config/node2:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=2

  clickhouse3:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - clickhouse3_data:/var/lib/clickhouse
      - ./config/node3:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=1

  clickhouse4:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse4
    ports:
      - "8126:8123"
      - "9003:9000"
    volumes:
      - clickhouse4_data:/var/lib/clickhouse
      - ./config/node4:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=2

volumes:
  clickhouse1_data:
  clickhouse2_data:
  clickhouse3_data:
  clickhouse4_data:
```

### 3.2 Cluster Configuration

Let's configure the cluster with two shards and two replicas per shard:

```xml
<!-- metrika.xml -->
<?xml version="1.0"?>
<clickhouse>
    <remote_servers>
        <analytics_cluster>
            <!-- First shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Second shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </analytics_cluster>
    </remote_servers>

    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <macros>
        <shard>{CLICKHOUSE_SHARD}</shard>
        <replica>{CLICKHOUSE_REPLICA}</replica>
    </macros>
</clickhouse>
```

## 4. Spring Boot Integration

Now let's integrate ClickHouse with Spring Boot for seamless data access and management.

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    
    <!-- ClickHouse JDBC Driver -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ClickHouse Configuration

Let's create a comprehensive configuration that supports both single-node and cluster setups:

```java
@Configuration
@EnableTransactionManagement
public class ClickHouseConfig {
    
    @Bean
    @ConfigurationProperties(prefix = "clickhouse.datasource")
    public DataSourceProperties clickHouseDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    public DataSource clickHouseDataSource() {
        ClickHouseDataSource dataSource = new ClickHouseDataSource(
            createClickHouseProperties());
            
        // Configure connection pooling
        HikariConfig config = new HikariConfig();
        config.setDataSource(dataSource);
        config.setMaximumPoolSize(10);
        config.setMinimumIdle(5);
        config.setIdleTimeout(300000);
        config.setConnectionTimeout(20000);
        
        return new HikariDataSource(config);
    }
    
    private ClickHouseProperties createClickHouseProperties() {
        ClickHouseProperties properties = new ClickHouseProperties();
        properties.setUser("default");
        properties.setPassword("default_password");
        properties.setDatabase("default");
        
        // Performance settings
        properties.setUseServerTimeZone(true);
        properties.setUseTimeZone("UTC");
        properties.setUseServerTimeZoneForDates(true);
        
        // Connection settings
        properties.setSocketTimeout(30000);
        properties.setConnectionTimeout(10000);
        
        // Buffer settings
        properties.setBufferSize(65536);
        
        return properties;
    }
    
    @Bean
    public ClickHouseTemplate clickHouseTemplate(
            DataSource clickHouseDataSource) {
        return new ClickHouseTemplate(clickHouseDataSource);
    }
}
```

### 4.3 ClickHouse Template

Let's create a template class for executing queries:

```java
@Component
@Slf4j
public class ClickHouseTemplate {
    
    private final JdbcTemplate jdbcTemplate;
    
    public ClickHouseTemplate(DataSource dataSource) {
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    /**
     * Execute a batch insert operation
     */
    public <T> void batchInsert(String tableName, 
            List<T> records, 
            ClickHouseInsertMapper<T> mapper) {
        String insertSql = createInsertSql(tableName, mapper.getColumns());
        
        jdbcTemplate.batchUpdate(insertSql, 
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                        throws SQLException {
                    mapper.mapRow(ps, records.get(i));
                }
                
                @Override
                public int getBatchSize() {
                    return records.size();
                }
            });
    }
    
    /**
     * Execute analytical query
     */
    public <T> List<T> query(String sql, 
            Class<T> resultType, 
            Object... params) {
        try {
            return jdbcTemplate.query(
                sql, 
                new BeanPropertyRowMapper<>(resultType), 
                params
            );
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute analytical query with custom row mapper
     */
    public <T> List<T> query(String sql, 
            RowMapper<T> rowMapper, 
            Object... params) {
        try {
            return jdbcTemplate.query(sql, rowMapper, params);
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute mutation query (INSERT, UPDATE, DELETE)
     */
    public int update(String sql, Object... params) {
        try {
            return jdbcTemplate.update(sql, params);
        } catch (DataAccessException e) {
            log.error("Error executing update: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute update", e);
        }
    }
    
    private String createInsertSql(String tableName, String[] columns) {
        StringBuilder sql = new StringBuilder()
            .append("INSERT INTO ")
            .append(tableName)
            .append(" (")
            .append(String.join(", ", columns))
            .append(") VALUES (")
            .append(String.join(", ", 
                Collections.nCopies(columns.length, "?")))
            .append(")");
            
        return sql.toString();
    }
}
```

### 4.4 Data Models and Repositories

Example of working with ClickHouse tables:

```java
@Data
@AllArgsConstructor
public class ClickHouseEvent {
    private LocalDateTime timestamp;
    private String eventType;
    private String userId;
    private Map<String, String> properties;
    private Long value;
}

@Repository
@Slf4j
public class ClickHouseEventRepository {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    private static final String CREATE_TABLE_SQL = """
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id String,
            properties Map(String, String),
            value Int64
        )
        ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (timestamp, event_type)
        """;
        
    private static final String INSERT_EVENT_SQL = """
        INSERT INTO events (
            timestamp, event_type, user_id, properties, value
        ) VALUES (?, ?, ?, ?, ?)
        """;
        
    public ClickHouseEventRepository(ClickHouseTemplate clickHouseTemplate) {
        this.clickHouseTemplate = clickHouseTemplate;
        initializeTable();
    }
    
    private void initializeTable() {
        try {
            clickHouseTemplate.update(CREATE_TABLE_SQL);
        } catch (if (activeReplicas < totalReplicas) {
                    alertService.sendAlert(
                        AlertLevel.ERROR,
                        "Replica Availability Issue",
                        String.format(
                            "Table %s.%s has only %d/%d active replicas",
                            database,
                            table,
                            activeReplicas,
                            totalReplicas
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private long getTotalMemory() {
        return Runtime.getRuntime().maxMemory();
    }
}

### 6.5 Query Performance Analysis

Performance analysis is crucial for maintaining a healthy ClickHouse cluster. Let's implement a comprehensive query analysis system:

```java
@Service
@Slf4j
public class ClickHouseQueryAnalyzer {

    private final ClickHouseTemplate clickHouseTemplate;
    private final MetricsService metricsService;
    
    /**
     * Analyzes query patterns to identify optimization opportunities and 
     * potential performance issues before they impact production workloads.
     */
    public QueryAnalysisReport analyzeQueryPerformance(
            Duration analysisWindow) {
        QueryAnalysisReport report = new QueryAnalysisReport();
        
        // Collect query statistics
        String queryAnalysisSQL = """
            SELECT
                normalized_query_hash,
                query,
                count() as execution_count,
                avg(query_duration_ms) as avg_duration,
                max(query_duration_ms) as max_duration,
                avg(memory_usage) as avg_memory,
                max(memory_usage) as max_memory,
                avg(read_rows) as avg_rows_read,
                max(read_rows) as max_rows_read,
                avg(peak_memory_usage) as avg_peak_memory
            FROM system.query_log
            WHERE event_time >= now() - ?
                AND type = 'QueryFinish'
                AND exception = ''
            GROUP BY normalized_query_hash, query
            ORDER BY avg_duration DESC
            LIMIT 100
            """;

        List<QueryPattern> patterns = clickHouseTemplate.query(
            queryAnalysisSQL,
            (rs, rowNum) -> {
                QueryPattern pattern = new QueryPattern();
                pattern.setQueryHash(rs.getString("normalized_query_hash"));
                pattern.setQueryText(rs.getString("query"));
                pattern.setExecutionCount(rs.getLong("execution_count"));
                pattern.setAvgDuration(rs.getDouble("avg_duration"));
                pattern.setMaxDuration(rs.getDouble("max_duration"));
                pattern.setAvgMemory(rs.getLong("avg_memory"));
                pattern.setMaxMemory(rs.getLong("max_memory"));
                pattern.setAvgRowsRead(rs.getLong("avg_rows_read"));
                pattern.setMaxRowsRead(rs.getLong("max_rows_read"));
                pattern.setAvgPeakMemory(rs.getLong("avg_peak_memory"));
                return pattern;
            },
            analysisWindow.toSeconds()
        );

        // Analyze each query pattern
        for (QueryPattern pattern : patterns) {
            analyzeQueryPattern(pattern, report);
        }

        return report;
    }

    /**
     * Analyzes individual query patterns to detect potential issues and 
     * suggest optimizations. This helps maintain consistent query performance.
     */
    private void analyzeQueryPattern(QueryPattern pattern, 
            QueryAnalysisReport report) {
        // Check for full table scans
        if (pattern.getAvgRowsRead() > 1_000_000 && 
                !pattern.getQueryText().toLowerCase()
                    .contains("where")) {
            report.addIssue(
                new QueryIssue(
                    IssueSeverity.HIGH,
                    "Full Table Scan Detected",
                    String.format(
                        "Query scanning average of %d rows without WHERE clause",
                        pattern.getAvgRowsRead()
                    ),
                    "Add appropriate WHERE clause and ensure proper indexing"
                )
            );
        }

        // Check for memory intensive queries
        if (pattern.getAvgPeakMemory() > 1_000_000_000) { // 1GB
            report.addIssue(
                new QueryIssue(
                    IssueSeverity.MEDIUM,
                    "High Memory Usage",
                    String.format(
                        "Query using average of %.2f GB memory",
                        pattern.getAvgPeakMemory() / 1_000_000_000.0
                    ),
                    "Consider optimizing GROUP BY or JOIN operations"
                )
            );
        }

        // Check for slow queries
        if (pattern.getAvgDuration() > 10_000) { // 10 seconds
            report.addIssue(
                new QueryIssue(
                    IssueSeverity.MEDIUM,
                    "Slow Query Performance",
                    String.format(
                        "Query taking average of %.2f seconds",
                        pattern.getAvgDuration() / 1000.0
                    ),
                    generateOptimizationSuggestions(pattern)
                )
            );
        }

        // Record query pattern metrics
        recordQueryMetrics(pattern);
    }

    /**
     * Generates specific optimization suggestions based on query characteristics.
     * This helps developers improve query performance systematically.
     */
    private String generateOptimizationSuggestions(QueryPattern pattern) {
        List<String> suggestions = new ArrayList<>();
        String query = pattern.getQueryText().toLowerCase();

        // Analyze query structure and suggest optimizations
        if (query.contains("group by")) {
            suggestions.add(
                "Consider adding pre-aggregation tables for GROUP BY operations"
            );
        }

        if (query.contains("join")) {
            suggestions.add(
                "Ensure JOIN conditions use indexed columns"
            );
            suggestions.add(
                "Consider denormalizing data if joins are frequent"
            );
        }

        if (pattern.getAvgRowsRead() > pattern.getExecutionCount() * 1000) {
            suggestions.add(
                "Add appropriate WHERE conditions to reduce scanned rows"
            );
            suggestions.add(
                "Verify index usage with EXPLAIN query"
            );
        }

        return String.join("\n", suggestions);
    }

    /**
     * Records detailed metrics about query patterns for long-term analysis
     * and trend detection. This helps identify performance degradation early.
     */
    private void recordQueryMetrics(QueryPattern pattern) {
        String queryType = categorizeQuery(pattern.getQueryText());

        metricsService.recordHistogram(
            "clickhouse.query.duration",
            pattern.getAvgDuration(),
            "type", queryType
        );

        metricsService.recordHistogram(
            "clickhouse.query.memory",
            pattern.getAvgMemory(),
            "type", queryType
        );

        metricsService.recordHistogram(
            "clickhouse.query.rows",
            pattern.getAvgRowsRead(),
            "type", queryType
        );
    }

    /**
     * Categorizes queries by type for better metric aggregation and analysis.
     * This helps identify patterns in different types of queries.
     */
    private String categorizeQuery(String query) {
        query = query.toLowerCase();
        
        if (query.startsWith("select")) {
            if (query.contains("group by")) {
                return "aggregate";
            } else if (query.contains("join")) {
                return "join";
            } else {
                return "select";
            }
        } else if (query.startsWith("insert")) {
            return "insert";
        } else if (query.startsWith("alter")) {
            return "alter";
        } else {
            return "other";
        }
    }
}

@Data
public class QueryAnalysisReport {
    private List<QueryIssue> issues = new ArrayList<>();
    private Map<String, List<QueryPattern>> queryPatterns = new HashMap<>();
    private Map<String, QueryOptimizationSuggestion> suggestions = 
        new HashMap<>();

    public void addIssue(QueryIssue issue) {
        issues.add(issue);
    }

    public void addQueryPattern(String category, QueryPattern pattern) {
        queryPatterns.computeIfAbsent(
            category, 
            k -> new ArrayList<>()
        ).add(pattern);
    }

    public void addSuggestion(
            String queryHash, 
            QueryOptimizationSuggestion suggestion) {
        suggestions.put(queryHash, suggestion);
    }

    public String generateSummary() {
        StringBuilder summary = new StringBuilder();
        summary.append("Query Analysis Summary\n\n");

        // Summarize issues
        summary.append("Issues Found: ").append(issues.size())
            .append("\n");
        issues.stream()
            .collect(Collectors.groupingBy(
                QueryIssue::getSeverity, 
                Collectors.counting()
            ))
            .forEach((severity, count) -> 
                summary.append("- ")
                    .append(severity)
                    .append(": ")
                    .append(count)
                    .append("\n")
            );

        // Summarize query patterns
        summary.append("\nQuery Patterns:\n");
        queryPatterns.forEach((category, patterns) -> 
            summary.append("- ")
                .append(category)
                .append(": ")
                .append(patterns.size())
                .append(" queries\n")
        );

        return summary.toString();
    }
}

@Data
@AllArgsConstructor
public class QueryIssue {
    private IssueSeverity severity;
    private String title;
    private String description;
    private String recommendation;
}

@Data
public class QueryOptimizationSuggestion {
    private String queryHash;
    private List<String> suggestions = new ArrayList<>();
    private Map<String, Object> metrics = new HashMap<>();
    private boolean requiresSchemaChange;
    private boolean requiresDataMigration;
}

enum IssueSeverity {
    HIGH, MEDIUM, LOW
}
```

### 6.6 Table Optimization Service

Let's implement a service that helps maintain optimal table performance over time:

```java
@Service
@Slf4j
public class TableOptimizationService {

    private final ClickHouseTemplate clickHouseTemplate;
    private final MetricsService metricsService;

    /**
     * Analyzes table structure and usage patterns to recommend optimizations.
     * This helps maintain optimal performance as data volumes grow.
     */
    public TableOptimizationReport analyzeTable(
            String database, 
            String table) {
        TableOptimizationReport report = new TableOptimizationReport();
        
        // Analyze table structure
        analyzeTableStructure(database, table, report);
        
        // Analyze partitioning
        analyzePartitioning(database, table, report);
        
        // Analyze indexes
        analyzeIndexes(database, table, report);
        
        // Check compression
        analyzeCompression(database, table, report);
        
        return report;
    }

    /**
     * Analyzes table structure including column types, ordering, and settings.
     * This helps identify potential structural improvements.
     */
    private void analyzeTableStructure(
            String database,
            String table,
            TableOptimizationReport report) {
        String structureQuery = """
            SELECT
                name,
                type,
                compression_codec,
                position
            FROM system.columns
            WHERE database = ? AND table = ?
            ORDER BY position
            """;

        List<ColumnInfo> columns = clickHouseTemplate.query(
            structureQuery,
            (rs, rowNum) -> new ColumnInfo(
                rs.getString("name"),
                rs.getString("type"),
                rs.getString("compression_codec"),
                rs.getInt("position")
            ),
            database,
            table
        );

        // Analyze column types and suggest optimizations
        for (ColumnInfo column : columns) {
            analyzeColumnType(column, report);
        }

        // Check primary key composition
        checkPrimaryKey(database, table, columns, report);
    }

    /**
     * Analyzes partitioning strategy and suggests improvements based on
     * data distribution and query patterns.
     */
    private void analyzePartitioning(
            String database,
            String table,
            TableOptimizationReport report) {
        String partitionQuery = """
            SELECT
                partition,
                sum(rows) as partition_rows,
                sum(bytes) as partition_bytes
            FROM system.parts
            WHERE database = ? AND table = ?
            GROUP BY partition
            ORDER BY partition_rows DESC
            """;

        List<PartitionInfo> partitions = clickHouseTemplate.query(
            partitionQuery,
            (rs, rowNum) -> new PartitionInfo(
                rs.getString("partition"),
                rs.getLong("partition_rows"),
                rs.getLong("partition_bytes")
            ),
            database,
            table
        );

        // Analyze partition size distribution
        analyzePartitionDistribution(partitions, report);
    }

    /**
     * Analyzes existing indexes and their effectiveness based on
     * query patterns and data access patterns.
     */
    private void analyzeIndexes(
            String database,
            String table,
            TableOptimizationReport report) {
        String indexQuery = """
            SELECT
                name,
                type,
                column_names,
                data_compression_method
            FROM system.indexes
            WHERE database = ? AND table = ?
            """;

        List<IndexInfo> indexes = clickHouseTemplate.query(
            indexQuery,
            (rs, rowNum) -> new IndexInfo(
                rs.getString("name"),
                rs.getString("type"),
                rs.getString("column_names"),
                rs.getString("data_compression_method")
            ),
            database,
            table
        );

        // Analyze index usage and effectiveness
        analyzeIndexEffectiveness(database, table, indexes, report);
    }

    /**
     * Analyzes compression settings and suggests improvements based on
     * data characteristics and access patterns.
     */
    private void analyzeCompression(
            String database,
            String table,
            TableOptimizationReport report) {
        String compressionQuery = """
            SELECT
                column,
                sum(compressed_size) as compressed_size,
                sum(uncompressed_size) as uncompressed_size,
                min(compression_ratio) as min_ratio,
                max(compression_ratio) as max_ratio,
                avg(compression_ratio) as avg_ratio
            FROM (
                SELECT
                    name as column,
                    data_compressed_bytes as compressed_size,
                    data_uncompressed_bytes as uncompressed_size,
                    data_uncompressed_bytes / data_compressed_bytes 
                        as compression_ratio
                FROM system.columns
                WHERE database = ? AND table = ?
            )
            GROUP BY column
            """;

        List<CompressionInfo> compressionStats = clickHouseTemplate.query(
            compressionQuery,
            (rs, rowNum) -> new CompressionInfo(
                rs.getString("column"),
                rs.getLong("compressed_size"),
                rs.getLong("uncompressed_size"),
                rs.getDouble("min_ratio"),
                rs.getDouble("max_ratio"),
                rs.getDouble("avg_ratio")
            ),
            database,
            table
        );

        // Analyze compression effectiveness
        analyzeCompressionEffectiveness(compressionStats, report);
    }}            // Freeze table to create a consistent point-in-time backup
            String freezeQuery = String.format("""
                ALTER TABLE %s.%s FREEZE
                """, 
                database, 
                table
            );
            
            clickHouseTemplate.update(freezeQuery);
            
            // Copy shadow directory to backup location
            Path shadowPath = Paths.get("/var/lib/clickhouse/shadow");
            Path backupPath = Paths.get(backupDirectory.toString(), 
                generateBackupName(database, table));
                
            Files.walk(shadowPath)
                .filter(path -> !Files.isDirectory(path))
                .forEach(source -> {
                    try {
                        Path relativePath = shadowPath.relativize(source);
                        Path targetPath = backupPath.resolve(relativePath);
                        
                        Files.createDirectories(targetPath.getParent());
                        Files.copy(source, targetPath, 
                            StandardCopyOption.REPLACE_EXISTING);
                            
                    } catch (IOException e) {
                        throw new ClickHouseBackupException(
                            "Failed to copy backup files", e);
                    }
                });
                
            // Create metadata backup
            backupMetadata(database, table, backupPath);
            
            log.info("Successfully created backup for {}.{} at {}", 
                database, table, backupPath);
                
        } catch (Exception e) {
            log.error("Backup creation failed for {}.{}", 
                database, table, e);
            throw new ClickHouseBackupException(
                "Failed to create backup", e);
        } finally {
            // Clean up shadow directory
            cleanupShadow();
        }
    }
    
    /**
     * Restores a table or database from a previously created backup.
     * This method ensures data consistency during the restore process.
     */
    public void restoreBackup(String database, 
            String table, 
            String backupName) {
        try {
            // Verify backup exists and is valid
            Path backupPath = Paths.get(backupDirectory.toString(), 
                backupName);
                
            if (!isValidBackup(backupPath)) {
                throw new InvalidBackupException(
                    "Backup validation failed");
            }
            
            // Restore metadata first
            restoreMetadata(database, table, backupPath);
            
            // Stop inserts to ensure consistency
            String readOnlyQuery = String.format("""
                ALTER TABLE %s.%s MODIFY SETTING read_only=1
                """,
                database,
                table
            );
            
            clickHouseTemplate.update(readOnlyQuery);
            
            try {
                // Detach current table
                String detachQuery = String.format("""
                    ALTER TABLE %s.%s DETACH
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(detachQuery);
                
                // Copy backup files to data directory
                Path dataPath = Paths.get("/var/lib/clickhouse/data", 
                    database, 
                    table);
                    
                Files.walk(backupPath)
                    .filter(path -> !Files.isDirectory(path))
                    .forEach(source -> {
                        try {
                            Path relativePath = backupPath
                                .relativize(source);
                            Path targetPath = dataPath
                                .resolve(relativePath);
                            
                            Files.createDirectories(targetPath
                                .getParent());
                            Files.copy(source, targetPath, 
                                StandardCopyOption.REPLACE_EXISTING);
                                
                        } catch (IOException e) {
                            throw new ClickHouseRestoreException(
                                "Failed to copy restore files", e);
                        }
                    });
                
                // Attach table back
                String attachQuery = String.format("""
                    ALTER TABLE %s.%s ATTACH
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(attachQuery);
                
            } finally {
                // Re-enable writes
                String writeEnableQuery = String.format("""
                    ALTER TABLE %s.%s MODIFY SETTING read_only=0
                    """,
                    database,
                    table
                );
                
                clickHouseTemplate.update(writeEnableQuery);
            }
            
            log.info("Successfully restored {}.{} from backup {}", 
                database, table, backupName);
                
        } catch (Exception e) {
            log.error("Restore failed for {}.{} from backup {}", 
                database, table, backupName, e);
            throw new ClickHouseRestoreException(
                "Failed to restore from backup", e);
        }
    }
    
    /**
     * Backs up table metadata including schema, settings, and configurations.
     * This ensures the table can be properly reconstructed during restore.
     */
    private void backupMetadata(String database, 
            String table, 
            Path backupPath) {
        try {
            // Get table metadata
            String metadataQuery = String.format("""
                SELECT create_table_query, 
                       engine,
                       partition_key,
                       sorting_key,
                       primary_key,
                       sampling_key
                FROM system.tables
                WHERE database = ? AND name = ?
                """);
                
            TableMetadata metadata = clickHouseTemplate.query(
                metadataQuery,
                (rs, rowNum) -> new TableMetadata(
                    rs.getString("create_table_query"),
                    rs.getString("engine"),
                    rs.getString("partition_key"),
                    rs.getString("sorting_key"),
                    rs.getString("primary_key"),
                    rs.getString("sampling_key")
                ),
                database,
                table
            ).get(0);
            
            // Save metadata to backup location
            Path metadataPath = backupPath.resolve("metadata.json");
            ObjectMapper mapper = new ObjectMapper();
            mapper.writeValue(metadataPath.toFile(), metadata);
            
        } catch (Exception e) {
            throw new ClickHouseBackupException(
                "Failed to backup metadata", e);
        }
    }
    
    /**
     * Restores table metadata before data restoration.
     * This ensures the table is properly configured for the restored data.
     */
    private void restoreMetadata(String database, 
            String table, 
            Path backupPath) {
        try {
            // Read metadata from backup
            Path metadataPath = backupPath.resolve("metadata.json");
            ObjectMapper mapper = new ObjectMapper();
            TableMetadata metadata = mapper.readValue(
                metadataPath.toFile(), 
                TableMetadata.class
            );
            
            // Drop existing table if exists
            String dropQuery = String.format("""
                DROP TABLE IF EXISTS %s.%s
                """,
                database,
                table
            );
            
            clickHouseTemplate.update(dropQuery);
            
            // Recreate table using backed up metadata
            clickHouseTemplate.update(metadata.getCreateTableQuery());
            
        } catch (Exception e) {
            throw new ClickHouseRestoreException(
                "Failed to restore metadata", e);
        }
    }
    
    private String generateBackupPath(String database, String table) {
        return Paths.get(
            backupDirectory.toString(),
            generateBackupName(database, table)
        ).toString();
    }
    
    private String generateBackupName(String database, String table) {
        return String.format("%s_%s_%s", 
            database,
            table,
            LocalDateTime.now().format(
                DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss")
            )
        );
    }
    
    private boolean isValidBackup(Path backupPath) {
        // Verify backup structure and integrity
        if (!Files.exists(backupPath)) {
            return false;
        }
        
        // Check metadata file exists
        if (!Files.exists(backupPath.resolve("metadata.json"))) {
            return false;
        }
        
        // Verify backup files
        try {
            return Files.walk(backupPath)
                .filter(path -> !Files.isDirectory(path))
                .allMatch(this::verifyBackupFile);
        } catch (IOException e) {
            return false;
        }
    }
    
    private boolean verifyBackupFile(Path file) {
        try {
            // Verify file integrity
            return Files.size(file) > 0;
        } catch (IOException e) {
            return false;
        }
    }
    
    private void cleanupShadow() {
        try {
            Path shadowPath = Paths.get("/var/lib/clickhouse/shadow");
            if (Files.exists(shadowPath)) {
                Files.walk(shadowPath)
                    .sorted(Comparator.reverseOrder())
                    .forEach(path -> {
                        try {
                            Files.delete(path);
                        } catch (IOException e) {
                            log.warn("Failed to cleanup shadow file: {}", 
                                path, e);
                        }
                    });
            }
        } catch (IOException e) {
            log.warn("Failed to cleanup shadow directory", e);
        }
    }
}

@Data
@AllArgsConstructor
@NoArgsConstructor
public class TableMetadata {
    private String createTableQuery;
    private String engine;
    private String partitionKey;
    private String sortingKey;
    private String primaryKey;
    private String samplingKey;
}
```

### 6.4 Monitoring and Alerting

Let's implement a comprehensive monitoring system that tracks key ClickHouse metrics and alerts on potential issues:

```java
@Service
@Slf4j
public class ClickHouseMonitoringService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final AlertService alertService;
    private final MeterRegistry meterRegistry;
    
    /**
     * Collects and analyzes system metrics to identify potential issues
     * before they affect production workloads.
     */
    @Scheduled(fixedRate = 60000) // Every minute
    public void collectSystemMetrics() {
        try {
            // Collect memory metrics
            collectMemoryMetrics();
            
            // Monitor query performance
            monitorQueryPerformance();
            
            // Check disk usage
            checkDiskUsage();
            
            // Monitor replication status
            checkReplicationStatus();
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            alertService.sendAlert(
                AlertLevel.ERROR,
                "Monitoring failed",
                "System metrics collection failed: " + e.getMessage()
            );
        }
    }
    
    private void collectMemoryMetrics() {
        String memoryQuery = """
            SELECT metric, value
            FROM system.metrics
            WHERE metric LIKE '%Memory%'
            """;
            
        clickHouseTemplate.query(
            memoryQuery,
            (rs, rowNum) -> {
                String metric = rs.getString("metric");
                double value = rs.getDouble("value");
                
                // Record metric
                meterRegistry.gauge(
                    "clickhouse.memory." + metric.toLowerCase(),
                    value
                );
                
                // Check thresholds
                if (metric.equals("MemoryTracking") && 
                        value > 0.9 * getTotalMemory()) {
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Memory Usage",
                        String.format(
                            "Memory usage is at %.2f%% of total", 
                            (value / getTotalMemory()) * 100
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void monitorQueryPerformance() {
        String queryMetricsQuery = """
            SELECT
                type,
                query,
                query_duration_ms,
                memory_usage,
                read_rows,
                written_rows
            FROM system.query_log
            WHERE event_time >= now() - INTERVAL 1 MINUTE
                AND type = 'QueryFinish'
            """;
            
        clickHouseTemplate.query(
            queryMetricsQuery,
            (rs, rowNum) -> {
                long duration = rs.getLong("query_duration_ms");
                long memoryUsage = rs.getLong("memory_usage");
                
                // Record metrics
                meterRegistry.timer("clickhouse.query.duration")
                    .record(duration, TimeUnit.MILLISECONDS);
                meterRegistry.gauge("clickhouse.query.memory", 
                    memoryUsage);
                
                // Alert on slow queries
                if (duration > 10000) { // 10 seconds
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "Slow Query Detected",
                        String.format(
                            "Query took %dms: %s",
                            duration,
                            rs.getString("query")
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void checkDiskUsage() {
        String diskQuery = """
            SELECT
                name,
                free_space,
                total_space
            FROM system.disks
            """;
            
        clickHouseTemplate.query(
            diskQuery,
            (rs, rowNum) -> {
                String name = rs.getString("name");
                long freeSpace = rs.getLong("free_space");
                long totalSpace = rs.getLong("total_space");
                double usagePercent = (1 - (double)freeSpace/totalSpace) 
                    * 100;
                
                // Record metrics
                meterRegistry.gauge(
                    "clickhouse.disk.usage.percent", 
                    usagePercent
                );
                
                // Alert on high disk usage
                if (usagePercent > 85) {
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Disk Usage",
                        String.format(
                            "Disk %s usage is at %.2f%%",
                            name,
                            usagePercent
                        )
                    );
                }
                
                return null;
            }
        );
    }
    
    private void checkReplicationStatus() {
        String replicationQuery = """
            SELECT
                database,
                table,
                absolute_delay,
                total_replicas,
                active_replicas
            FROM system.replicas
            """;
            
        clickHouseTemplate.query(
            replicationQuery,
            (rs, rowNum) -> {
                String database = rs.getString("database");
                String table = rs.getString("table");
                long delay = rs.getLong("absolute_delay");
                int totalReplicas = rs.getInt("total_replicas");
                int activeReplicas = rs.getInt("active_replicas");
                
                // Record metrics
                meterRegistry.gauge(
                    "clickhouse.replication.delay",
                    delay
                );
                meterRegistry.gauge(
                    "clickhouse.replication.active_replicas",
                    activeReplicas
                );
                
                // Alert on replication issues
                if (delay > 300) { // 5 minutes
                    alertService.sendAlert(
                        AlertLevel.WARNING,
                        "High Replication Delay",
                        String.format(
                            "Table %s.%s has %d seconds replication delay",
                            database,
                            table,
                            delay
                        )
                    );
                }            clickHouseTemplate.update(cleanupQuery, cutoffDate);
            
            log.info("Archived data from {} to {} before {}", 
                sourceTable, archiveTable, cutoffDate);
                
        } catch (Exception e) {
            log.error("Failed to archive data for table: {}", 
                sourceTable, e);
            throw new ClickHouseOperationException(
                "Data archiving failed", e);
        }
    }
}

## 6. Production Best Practices

### 6.1 Performance Optimization Strategies

When deploying ClickHouse in production, several key optimization strategies can significantly improve performance. Let's implement a comprehensive optimization service:

```java
@Service
@Slf4j
public class ClickHouseOptimizationService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final ClickHouseMetricsCollector metricsCollector;
    
    /**
     * Analyzes and optimizes table structure based on query patterns and data volume.
     * This helps maintain high query performance as data grows.
     */
    public TableOptimizationPlan analyzeTablePerformance(String tableName) {
        TableOptimizationPlan plan = new TableOptimizationPlan();
        
        // Analyze table statistics
        String statsQuery = """
            SELECT
                table,
                total_rows,
                total_bytes,
                primary_key_bytes_in_memory,
                bytes_on_disk
            FROM system.tables
            WHERE name = ?
            """;
            
        clickHouseTemplate.query(
            statsQuery,
            (rs) -> {
                plan.setTotalRows(rs.getLong("total_rows"));
                plan.setTotalBytes(rs.getLong("total_bytes"));
                plan.setPrimaryKeyMemory(
                    rs.getLong("primary_key_bytes_in_memory"));
                plan.setDiskUsage(rs.getLong("bytes_on_disk"));
                return null;
            },
            tableName
        );
        
        // Analyze query patterns
        String queryPatternsQuery = """
            SELECT
                query_kind,
                count() as query_count,
                avg(query_duration_ms) as avg_duration,
                max(read_rows) as max_rows_read,
                max(memory_usage) as max_memory
            FROM system.query_log
            WHERE table = ?
                AND event_time >= now() - INTERVAL 1 DAY
            GROUP BY query_kind
            """;
            
        clickHouseTemplate.query(
            queryPatternsQuery,
            (rs) -> {
                QueryPattern pattern = new QueryPattern(
                    rs.getString("query_kind"),
                    rs.getLong("query_count"),
                    rs.getDouble("avg_duration"),
                    rs.getLong("max_rows_read"),
                    rs.getLong("max_memory")
                );
                plan.addQueryPattern(pattern);
                return null;
            },
            tableName
        );
        
        // Generate optimization recommendations
        plan.generateRecommendations();
        
        return plan;
    }
    
    /**
     * Implements the recommended optimizations for a table.
     * This method carefully applies changes while ensuring data consistency.
     */
    public void applyOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        try {
            // Apply index optimizations
            if (plan.isIndexOptimizationNeeded()) {
                applyIndexOptimizations(tableName, plan);
            }
            
            // Optimize partitioning
            if (plan.isPartitioningOptimizationNeeded()) {
                optimizePartitioning(tableName, plan);
            }
            
            // Adjust compression settings
            if (plan.isCompressionOptimizationNeeded()) {
                optimizeCompression(tableName, plan);
            }
            
            log.info("Successfully applied optimizations for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to apply optimizations for table: {}", 
                tableName, e);
            throw new ClickHouseOptimizationException(
                "Optimization application failed", e);
        }
    }
    
    private void applyIndexOptimizations(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply skip indexes for frequently filtered columns
        for (String column : plan.getFrequentlyFilteredColumns()) {
            String indexQuery = String.format("""
                ALTER TABLE %s
                ADD INDEX %s_idx %s TYPE minmax GRANULARITY 3
                """,
                tableName,
                column,
                column
            );
            
            clickHouseTemplate.update(indexQuery);
        }
        
        // Optimize primary key if needed
        if (plan.isPrimaryKeyOptimizationNeeded()) {
            String primaryKeyQuery = String.format("""
                ALTER TABLE %s
                MODIFY ORDER BY %s
                """,
                tableName,
                String.join(", ", plan.getRecommendedPrimaryKeyColumns())
            );
            
            clickHouseTemplate.update(primaryKeyQuery);
        }
    }
    
    private void optimizePartitioning(
            String tableName, 
            TableOptimizationPlan plan) {
        if (plan.getTotalRows() > 100_000_000) {
            // For large tables, implement more granular partitioning
            String partitioningQuery = String.format("""
                ALTER TABLE %s
                MODIFY PARTITION BY toYYYYMM(timestamp)
                """,
                tableName
            );
            
            clickHouseTemplate.update(partitioningQuery);
        }
    }
    
    private void optimizeCompression(
            String tableName, 
            TableOptimizationPlan plan) {
        // Apply column-specific compression settings
        for (Map.Entry<String, String> entry : 
                plan.getCompressionRecommendations().entrySet()) {
            String compressionQuery = String.format("""
                ALTER TABLE %s
                MODIFY COLUMN %s %s CODEC(%s)
                """,
                tableName,
                entry.getKey(),
                getColumnType(tableName, entry.getKey()),
                entry.getValue()
            );
            
            clickHouseTemplate.update(compressionQuery);
        }
    }
    
    private String getColumnType(String tableName, String columnName) {
        String query = String.format("""
            SELECT type
            FROM system.columns
            WHERE table = ? AND name = ?
            """);
            
        return clickHouseTemplate.query(
            query,
            (rs, rowNum) -> rs.getString("type"),
            tableName,
            columnName
        ).get(0);
    }
}

@Data
public class TableOptimizationPlan {
    private long totalRows;
    private long totalBytes;
    private long primaryKeyMemory;
    private long diskUsage;
    private List<QueryPattern> queryPatterns = new ArrayList<>();
    private List<String> recommendedPrimaryKeyColumns = new ArrayList<>();
    private Map<String, String> compressionRecommendations = new HashMap<>();
    private Set<String> frequentlyFilteredColumns = new HashSet<>();
    
    public void generateRecommendations() {
        analyzeQueryPatterns();
        generateCompressionRecommendations();
        optimizePrimaryKey();
    }
    
    private void analyzeQueryPatterns() {
        // Analyze query patterns to identify optimization opportunities
        queryPatterns.forEach(pattern -> {
            if (pattern.getQueryCount() > 1000 && 
                    pattern.getAvgDuration() > 1000) {
                // High-frequency, slow queries need optimization
                analyzeQueryForOptimization(pattern);
            }
        });
    }
    
    private void analyzeQueryForOptimization(QueryPattern pattern) {
        // Extract frequently filtered columns from query pattern
        // This is a simplified example - in practice, you would
        // implement more sophisticated query analysis
        if (pattern.getMaxRowsRead() > totalRows * 0.5) {
            // Query is scanning too much data, might need better indexing
            extractFilterColumns(pattern).forEach(column ->
                frequentlyFilteredColumns.add(column)
            );
        }
    }
    
    private Set<String> extractFilterColumns(QueryPattern pattern) {
        // In practice, implement query parsing to extract filter columns
        return new HashSet<>();
    }
    
    private void generateCompressionRecommendations() {
        // Generate compression recommendations based on data patterns
        // This is a simplified example - in practice, you would analyze
        // actual data patterns and column value distributions
        queryPatterns.forEach(pattern -> {
            if (pattern.getMaxMemory() > 1024 * 1024 * 1024) {
                // High memory usage queries might benefit from better compression
                recommendHighCompressionForLargeColumns(pattern);
            }
        });
    }
    
    private void recommendHighCompressionForLargeColumns(QueryPattern pattern) {
        // Add compression recommendations for columns with high memory usage
        // In practice, you would analyze column-specific characteristics
        compressionRecommendations.put(
            "high_cardinality_column", 
            "ZSTD(3)"
        );
        compressionRecommendations.put(
            "low_cardinality_column",
            "LowCardinality"
        );
    }
    
    private void optimizePrimaryKey() {
        // Analyze query patterns to recommend optimal primary key order
        // This is a simplified example - in practice, you would analyze
        // actual query access patterns and data distribution
        if (!frequentlyFilteredColumns.isEmpty()) {
            recommendedPrimaryKeyColumns.addAll(frequentlyFilteredColumns);
        }
    }
}

@Data
@AllArgsConstructor
public class QueryPattern {
    private String queryKind;
    private long queryCount;
    private double avgDuration;
    private long maxRowsRead;
    private long maxMemory;
}
```

### 6.2 Fault Tolerance and High Availability

Implementing robust fault tolerance mechanisms for ClickHouse deployments:

```java
@Service
@Slf4j
public class ClickHouseHighAvailabilityService {
    
    private final List<ClickHouseTemplate> clickHouseTemplates;
    private final LoadBalancer loadBalancer;
    private final CircuitBreaker circuitBreaker;
    
    /**
     * Executes queries with automatic failover and retry mechanisms.
     * This ensures continuous operation even when some nodes are unavailable.
     */
    public <T> T executeWithFailover(ClickHouseOperation<T> operation) {
        Exception lastException = null;
        
        for (int attempt = 0; attempt < 3; attempt++) {
            ClickHouseTemplate template = loadBalancer.getNextAvailable();
            
            if (template == null) {
                throw new NoAvailableNodesException(
                    "No ClickHouse nodes available");
            }
            
            try {
                if (!circuitBreaker.isAllowed()) {
                    continue;
                }
                
                T result = operation.execute(template);
                circuitBreaker.recordSuccess();
                return result;
                
            } catch (Exception e) {
                lastException = e;
                circuitBreaker.recordFailure();
                loadBalancer.markNodeAsFailed(template);
                
                log.warn("Query execution failed on attempt {}: {}", 
                    attempt + 1, e.getMessage());
            }
        }
        
        throw new ClickHouseFailoverException(
            "All failover attempts exhausted", lastException);
    }
}

@Component
@Slf4j
public class LoadBalancer {
    private final List<ClickHouseTemplate> availableNodes;
    private final AtomicInteger currentIndex = new AtomicInteger(0);
    private final Map<ClickHouseTemplate, HealthStatus> nodeStatus;
    
    public LoadBalancer(List<ClickHouseTemplate> nodes) {
        this.availableNodes = new CopyOnWriteArrayList<>(nodes);
        this.nodeStatus = new ConcurrentHashMap<>();
        
        nodes.forEach(node -> 
            nodeStatus.put(node, new HealthStatus()));
            
        // Start health check schedule
        startHealthChecks();
    }
    
    public ClickHouseTemplate getNextAvailable() {
        int attempts = availableNodes.size();
        
        while (attempts > 0) {
            ClickHouseTemplate node = availableNodes.get(
                currentIndex.getAndIncrement() % availableNodes.size()
            );
            
            if (isNodeHealthy(node)) {
                return node;
            }
            
            attempts--;
        }
        
        return null;
    }
    
    public void markNodeAsFailed(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        status.recordFailure();
        
        if (status.getConsecutiveFailures() >= 3) {
            availableNodes.remove(node);
            log.warn("Removed unhealthy node from rotation");
        }
    }
    
    private boolean isNodeHealthy(ClickHouseTemplate node) {
        HealthStatus status = nodeStatus.get(node);
        return status.isHealthy() && checkNodeConnection(node);
    }
    
    private boolean checkNodeConnection(ClickHouseTemplate node) {
        try {
            node.query("SELECT 1", (rs, rowNum) -> 1);
            return true;
        } catch (Exception e) {
            log.warn("Node health check failed: {}", e.getMessage());
            return false;
        }
    }
    
    private void startHealthChecks() {
        ScheduledExecutorService executor = 
            Executors.newSingleThreadScheduledExecutor();
            
        executor.scheduleAtFixedRate(
            this::performHealthChecks,
            0,
            30,
            TimeUnit.SECONDS
        );
    }
    
    private void performHealthChecks() {
        nodeStatus.forEach((node, status) -> {
            if (checkNodeConnection(node)) {
                status.recordSuccess();
                if (!availableNodes.contains(node)) {
                    availableNodes.add(node);
                    log.info("Restored healthy node to rotation");
                }
            } else {
                markNodeAsFailed(node);
            }
        });
    }
}

@Data
public class HealthStatus {
    private AtomicInteger consecutiveFailures = new AtomicInteger(0);
    private AtomicLong lastSuccess = new AtomicLong(0);
    
    public void recordSuccess() {
        consecutiveFailures.set(0);
        lastSuccess.set(System.currentTimeMillis());
    }
    
    public void recordFailure() {
        consecutiveFailures.incrementAndGet();
    }
    
    public boolean isHealthy() {
        return consecutiveFailures.get() < 3 &&
            (System.currentTimeMillis() - lastSuccess.get()) < 60000;
    }
}
```

### 6.3 Backup and Recovery

Implementing comprehensive backup and recovery mechanisms:

```java
@Service
@Slf4j
public class ClickHouseBackupService {
    
    private final ClickHouseTemplate clickHouseTemplate;
    private final Path backupDirectory;
    
    /**
     * Creates a consistent backup of a ClickHouse table or database.
     * This ensures data can be recovered in case of failures.
     */
    public void createBackup(String database, String table) {
        String backupPath = generateBackupPath(database, table);
        
        try {
            // Freeze        } catch (Exception e) {
            log.error("Failed to initialize events table", e);
            throw new ClickHouseInitializationException(
                "Could not create events table", e);
        }
    }
    
    /**
     * Stores a single event in ClickHouse. The method ensures proper handling of
     * the Map type for properties and uses prepared statements for efficiency.
     */
    public void storeEvent(ClickHouseEvent event) {
        try {
            clickHouseTemplate.update(
                INSERT_EVENT_SQL,
                event.getTimestamp(),
                event.getEventType(),
                event.getUserId(),
                convertPropertiesToClickHouseFormat(event.getProperties()),
                event.getValue()
            );
        } catch (Exception e) {
            log.error("Failed to store event: {}", event, e);
            throw new ClickHouseOperationException(
                "Could not store event", e);
        }
    }
    
    /**
     * Performs batch insertion of events for better performance when dealing with
     * multiple records. This method uses the ClickHouse native format for efficient
     * data loading.
     */
    public void storeBatchEvents(List<ClickHouseEvent> events) {
        try {
            clickHouseTemplate.batchInsert(
                "events",
                events,
                new ClickHouseInsertMapper<ClickHouseEvent>() {
                    @Override
                    public String[] getColumns() {
                        return new String[] {
                            "timestamp", "event_type", "user_id", 
                            "properties", "value"
                        };
                    }
                    
                    @Override
                    public void mapRow(PreparedStatement ps, ClickHouseEvent event) 
                            throws SQLException {
                        ps.setObject(1, event.getTimestamp());
                        ps.setString(2, event.getEventType());
                        ps.setString(3, event.getUserId());
                        ps.setObject(4, convertPropertiesToClickHouseFormat(
                            event.getProperties()));
                        ps.setLong(5, event.getValue());
                    }
                }
            );
        } catch (Exception e) {
            log.error("Failed to store batch events", e);
            throw new ClickHouseOperationException(
                "Could not store batch events", e);
        }
    }
    
    /**
     * Queries events within a specific time range. This method demonstrates
     * how to work with ClickHouse's powerful date and time functions.
     */
    public List<ClickHouseEvent> queryEventsByTimeRange(
            LocalDateTime startTime,
            LocalDateTime endTime,
            String eventType) {
        String sql = """
            SELECT
                timestamp,
                event_type,
                user_id,
                properties,
                value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
                AND event_type = ?
            ORDER BY timestamp
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new ClickHouseEvent(
                rs.getObject("timestamp", LocalDateTime.class),
                rs.getString("event_type"),
                rs.getString("user_id"),
                convertClickHouseMapToJava(rs.getString("properties")),
                rs.getLong("value")
            ),
            startTime,
            endTime,
            eventType
        );
    }
    
    /**
     * Performs an analytical query to aggregate events. This demonstrates
     * ClickHouse's analytical capabilities and performance optimizations.
     */
    public Map<String, Long> aggregateEventsByType(
            LocalDateTime startTime,
            LocalDateTime endTime) {
        String sql = """
            SELECT
                event_type,
                count() as event_count,
                sum(value) as total_value
            FROM events
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY event_type
            WITH TOTALS
            """;
            
        return clickHouseTemplate.query(
            sql,
            (rs, rowNum) -> new AbstractMap.SimpleEntry<>(
                rs.getString("event_type"),
                rs.getLong("event_count")
            ),
            startTime,
            endTime
        ).stream().collect(Collectors.toMap(
            Map.Entry::getKey,
            Map.Entry::getValue
        ));
    }
    
    private String convertPropertiesToClickHouseFormat(
            Map<String, String> properties) {
        // Convert Java Map to ClickHouse Map format
        return properties.entrySet().stream()
            .map(e -> String.format("'%s':'%s'",
                e.getKey().replace("'", "\\'"),
                e.getValue().replace("'", "\\'"))
            )
            .collect(Collectors.joining(",", "{", "}"));
    }
    
    private Map<String, String> convertClickHouseMapToJava(String mapStr) {
        // Convert ClickHouse Map string format to Java Map
        if (mapStr == null || mapStr.length() < 2) {
            return Collections.emptyMap();
        }
        
        String content = mapStr.substring(1, mapStr.length() - 1);
        Map<String, String> result = new HashMap<>();
        
        Pattern pattern = Pattern.compile("'([^']*)'\\s*:\\s*'([^']*)'");
        Matcher matcher = pattern.matcher(content);
        
        while (matcher.find()) {
            result.put(
                matcher.group(1).replace("\\'", "'"),
                matcher.group(2).replace("\\'", "'")
            );
        }
        
        return result;
    }
}
```

## 5. Advanced Features and Optimizations

### 5.1 Query Optimization Service

Let's create a service that helps optimize ClickHouse queries for better performance:

```java
@Service
@Slf4j
public class ClickHouseQueryOptimizer {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Analyzes and optimizes a given query using ClickHouse's explain functionality.
     * This helps understand query execution plans and potential optimizations.
     */
    public String analyzeQuery(String query) {
        String explainQuery = String.format(
            "EXPLAIN indexes = 1, actions = 1, description = 1 %s",
            query
        );
        
        try {
            return clickHouseTemplate.query(
                explainQuery,
                (rs, rowNum) -> rs.getString(1)
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze query: {}", query, e);
            throw new ClickHouseQueryException(
                "Query analysis failed", e);
        }
    }
    
    /**
     * Suggests optimal table settings based on data patterns and query types.
     * This helps improve query performance through better table design.
     */
    public TableOptimizationRecommendations analyzeTableStructure(
            String tableName) {
        String analyzeQuery = """
            SELECT
                count() as total_rows,
                max(bytes_on_disk) as disk_usage,
                max(compression_ratio) as compression_ratio
            FROM system.parts
            WHERE table = ?
            """;
            
        try {
            return clickHouseTemplate.query(
                analyzeQuery,
                (rs, rowNum) -> new TableOptimizationRecommendations(
                    rs.getLong("total_rows"),
                    rs.getLong("disk_usage"),
                    rs.getDouble("compression_ratio")
                ),
                tableName
            ).get(0);
        } catch (Exception e) {
            log.error("Failed to analyze table: {}", tableName, e);
            throw new ClickHouseQueryException(
                "Table analysis failed", e);
        }
    }
}

@Data
@AllArgsConstructor
public class TableOptimizationRecommendations {
    private long totalRows;
    private long diskUsage;
    private double compressionRatio;
    
    public String getPartitioningRecommendation() {
        if (totalRows > 100_000_000) {
            return "Consider partitioning by date for better query performance";
        }
        return "Current partitioning scheme appears adequate";
    }
    
    public String getCompressionRecommendation() {
        if (compressionRatio < 3.0) {
            return "Consider using more aggressive compression settings";
        }
        return "Compression ratio is within acceptable range";
    }
}
```

### 5.2 Monitoring and Metrics

Implementing comprehensive monitoring for ClickHouse:

```java
@Configuration
public class ClickHouseMonitoringConfig {
    
    @Bean
    public MeterRegistry meterRegistry() {
        return new SimpleMeterRegistry();
    }
    
    @Bean
    public ClickHouseMetricsCollector metricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        return new ClickHouseMetricsCollector(registry, clickHouseTemplate);
    }
}

@Component
@Slf4j
public class ClickHouseMetricsCollector {
    
    private final MeterRegistry registry;
    private final ClickHouseTemplate clickHouseTemplate;
    private final Timer queryTimer;
    private final Timer insertTimer;
    private final Counter errorCounter;
    
    public ClickHouseMetricsCollector(
            MeterRegistry registry,
            ClickHouseTemplate clickHouseTemplate) {
        this.registry = registry;
        this.clickHouseTemplate = clickHouseTemplate;
        
        // Initialize metrics
        this.queryTimer = Timer.builder("clickhouse.query.duration")
            .description("Time taken for ClickHouse queries")
            .register(registry);
            
        this.insertTimer = Timer.builder("clickhouse.insert.duration")
            .description("Time taken for ClickHouse inserts")
            .register(registry);
            
        this.errorCounter = Counter.builder("clickhouse.errors")
            .description("Number of ClickHouse errors")
            .register(registry);
            
        // Start collecting system metrics
        scheduleMetricsCollection();
    }
    
    private void scheduleMetricsCollection() {
        Executors.newSingleThreadScheduledExecutor()
            .scheduleAtFixedRate(
                this::collectSystemMetrics,
                0,
                1,
                TimeUnit.MINUTES
            );
    }
    
    private void collectSystemMetrics() {
        try {
            // Collect memory metrics
            String memoryQuery = """
                SELECT
                    metric,
                    value
                FROM system.metrics
                WHERE metric IN ('MemoryTracking', 'MemoryTrackingInBackgroundProcesses')
                """;
                
            clickHouseTemplate.query(
                memoryQuery,
                (rs, rowNum) -> {
                    String metric = rs.getString("metric");
                    double value = rs.getDouble("value");
                    registry.gauge(
                        "clickhouse.memory." + metric.toLowerCase(),
                        value
                    );
                    return null;
                }
            );
            
            // Collect query metrics
            String queryMetricsQuery = """
                SELECT
                    event_type,
                    count() as count
                FROM system.query_log
                WHERE event_time >= now() - INTERVAL 1 MINUTE
                GROUP BY event_type
                """;
                
            clickHouseTemplate.query(
                queryMetricsQuery,
                (rs, rowNum) -> {
                    String eventType = rs.getString("event_type");
                    long count = rs.getLong("count");
                    registry.counter(
                        "clickhouse.queries",
                        "type",
                        eventType
                    ).increment(count);
                    return null;
                }
            );
            
        } catch (Exception e) {
            log.error("Failed to collect system metrics", e);
            errorCounter.increment();
        }
    }
    
    public Timer.Sample startQueryTimer() {
        return Timer.start(registry);
    }
    
    public void stopQueryTimer(Timer.Sample sample) {
        sample.stop(queryTimer);
    }
    
    public Timer.Sample startInsertTimer() {
        return Timer.start(registry);
    }
    
    public void stopInsertTimer(Timer.Sample sample) {
        sample.stop(insertTimer);
    }
    
    public void recordError(String errorType) {
        errorCounter.increment();
        registry.counter(
            "clickhouse.errors",
            "type",
            errorType
        ).increment();
    }
}
```

### 5.3 Data Lifecycle Management

Implementing data lifecycle management for ClickHouse tables:

```java
@Service
@Slf4j
public class ClickHouseDataLifecycleManager {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    /**
     * Manages the lifecycle of data in ClickHouse tables, including
     * partitioning, TTL, and data archiving strategies.
     */
    public void configureTableLifecycle(
            String tableName,
            Period retentionPeriod,
            String partitioningColumn) {
        try {
            // Configure TTL for data retention
            String ttlQuery = String.format("""
                ALTER TABLE %s
                MODIFY TTL
                    %s + INTERVAL %d MONTH
                    DELETE
                """,
                tableName,
                partitioningColumn,
                retentionPeriod.toTotalMonths()
            );
            
            clickHouseTemplate.update(ttlQuery);
            
            // Configure storage policy for cold data
            String storagePolicyQuery = String.format("""
                ALTER TABLE %s
                MODIFY SETTING storage_policy = 'tiered'
                """,
                tableName
            );
            
            clickHouseTemplate.update(storagePolicyQuery);
            
            log.info("Configured lifecycle management for table: {}", 
                tableName);
                
        } catch (Exception e) {
            log.error("Failed to configure table lifecycle: {}", 
                tableName, e);
            throw new ClickHouseOperationException(
                "Lifecycle configuration failed", e);
        }
    }
    
    /**
     * Optimizes table storage by merging partitions and reorganizing data.
     * This helps maintain query performance over time.
     */
    public void optimizeTable(String tableName) {
        try {
            String optimizeQuery = String.format(
                "OPTIMIZE TABLE %s FINAL",
                tableName
            );
            
            clickHouseTemplate.update(optimizeQuery);
            log.info("Optimized table: {}", tableName);
            
        } catch (Exception e) {
            log.error("Failed to optimize table: {}", tableName, e);
            throw new ClickHouseOperationException(
                "Table optimization failed", e);
        }
    }
    
    /**
     * Archives old data to a different storage tier while maintaining
     * accessibility for historical analysis.
     */
    public void archiveOldData(
            String sourceTable,
            String archiveTable,
            LocalDateTime cutoffDate) {
        try {
            // Move data to archive table
            String archiveQuery = String.format("""
                INSERT INTO %s
                SELECT *
                FROM %s
                WHERE timestamp < ?
                """,
                archiveTable,
                sourceTable
            );
            
            clickHouseTemplate.update(archiveQuery, cutoffDate);
            
            // Delete archived data from source table
            String cleanupQuery = String.format("""
                ALTER TABLE %s
                DELETE WHERE timestamp < ?
                """,
                sourceTable
            );
            
            clickHouseTemplate.update# ClickHouse Configuration Guide with Spring Boot 3.2.9

## 1. Understanding ClickHouse Architecture

Before diving into configuration details, let's understand why ClickHouse is particularly well-suited for analytical workloads and how its architecture supports high-performance analytics.

ClickHouse is a column-oriented database management system designed for online analytical processing (OLAP). Its architecture is optimized for:
- Fast query processing on large datasets
- Real-time data ingestion
- Efficient compression
- Distributed query execution
- Fault tolerance and high availability

### 1.1 Key Concepts

Several fundamental concepts make ClickHouse powerful:

1. Column-oriented Storage: Data is stored by columns rather than rows, enabling:
   - Better compression ratios
   - Faster analytical queries
   - Efficient scanning of specific columns

2. Data Types and Encodings:
   - Specialized data types for analytics
   - Custom compression methods per column
   - Efficient numeric and string encodings

3. Distributed Architecture:
   - Sharding for horizontal scaling
   - Replication for fault tolerance
   - Distributed query execution

## 2. Single Node Setup

Let's start with a basic single-node ClickHouse installation using Docker, which is perfect for development and testing environments.

### 2.1 Docker-based Installation

```yaml
version: '3.8'
services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
      - "9009:9009"   # Internal port for replication
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config:/etc/clickhouse-server/config.d
      - ./users:/etc/clickhouse-server/users.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

volumes:
  clickhouse_data:
```

### 2.2 Basic Configuration

The configuration consists of several XML files. Here's the basic setup:

```xml
<!-- config.xml -->
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>

    <memory>
        <!-- Recommended to set to 80% of available RAM -->
        <max_server_memory_usage>8589934592</max_server_memory_usage>
        <max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>
    </memory>

    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
        </disks>
        <policies>
            <default>
                <volumes>
                    <default>
                        <disk>default</disk>
                    </default>
                </volumes>
            </default>
        </policies>
    </storage_configuration>

    <!-- Performance settings -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>
    <default_max_threads>8</default_max_threads>
</clickhouse>

<!-- users.xml -->
<?xml version="1.0"?>
<clickhouse>
    <users>
        <default>
            <password>default_password</password>
            <networks>
                <ip>::/0</ip>
            </networks>
            <profile>default</profile>
            <quota>default</quota>
        </default>
    </users>

    <profiles>
        <default>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_bytes_before_external_group_by>50000000000</max_bytes_before_external_group_by>
            <max_bytes_before_external_sort>50000000000</max_bytes_before_external_sort>
        </default>
    </profiles>

    <quotas>
        <default>
            <interval>
                <duration>3600</duration>
                <queries>0</queries>
                <errors>0</errors>
                <result_rows>0</result_rows>
                <read_rows>0</read_rows>
                <execution_time>0</execution_time>
            </interval>
        </default>
    </quotas>
</clickhouse>
```

## 3. Distributed Setup

For production environments, we'll configure a ClickHouse cluster with multiple nodes for high availability and better performance.

### 3.1 Multi-node Cluster Configuration

```yaml
version: '3.8'
services:
  clickhouse1:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse1_data:/var/lib/clickhouse
      - ./config/node1:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=1

  clickhouse2:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse2
    ports:
      - "8124:8123"
      - "9001:9000"
    volumes:
      - clickhouse2_data:/var/lib/clickhouse
      - ./config/node2:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=1
      - CLICKHOUSE_REPLICA=2

  clickhouse3:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse3
    ports:
      - "8125:8123"
      - "9002:9000"
    volumes:
      - clickhouse3_data:/var/lib/clickhouse
      - ./config/node3:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=1

  clickhouse4:
    image: clickhouse/clickhouse-server:23.8
    hostname: clickhouse4
    ports:
      - "8126:8123"
      - "9003:9000"
    volumes:
      - clickhouse4_data:/var/lib/clickhouse
      - ./config/node4:/etc/clickhouse-server/config.d
    environment:
      - CLICKHOUSE_SHARD=2
      - CLICKHOUSE_REPLICA=2

volumes:
  clickhouse1_data:
  clickhouse2_data:
  clickhouse3_data:
  clickhouse4_data:
```

### 3.2 Cluster Configuration

Let's configure the cluster with two shards and two replicas per shard:

```xml
<!-- metrika.xml -->
<?xml version="1.0"?>
<clickhouse>
    <remote_servers>
        <analytics_cluster>
            <!-- First shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse1</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse2</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- Second shard -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>clickhouse3</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>clickhouse4</host>
                    <port>9000</port>
                </replica>
            </shard>
        </analytics_cluster>
    </remote_servers>

    <zookeeper>
        <node>
            <host>zookeeper</host>
            <port>2181</port>
        </node>
    </zookeeper>

    <macros>
        <shard>{CLICKHOUSE_SHARD}</shard>
        <replica>{CLICKHOUSE_REPLICA}</replica>
    </macros>
</clickhouse>
```

## 4. Spring Boot Integration

Now let's integrate ClickHouse with Spring Boot for seamless data access and management.

### 4.1 Dependencies Configuration

Add the required dependencies to your pom.xml:

```xml
<dependencies>
    <!-- Spring Boot Starter -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>
    
    <!-- ClickHouse JDBC Driver -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.5.0</version>
    </dependency>
</dependencies>
```

### 4.2 ClickHouse Configuration

Let's create a comprehensive configuration that supports both single-node and cluster setups:

```java
@Configuration
@EnableTransactionManagement
public class ClickHouseConfig {
    
    @Bean
    @ConfigurationProperties(prefix = "clickhouse.datasource")
    public DataSourceProperties clickHouseDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    public DataSource clickHouseDataSource() {
        ClickHouseDataSource dataSource = new ClickHouseDataSource(
            createClickHouseProperties());
            
        // Configure connection pooling
        HikariConfig config = new HikariConfig();
        config.setDataSource(dataSource);
        config.setMaximumPoolSize(10);
        config.setMinimumIdle(5);
        config.setIdleTimeout(300000);
        config.setConnectionTimeout(20000);
        
        return new HikariDataSource(config);
    }
    
    private ClickHouseProperties createClickHouseProperties() {
        ClickHouseProperties properties = new ClickHouseProperties();
        properties.setUser("default");
        properties.setPassword("default_password");
        properties.setDatabase("default");
        
        // Performance settings
        properties.setUseServerTimeZone(true);
        properties.setUseTimeZone("UTC");
        properties.setUseServerTimeZoneForDates(true);
        
        // Connection settings
        properties.setSocketTimeout(30000);
        properties.setConnectionTimeout(10000);
        
        // Buffer settings
        properties.setBufferSize(65536);
        
        return properties;
    }
    
    @Bean
    public ClickHouseTemplate clickHouseTemplate(
            DataSource clickHouseDataSource) {
        return new ClickHouseTemplate(clickHouseDataSource);
    }
}
```

### 4.3 ClickHouse Template

Let's create a template class for executing queries:

```java
@Component
@Slf4j
public class ClickHouseTemplate {
    
    private final JdbcTemplate jdbcTemplate;
    
    public ClickHouseTemplate(DataSource dataSource) {
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    /**
     * Execute a batch insert operation
     */
    public <T> void batchInsert(String tableName, 
            List<T> records, 
            ClickHouseInsertMapper<T> mapper) {
        String insertSql = createInsertSql(tableName, mapper.getColumns());
        
        jdbcTemplate.batchUpdate(insertSql, 
            new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) 
                        throws SQLException {
                    mapper.mapRow(ps, records.get(i));
                }
                
                @Override
                public int getBatchSize() {
                    return records.size();
                }
            });
    }
    
    /**
     * Execute analytical query
     */
    public <T> List<T> query(String sql, 
            Class<T> resultType, 
            Object... params) {
        try {
            return jdbcTemplate.query(
                sql, 
                new BeanPropertyRowMapper<>(resultType), 
                params
            );
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute analytical query with custom row mapper
     */
    public <T> List<T> query(String sql, 
            RowMapper<T> rowMapper, 
            Object... params) {
        try {
            return jdbcTemplate.query(sql, rowMapper, params);
        } catch (DataAccessException e) {
            log.error("Error executing query: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute query", e);
        }
    }
    
    /**
     * Execute mutation query (INSERT, UPDATE, DELETE)
     */
    public int update(String sql, Object... params) {
        try {
            return jdbcTemplate.update(sql, params);
        } catch (DataAccessException e) {
            log.error("Error executing update: {}", sql, e);
            throw new ClickHouseQueryException(
                "Failed to execute update", e);
        }
    }
    
    private String createInsertSql(String tableName, String[] columns) {
        StringBuilder sql = new StringBuilder()
            .append("INSERT INTO ")
            .append(tableName)
            .append(" (")
            .append(String.join(", ", columns))
            .append(") VALUES (")
            .append(String.join(", ", 
                Collections.nCopies(columns.length, "?")))
            .append(")");
            
        return sql.toString();
    }
}
```

### 4.4 Data Models and Repositories

Example of working with ClickHouse tables:

```java
@Data
@AllArgsConstructor
public class ClickHouseEvent {
    private LocalDateTime timestamp;
    private String eventType;
    private String userId;
    private Map<String, String> properties;
    private Long value;
}

@Repository
@Slf4j
public class ClickHouseEventRepository {
    
    private final ClickHouseTemplate clickHouseTemplate;
    
    private static final String CREATE_TABLE_SQL = """
        CREATE TABLE IF NOT EXISTS events (
            timestamp DateTime,
            event_type String,
            user_id String,
            properties Map(String, String),
            value Int64
        )
        ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/events', '{replica}')
        PARTITION BY toYYYYMM(timestamp)
        ORDER BY (timestamp, event_type)
        """;
        
    private static final String INSERT_EVENT_SQL = """
        INSERT INTO events (
            timestamp, event_type, user_id, properties, value
        ) VALUES (?, ?, ?, ?, ?)
        """;
        
    public ClickHouseEventRepository(ClickHouseTemplate clickHouseTemplate) {
        this.clickHouseTemplate = clickHouseTemplate;
        initializeTable();
    }
    
    private void initializeTable() {
        try {
            clickHouseTemplate.update(CREATE_TABLE_SQL);
        } catch (