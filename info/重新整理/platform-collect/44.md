I'll help you create a comprehensive guide for setting up and configuring various databases with Spring Boot 3.2.9. Let's break this down systematically, starting with MySQL.



I've started with detailed configurations for MySQL and Redis. Let me know if you'd like me to continue with the configurations for:

1. RabbitMQ
2. Kafka
3. Zookeeper
4. ClickHouse
5. MongoDB

For each one, I'll cover:
- Single instance setup
- Cluster configuration
- Multi-source configuration where applicable
- Connection pooling and optimization
- Common usage patterns
- Transaction management
- Error handling and retry mechanisms

Which database would you like me to cover next?

Also, for the configurations I've already provided, would you like me to explain any specific part in more detail or provide additional examples for particular use cases?# Database Configuration Guide for Spring Boot 3.2.9

## MySQL Configuration

### Single Instance Setup

1. First, add the necessary dependencies to your `pom.xml`:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>com.mysql</groupId>
    <artifactId>mysql-connector-j</artifactId>
</dependency>
```

2. Create the base configuration class:

```java
@Configuration
@EnableTransactionManagement
public class MySQLConfig {
    
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }
    
    @Primary
    @Bean(name = "entityManagerFactory")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity")
            .persistenceUnit("primaryPU")
            .build();
    }
    
    @Primary
    @Bean(name = "transactionManager")
    public PlatformTransactionManager transactionManager(
            @Qualifier("entityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

3. Configure application.yml:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false&serverTimezone=UTC
      username: root
      password: root
      hikari:
        maximum-pool-size: 10
        minimum-idle: 5
        idle-timeout: 300000
        connection-timeout: 20000
        connection-test-query: SELECT 1
```

### Multi-Source Configuration

For multiple data sources, extend the configuration:

```java
@Configuration
@EnableTransactionManagement
public class MultiDataSourceConfig {

    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @ConfigurationProperties(prefix = "spring.datasource.secondary")
    public DataSourceProperties secondaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Bean
    public DataSource secondaryDataSource() {
        return secondaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Primary
    @Bean(name = "primaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean primaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity.primary")
            .persistenceUnit("primaryPU")
            .build();
    }

    @Bean(name = "secondaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean secondaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(secondaryDataSource())
            .packages("com.study.collect.domain.entity.secondary")
            .persistenceUnit("secondaryPU")
            .build();
    }

    @Primary
    @Bean(name = "primaryTransactionManager")
    public PlatformTransactionManager primaryTransactionManager(
            @Qualifier("primaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }

    @Bean(name = "secondaryTransactionManager")
    public PlatformTransactionManager secondaryTransactionManager(
            @Qualifier("secondaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

Update application.yml for multiple sources:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false
      username: root
      password: root
    secondary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/secondary_db?useSSL=false
      username: root
      password: root
```

### MySQL Cluster Configuration

For MySQL cluster setup:

1. Update application.yml for cluster configuration:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql:replication://master:3306,slave1:3306,slave2:3306/primary_db?useSSL=false
      username: root
      password: root
      hikari:
        maximum-pool-size: 20
        minimum-idle: 10
        connection-timeout: 30000
```

2. Create a routing data source for read-write splitting:

```java
public class RoutingDataSource extends AbstractRoutingDataSource {
    @Override
    protected Object determineCurrentLookupKey() {
        return TransactionSynchronizationManager.isCurrentTransactionReadOnly() 
            ? "slave" : "master";
    }
}

@Configuration
public class MySQLClusterConfig {
    
    @Bean
    public DataSource routingDataSource(
            @Qualifier("masterDataSource") DataSource masterDataSource,
            @Qualifier("slaveDataSource") DataSource slaveDataSource) {
        Map<Object, Object> targetDataSources = new HashMap<>();
        targetDataSources.put("master", masterDataSource);
        targetDataSources.put("slave", slaveDataSource);
        
        RoutingDataSource routingDataSource = new RoutingDataSource();
        routingDataSource.setTargetDataSources(targetDataSources);
        routingDataSource.setDefaultTargetDataSource(masterDataSource);
        
        return routingDataSource;
    }
}
```

## Redis Configuration

### Single Instance Setup

1. Add dependency:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

2. Create configuration:

```java
@Configuration
public class RedisConfig {
    
    @Bean
    @Primary
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Set serializers
        Jackson2JsonRedisSerializer<Object> jackson2JsonRedisSerializer = 
            new Jackson2JsonRedisSerializer<>(Object.class);
        ObjectMapper om = new ObjectMapper();
        om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        om.activateDefaultTyping(LaissezFaireSubTypeValidator.instance, 
            ObjectMapper.DefaultTyping.NON_FINAL);
        jackson2JsonRedisSerializer.setObjectMapper(om);
        
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        template.setKeySerializer(stringRedisSerializer);
        template.setHashKeySerializer(stringRedisSerializer);
        template.setValueSerializer(jackson2JsonRedisSerializer);
        template.setHashValueSerializer(jackson2JsonRedisSerializer);
        
        return template;
    }
}
```

3. Configure application.yml:

```yaml
spring:
  data:
    redis:
      host: localhost
      port: 6379
      password: 
      database: 0
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

### Redis Cluster Configuration

1. Update configuration for cluster:

```java
@Configuration
public class RedisClusterConfig {
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Serializer configuration remains the same
        // ... 
        
        return template;
    }
    
    @Bean
    public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
        return clientConfigurationBuilder -> {
            clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);
        };
    }
}
```

2. Configure application.yml for cluster:

```yaml
spring:
  data:
    redis:
      cluster:
        nodes:
          - redis-node1:6379
          - redis-node2:6379
          - redis-node3:6379
        max-redirects: 3
      password: 
      lettuce:
        pool:
          max-active: 16
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

More database configurations (RabbitMQ, Kafka, MongoDB, etc.) will be continued in subsequent sections. Would you like me to proceed with any specific database configuration next?

# Database Configuration Guide for Spring Boot 3.2.9

## MySQL Configuration

### Single Instance Setup

1. First, add the necessary dependencies to your `pom.xml`:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>com.mysql</groupId>
    <artifactId>mysql-connector-j</artifactId>
</dependency>
```

2. Create the base configuration class:

```java
@Configuration
@EnableTransactionManagement
public class MySQLConfig {
    
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }
    
    @Primary
    @Bean(name = "entityManagerFactory")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity")
            .persistenceUnit("primaryPU")
            .build();
    }
    
    @Primary
    @Bean(name = "transactionManager")
    public PlatformTransactionManager transactionManager(
            @Qualifier("entityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

3. Configure application.yml:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false&serverTimezone=UTC
      username: root
      password: root
      hikari:
        maximum-pool-size: 10
        minimum-idle: 5
        idle-timeout: 300000
        connection-timeout: 20000
        connection-test-query: SELECT 1
```

### Multi-Source Configuration

For multiple data sources, extend the configuration:

```java
@Configuration
@EnableTransactionManagement
public class MultiDataSourceConfig {

    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @ConfigurationProperties(prefix = "spring.datasource.secondary")
    public DataSourceProperties secondaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Bean
    public DataSource secondaryDataSource() {
        return secondaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Primary
    @Bean(name = "primaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean primaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity.primary")
            .persistenceUnit("primaryPU")
            .build();
    }

    @Bean(name = "secondaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean secondaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(secondaryDataSource())
            .packages("com.study.collect.domain.entity.secondary")
            .persistenceUnit("secondaryPU")
            .build();
    }

    @Primary
    @Bean(name = "primaryTransactionManager")
    public PlatformTransactionManager primaryTransactionManager(
            @Qualifier("primaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }

    @Bean(name = "secondaryTransactionManager")
    public PlatformTransactionManager secondaryTransactionManager(
            @Qualifier("secondaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

Update application.yml for multiple sources:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false
      username: root
      password: root
    secondary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/secondary_db?useSSL=false
      username: root
      password: root
```

### MySQL Cluster Configuration

For MySQL cluster setup:

1. Update application.yml for cluster configuration:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql:replication://master:3306,slave1:3306,slave2:3306/primary_db?useSSL=false
      username: root
      password: root
      hikari:
        maximum-pool-size: 20
        minimum-idle: 10
        connection-timeout: 30000
```

2. Create a routing data source for read-write splitting:

```java
public class RoutingDataSource extends AbstractRoutingDataSource {
    @Override
    protected Object determineCurrentLookupKey() {
        return TransactionSynchronizationManager.isCurrentTransactionReadOnly() 
            ? "slave" : "master";
    }
}

@Configuration
public class MySQLClusterConfig {
    
    @Bean
    public DataSource routingDataSource(
            @Qualifier("masterDataSource") DataSource masterDataSource,
            @Qualifier("slaveDataSource") DataSource slaveDataSource) {
        Map<Object, Object> targetDataSources = new HashMap<>();
        targetDataSources.put("master", masterDataSource);
        targetDataSources.put("slave", slaveDataSource);
        
        RoutingDataSource routingDataSource = new RoutingDataSource();
        routingDataSource.setTargetDataSources(targetDataSources);
        routingDataSource.setDefaultTargetDataSource(masterDataSource);
        
        return routingDataSource;
    }
}
```

## Redis Configuration

### Single Instance Setup

1. Add dependency:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

2. Create configuration:

```java
@Configuration
public class RedisConfig {
    
    @Bean
    @Primary
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Set serializers
        Jackson2JsonRedisSerializer<Object> jackson2JsonRedisSerializer = 
            new Jackson2JsonRedisSerializer<>(Object.class);
        ObjectMapper om = new ObjectMapper();
        om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        om.activateDefaultTyping(LaissezFaireSubTypeValidator.instance, 
            ObjectMapper.DefaultTyping.NON_FINAL);
        jackson2JsonRedisSerializer.setObjectMapper(om);
        
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        template.setKeySerializer(stringRedisSerializer);
        template.setHashKeySerializer(stringRedisSerializer);
        template.setValueSerializer(jackson2JsonRedisSerializer);
        template.setHashValueSerializer(jackson2JsonRedisSerializer);
        
        return template;
    }
}
```

3. Configure application.yml:

```yaml
spring:
  data:
    redis:
      host: localhost
      port: 6379
      password: 
      database: 0
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

### Redis Cluster Configuration

1. Update configuration for cluster:

```java
@Configuration
public class RedisClusterConfig {
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Serializer configuration remains the same
        // ... 
        
        return template;
    }
    
    @Bean
    public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
        return clientConfigurationBuilder -> {
            clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);
        };
    }
}
```

2. Configure application.yml for cluster:

```yaml
spring:
  data:
    redis:
      cluster:
        nodes:
          - redis-node1:6379
          - redis-node2:6379
          - redis-node3:6379
        max-redirects: 3
      password: 
      lettuce:
        pool:
          max-active: 16
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

## RabbitMQ 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class RabbitConfig {
    
    @Bean
    public Queue queue() {
        // durable: 是否持久化
        // exclusive: 是否独占
        // autoDelete: 是否自动删除
        return new Queue("defaultQueue", true, false, false);
    }
    
    @Bean
    public DirectExchange directExchange() {
        return new DirectExchange("defaultExchange", true, false);
    }
    
    @Bean
    public Binding binding(Queue queue, DirectExchange exchange) {
        return BindingBuilder.bind(queue).to(exchange).with("defaultRouting");
    }
    
    @Bean
    public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
        // 消息转换器
        rabbitTemplate.setMessageConverter(new Jackson2JsonMessageConverter());
        // 开启确认模式
        rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -> {
            if (!ack) {
                log.error("消息发送失败：" + cause);
            }
        });
        return rabbitTemplate;
    }
    
    @Bean 
    public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(
            ConnectionFactory connectionFactory) {
        SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        factory.setMessageConverter(new Jackson2JsonMessageConverter());
        // 设置并发消费者数量
        factory.setConcurrentConsumers(3);
        factory.setMaxConcurrentConsumers(10);
        // 设置预取数量
        factory.setPrefetchCount(5);
        return factory;
    }
}
```

3. 配置 application.yml:

```yaml
spring:
  rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest
    virtual-host: /
    # 开启发布确认
    publisher-confirm-type: correlated
    # 开启发布返回
    publisher-returns: true
    listener:
      simple:
        # 手动确认
        acknowledge-mode: manual
        # 最小消费者数量
        concurrency: 3
        # 最大消费者数量
        max-concurrency: 10
        # 预取数量
        prefetch: 5
        # 启动时自动启动容器
        auto-startup: true
        # 消费者默认启动
        default-requeue-rejected: true
```

### 集群配置

1. 更新配置类支持集群:

```java
@Configuration
public class RabbitClusterConfig {
    
    @Bean
    public ConnectionFactory connectionFactory() {
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        // 配置集群地址
        connectionFactory.setAddresses("rabbit1:5672,rabbit2:5672,rabbit3:5672");
        connectionFactory.setUsername("guest");
        connectionFactory.setPassword("guest");
        connectionFactory.setVirtualHost("/");
        // 开启发布确认
        connectionFactory.setPublisherConfirms(true);
        // 开启发布返回
        connectionFactory.setPublisherReturns(true);
        return connectionFactory;
    }
    
    @Bean
    public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) {
        return new RabbitAdmin(connectionFactory);
    }
    
    // 其他Bean配置保持不变...
}
```

2. 集群配置 application.yml:

```yaml
spring:
  rabbitmq:
    addresses: rabbit1:5672,rabbit2:5672,rabbit3:5672
    username: guest
    password: guest
    virtual-host: /
    # 集群配置
    cluster:
      # 节点间通信端口
      node-port: 25672
      # 集群名称
      name: rabbit@cluster
    # 连接池配置  
    connection:
      # 缓存连接数
      cache-size: 20
      # 最大连接数
      max-connections: 50
```

### 使用示例

1. 生产者示例:

```java
@Service
@Slf4j
public class MessageProducer {
    
    @Autowired
    private RabbitTemplate rabbitTemplate;
    
    public void sendMessage(Object message, String routingKey) {
        try {
            // 构建消息
            Message msg = MessageBuilder
                .withBody(new ObjectMapper().writeValueAsBytes(message))
                .setContentType(MessageProperties.CONTENT_TYPE_JSON)
                .setMessageId(UUID.randomUUID().toString())
                .setTimestamp(new Date())
                .build();
                
            // 发送消息并获取结果    
            rabbitTemplate.convertAndSend("defaultExchange", routingKey, msg, messagePostProcessor -> {
                // 设置消息持久化
                messagePostProcessor.getMessageProperties().setDeliveryMode(MessageDeliveryMode.PERSISTENT);
                return messagePostProcessor;
            });
            
        } catch (Exception e) {
            log.error("消息发送失败: ", e);
            throw new RuntimeException("消息发送失败", e);
        }
    }
}
```

2. 消费者示例:

```java
@Component
@Slf4j
public class MessageConsumer {

    @RabbitListener(queues = "defaultQueue")
    public void processMessage(Message message, Channel channel) {
        try {
            // 获取消息标识
            long deliveryTag = message.getMessageProperties().getDeliveryTag();
            
            // 处理消息
            String messageBody = new String(message.getBody());
            log.info("收到消息: {}", messageBody);
            
            // 确认消息
            channel.basicAck(deliveryTag, false);
            
        } catch (Exception e) {
            try {
                // 消息处理失败,拒绝消息
                channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true);
            } catch (IOException ex) {
                log.error("消息拒绝失败: ", ex);
            }
            log.error("消息处理失败: ", e);
        }
    }
}
```

## Kafka 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class KafkaConfig {
    
    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "default-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        // 自动提交配置
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        // 消费者超时时间
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "15000");
        return new DefaultKafkaConsumerFactory<>(props);
    }
    
    @Bean
    public ProducerFactory<String, String> producerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        // 生产者确认机制
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        // 重试次数
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        // 批量大小
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        // 缓冲区大小
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
        return new DefaultKafkaProducerFactory<>(props);
    }
    
    @Bean
    public KafkaTemplate<String, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        // 设置并发数
        factory.setConcurrency(3);
        // 批量监听
        factory.setBatchListener(true);
        return factory;
    }
}
```

3. 配置 application.yml:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      # 重试次数
      retries: 3
      # 批量大小
      batch-size: 16384
      # 缓冲区大小
      buffer-memory: 33554432
      # 键的序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 值的序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 确认机制
      acks: all
    consumer:
      # 默认组id  
      group-id: default-group
      # 自动提交
      enable-auto-commit: false
      # 自动提交间隔
      auto-commit-interval: 1000
      # 键的反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 值的反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 当各分区下有已提交的offset时,从提交的offset开始消费;无提交的offset时,从头开始消费  
      auto-offset-reset: earliest
    listener:
      # 手动提交
      ack-mode: manual
      # 并发数
      concurrency: 3
```

### 集群配置

1. 更新配置类支持集群:

```java
@Configuration
public class KafkaClusterConfig {

    @Bean
    public Map<String, Object> producerConfigs() {
        Map<String, Object> props = new HashMap<>();
        // 集群地址
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
            "kafka1:9092,kafka2:9092,kafka3:9092");
        // 重试次数
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        // 幂等性
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        // 事务ID前缀
        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "tx-");
        // 其他配置保持不变...
        return props;
    }
    
    @Bean
    public Map<String, Object> consumerConfigs() {
        Map<String, Object> props = new HashMap<>();
        // 集群地址
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
            "kafka1:9092,kafka2:9092,kafka3:9092");
        // 隔离级别
        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        // 其他配置保持不变...
        return props;
    }
    
    // 其他Bean配置保持不变...
}
```

2. 集群配置 application.yml:

```yaml
spring:
  kafka:
    bootstrap-servers: kafka1:9092,kafka2:9092,kafka3:9092
    producer:
      # 启用事务
      transaction-id-prefix: tx-
      # 启用幂等性
      properties:
        enable.idempotence: true
      # 其他配置保持不变...
    consumer:
      # 事务隔离级别
      properties:
        isolation.level: read_committed
      # 其他配置保持不变...
```

### 使用示例

1. 生产者示例:

```java
@Service
@Slf4j
public class KafkaProducer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;
    
    public void sendMessage(String topic, String message) {
        try {
            ProducerRecord<String, String> record = new ProducerRecord<>(
                topic,
                UUID.randomUUID().toString(),
                message
            );
            
            ListenableFuture<SendResult<String, String>> future = 
                kafkaTemplate.send(record);
                
            future.addCallback(new ListenableFutureCallback<>() {
                @Override
                public void onSuccess(SendResult<String, String> result) {
                    log.info("消息发送成功: topic = {}, partition = {}, offset = {}",
                        result.getRecordMetadata().topic(),
                        result.getRecordMetadata().partition(),
                        result.getRecordMetadata().offset());
                }
                
                @Override
                public void onFailure(Throwable ex) {
                    log.error("消息发送失败: ", ex);
                }
            });
        } catch (Exception e) {
            log.error("消息发送异常: ", e);
            throw new RuntimeException("消息发送失败", e);
        }
    }
    
    // 事务发送示例
    @Transactional
    public void sendMessageInTransaction(String topic, List<String> messages) {
        try {
            messages.forEach(message -> 
                kafkaTemplate.send(topic, UUID.randomUUID().toString(), message));
        } catch (Exception e) {
            log.error("事务消息发送失败: ",# Database Configuration Guide for Spring Boot 3.2.9

## MySQL Configuration

### Single Instance Setup

1. First, add the necessary dependencies to your `pom.xml`:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>com.mysql</groupId>
    <artifactId>mysql-connector-j</artifactId>
</dependency>
```

2. Create the base configuration class:

```java
@Configuration
@EnableTransactionManagement
public class MySQLConfig {
    
    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }
    
    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }
    
    @Primary
    @Bean(name = "entityManagerFactory")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity")
            .persistenceUnit("primaryPU")
            .build();
    }
    
    @Primary
    @Bean(name = "transactionManager")
    public PlatformTransactionManager transactionManager(
            @Qualifier("entityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

3. Configure application.yml:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false&serverTimezone=UTC
      username: root
      password: root
      hikari:
        maximum-pool-size: 10
        minimum-idle: 5
        idle-timeout: 300000
        connection-timeout: 20000
        connection-test-query: SELECT 1
```

### Multi-Source Configuration

For multiple data sources, extend the configuration:

```java
@Configuration
@EnableTransactionManagement
public class MultiDataSourceConfig {

    @Bean
    @Primary
    @ConfigurationProperties(prefix = "spring.datasource.primary")
    public DataSourceProperties primaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @ConfigurationProperties(prefix = "spring.datasource.secondary")
    public DataSourceProperties secondaryDataSourceProperties() {
        return new DataSourceProperties();
    }

    @Bean
    @Primary
    public DataSource primaryDataSource() {
        return primaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Bean
    public DataSource secondaryDataSource() {
        return secondaryDataSourceProperties()
            .initializeDataSourceBuilder()
            .type(HikariDataSource.class)
            .build();
    }

    @Primary
    @Bean(name = "primaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean primaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(primaryDataSource())
            .packages("com.study.collect.domain.entity.primary")
            .persistenceUnit("primaryPU")
            .build();
    }

    @Bean(name = "secondaryEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean secondaryEntityManagerFactory(
            EntityManagerFactoryBuilder builder) {
        return builder
            .dataSource(secondaryDataSource())
            .packages("com.study.collect.domain.entity.secondary")
            .persistenceUnit("secondaryPU")
            .build();
    }

    @Primary
    @Bean(name = "primaryTransactionManager")
    public PlatformTransactionManager primaryTransactionManager(
            @Qualifier("primaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }

    @Bean(name = "secondaryTransactionManager")
    public PlatformTransactionManager secondaryTransactionManager(
            @Qualifier("secondaryEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
```

Update application.yml for multiple sources:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/primary_db?useSSL=false
      username: root
      password: root
    secondary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/secondary_db?useSSL=false
      username: root
      password: root
```

### MySQL Cluster Configuration

For MySQL cluster setup:

1. Update application.yml for cluster configuration:

```yaml
spring:
  datasource:
    primary:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql:replication://master:3306,slave1:3306,slave2:3306/primary_db?useSSL=false
      username: root
      password: root
      hikari:
        maximum-pool-size: 20
        minimum-idle: 10
        connection-timeout: 30000
```

2. Create a routing data source for read-write splitting:

```java
public class RoutingDataSource extends AbstractRoutingDataSource {
    @Override
    protected Object determineCurrentLookupKey() {
        return TransactionSynchronizationManager.isCurrentTransactionReadOnly() 
            ? "slave" : "master";
    }
}

@Configuration
public class MySQLClusterConfig {
    
    @Bean
    public DataSource routingDataSource(
            @Qualifier("masterDataSource") DataSource masterDataSource,
            @Qualifier("slaveDataSource") DataSource slaveDataSource) {
        Map<Object, Object> targetDataSources = new HashMap<>();
        targetDataSources.put("master", masterDataSource);
        targetDataSources.put("slave", slaveDataSource);
        
        RoutingDataSource routingDataSource = new RoutingDataSource();
        routingDataSource.setTargetDataSources(targetDataSources);
        routingDataSource.setDefaultTargetDataSource(masterDataSource);
        
        return routingDataSource;
    }
}
```

## Redis Configuration

### Single Instance Setup

1. Add dependency:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

2. Create configuration:

```java
@Configuration
public class RedisConfig {
    
    @Bean
    @Primary
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Set serializers
        Jackson2JsonRedisSerializer<Object> jackson2JsonRedisSerializer = 
            new Jackson2JsonRedisSerializer<>(Object.class);
        ObjectMapper om = new ObjectMapper();
        om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        om.activateDefaultTyping(LaissezFaireSubTypeValidator.instance, 
            ObjectMapper.DefaultTyping.NON_FINAL);
        jackson2JsonRedisSerializer.setObjectMapper(om);
        
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        template.setKeySerializer(stringRedisSerializer);
        template.setHashKeySerializer(stringRedisSerializer);
        template.setValueSerializer(jackson2JsonRedisSerializer);
        template.setHashValueSerializer(jackson2JsonRedisSerializer);
        
        return template;
    }
}
```

3. Configure application.yml:

```yaml
spring:
  data:
    redis:
      host: localhost
      port: 6379
      password: 
      database: 0
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

### Redis Cluster Configuration

1. Update configuration for cluster:

```java
@Configuration
public class RedisClusterConfig {
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Serializer configuration remains the same
        // ... 
        
        return template;
    }
    
    @Bean
    public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
        return clientConfigurationBuilder -> {
            clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);
        };
    }
}
```

2. Configure application.yml for cluster:

```yaml
spring:
  data:
    redis:
      cluster:
        nodes:
          - redis-node1:6379
          - redis-node2:6379
          - redis-node3:6379
        max-redirects: 3
      password: 
      lettuce:
        pool:
          max-active: 16
          max-idle: 8
          min-idle: 0
          max-wait: -1ms
```

## RabbitMQ 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class RabbitConfig {
    
    @Bean
    public Queue queue() {
        // durable: 是否持久化
        // exclusive: 是否独占
        // autoDelete: 是否自动删除
        return new Queue("defaultQueue", true, false, false);
    }
    
    @Bean
    public DirectExchange directExchange() {
        return new DirectExchange("defaultExchange", true, false);
    }
    
    @Bean
    public Binding binding(Queue queue, DirectExchange exchange) {
        return BindingBuilder.bind(queue).to(exchange).with("defaultRouting");
    }
    
    @Bean
    public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
        // 消息转换器
        rabbitTemplate.setMessageConverter(new Jackson2JsonMessageConverter());
        // 开启确认模式
        rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -> {
            if (!ack) {
                log.error("消息发送失败：" + cause);
            }
        });
        return rabbitTemplate;
    }
    
    @Bean 
    public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(
            ConnectionFactory connectionFactory) {
        SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();
        factory.setConnectionFactory(connectionFactory);
        factory.setMessageConverter(new Jackson2JsonMessageConverter());
        // 设置并发消费者数量
        factory.setConcurrentConsumers(3);
        factory.setMaxConcurrentConsumers(10);
        // 设置预取数量
        factory.setPrefetchCount(5);
        return factory;
    }
}
```

3. 配置 application.yml:

```yaml
spring:
  rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest
    virtual-host: /
    # 开启发布确认
    publisher-confirm-type: correlated
    # 开启发布返回
    publisher-returns: true
    listener:
      simple:
        # 手动确认
        acknowledge-mode: manual
        # 最小消费者数量
        concurrency: 3
        # 最大消费者数量
        max-concurrency: 10
        # 预取数量
        prefetch: 5
        # 启动时自动启动容器
        auto-startup: true
        # 消费者默认启动
        default-requeue-rejected: true
```

### 集群配置

1. 更新配置类支持集群:

```java
@Configuration
public class RabbitClusterConfig {
    
    @Bean
    public ConnectionFactory connectionFactory() {
        CachingConnectionFactory connectionFactory = new CachingConnectionFactory();
        // 配置集群地址
        connectionFactory.setAddresses("rabbit1:5672,rabbit2:5672,rabbit3:5672");
        connectionFactory.setUsername("guest");
        connectionFactory.setPassword("guest");
        connectionFactory.setVirtualHost("/");
        // 开启发布确认
        connectionFactory.setPublisherConfirms(true);
        // 开启发布返回
        connectionFactory.setPublisherReturns(true);
        return connectionFactory;
    }
    
    @Bean
    public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) {
        return new RabbitAdmin(connectionFactory);
    }
    
    // 其他Bean配置保持不变...
}
```

2. 集群配置 application.yml:

```yaml
spring:
  rabbitmq:
    addresses: rabbit1:5672,rabbit2:5672,rabbit3:5672
    username: guest
    password: guest
    virtual-host: /
    # 集群配置
    cluster:
      # 节点间通信端口
      node-port: 25672
      # 集群名称
      name: rabbit@cluster
    # 连接池配置  
    connection:
      # 缓存连接数
      cache-size: 20
      # 最大连接数
      max-connections: 50
```

### 使用示例

1. 生产者示例:

```java
@Service
@Slf4j
public class MessageProducer {
    
    @Autowired
    private RabbitTemplate rabbitTemplate;
    
    public void sendMessage(Object message, String routingKey) {
        try {
            // 构建消息
            Message msg = MessageBuilder
                .withBody(new ObjectMapper().writeValueAsBytes(message))
                .setContentType(MessageProperties.CONTENT_TYPE_JSON)
                .setMessageId(UUID.randomUUID().toString())
                .setTimestamp(new Date())
                .build();
                
            // 发送消息并获取结果    
            rabbitTemplate.convertAndSend("defaultExchange", routingKey, msg, messagePostProcessor -> {
                // 设置消息持久化
                messagePostProcessor.getMessageProperties().setDeliveryMode(MessageDeliveryMode.PERSISTENT);
                return messagePostProcessor;
            });
            
        } catch (Exception e) {
            log.error("消息发送失败: ", e);
            throw new RuntimeException("消息发送失败", e);
        }
    }
}
```

2. 消费者示例:

```java
@Component
@Slf4j
public class MessageConsumer {

    @RabbitListener(queues = "defaultQueue")
    public void processMessage(Message message, Channel channel) {
        try {
            // 获取消息标识
            long deliveryTag = message.getMessageProperties().getDeliveryTag();
            
            // 处理消息
            String messageBody = new String(message.getBody());
            log.info("收到消息: {}", messageBody);
            
            // 确认消息
            channel.basicAck(deliveryTag, false);
            
        } catch (Exception e) {
            try {
                // 消息处理失败,拒绝消息
                channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true);
            } catch (IOException ex) {
                log.error("消息拒绝失败: ", ex);
            }
            log.error("消息处理失败: ", e);
        }
    }
}
```

## Kafka 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class KafkaConfig {
    
    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "default-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        // 自动提交配置
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        // 消费者超时时间
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "15000");
        return new DefaultKafkaConsumerFactory<>(props);
    }
    
    @Bean
    public ProducerFactory<String, String> producerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        // 生产者确认机制
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        // 重试次数
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        // 批量大小
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
        // 缓冲区大小
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
        return new DefaultKafkaProducerFactory<>(props);
    }
    
    @Bean
    public KafkaTemplate<String, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        // 设置并发数
        factory.setConcurrency(3);
        // 批量监听
        factory.setBatchListener(true);
        return factory;
    }
}
```

3. 配置 application.yml:

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      # 重试次数
      retries: 3
      # 批量大小
      batch-size: 16384
      # 缓冲区大小
      buffer-memory: 33554432
      # 键的序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 值的序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 确认机制
      acks: all
    consumer:
      # 默认组id  
      group-id: default-group
      # 自动提交
      enable-auto-commit: false
      # 自动提交间隔
      auto-commit-interval: 1000
      # 键的反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 值的反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 当各分区下有已提交的offset时,从提交的offset开始消费;无提交的offset时,从头开始消费  
      auto-offset-reset: earliest
    listener:
      # 手动提交
      ack-mode: manual
      # 并发数
      concurrency: 3
```

### 集群配置

1. 更新配置类支持集群:

```java
@Configuration
public class KafkaClusterConfig {

    @Bean
    public Map<String, Object> producerConfigs() {
        Map<String, Object> props = new HashMap<>();
        // 集群地址
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
            "kafka1:9092,kafka2:9092,kafka3:9092");
        // 重试次数
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        // 幂等性
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        // 事务ID前缀
        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "tx-");
        // 其他配置保持不变...
        return props;
    }
    
    @Bean
    public Map<String, Object> consumerConfigs() {
        Map<String, Object> props = new HashMap<>();
        // 集群地址
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
            "kafka1:9092,kafka2:9092,kafka3:9092");
        // 隔离级别
        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        // 其他配置保持不变...
        return props;
    }
    
    // 其他Bean配置保持不变...
}
```

2. 集群配置 application.yml:

```yaml
spring:
  kafka:
    bootstrap-servers: kafka1:9092,kafka2:9092,kafka3:9092
    producer:
      # 启用事务
      transaction-id-prefix: tx-
      # 启用幂等性
      properties:
        enable.idempotence: true
      # 其他配置保持不变...
    consumer:
      # 事务隔离级别
      properties:
        isolation.level: read_committed
      # 其他配置保持不变...
```

### 使用示例

1. 生产者示例:

```java
@Service
@Slf4j
public class KafkaProducer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;
    
    public void sendMessage(String topic, String message) {
        try {
            ProducerRecord<String, String> record = new ProducerRecord<>(
                topic,
                UUID.randomUUID().toString(),
                message
            );
            
            ListenableFuture<SendResult<String, String>> future = 
                kafkaTemplate.send(record);
                
            future.addCallback(new ListenableFutureCallback<>() {
                @Override
                public void onSuccess(SendResult<String, String> result) {
                    log.info("消息发送成功: topic = {}, partition = {}, offset = {}",
                        result.getRecordMetadata().topic(),
                        result.getRecordMetadata().partition(),
                        result.getRecordMetadata().offset());
                }
                
                @Override
                public void onFailure(Throwable ex) {
                    log.error("消息发送失败: ", ex);
                }
            });
        } catch (Exception e) {
            log.error("消息发送异常: ", e);
            throw new RuntimeException("消息发送失败", e);
        }
    }
    
    // 事务发送示例
    @Transactional
    public void sendMessageInTransaction(String topic, List<String> messages) {
        try {
            messages.forEach(message -> 
                kafkaTemplate.send(topic, UUID.randomUUID().toString(), message));
        } catch (Exception e) {
            log.error("事务消息发送失败: ", e);
            // 事务会自动回滚
            throw new RuntimeException("事务消息发送失败", e);
        }
    }
}
```

2. 消费者示例:

```java
@Component
@Slf4j
public class KafkaConsumer {

    @KafkaListener(topics = "test-topic", groupId = "test-group")
    public void handleMessage(ConsumerRecord<String, String> record,
                            Acknowledgment ack) {
        try {
            // 获取消息内容
            String message = record.value();
            log.info("收到消息: topic = {}, partition = {}, offset = {}, message = {}",
                record.topic(), record.partition(), record.offset(), message);
            
            // 处理业务逻辑
            processMessage(message);
            
            // 手动提交确认
            ack.acknowledge();
            
        } catch (Exception e) {
            // 处理异常,可以选择重试或者放入死信队列
            log.error("消息处理失败: ", e);
            handleMessageError(record);
        }
    }
    
    // 批量消费示例
    @KafkaListener(topics = "batch-topic", groupId = "batch-group", 
                  containerFactory = "batchKafkaListenerContainerFactory")
    public void handleBatchMessages(List<ConsumerRecord<String, String>> records,
                                  Acknowledgment ack) {
        try {
            // 批量处理消息
            for (ConsumerRecord<String, String> record : records) {
                processMessage(record.value());
            }
            
            // 批量确认
            ack.acknowledge();
            
        } catch (Exception e) {
            log.error("批量消息处理失败: ", e);
            // 可以将失败的消息放入重试队列
            handleBatchError(records);
        }
    }
    
    private void processMessage(String message) {
        // 具体的业务处理逻辑
    }
    
    private void handleMessageError(ConsumerRecord<String, String> record) {
        // 错误处理逻辑,如重试或放入死信队列
    }
    
    private void handleBatchError(List<ConsumerRecord<String, String>> records) {
        // 批量错误处理逻辑
    }
}
```

## Zookeeper 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-framework</artifactId>
    <version>5.5.0</version>
</dependency>
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>5.5.0</version>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class ZookeeperConfig {
    
    @Value("${zookeeper.connectString}")
    private String connectString;
    
    @Value("${zookeeper.sessionTimeoutMs}")
    private int sessionTimeoutMs;
    
    @Value("${zookeeper.connectionTimeoutMs}")
    private int connectionTimeoutMs;
    
    @Value("${zookeeper.namespace}")
    private String namespace;
    
    @Bean(initMethod = "start", destroyMethod = "close")
    public CuratorFramework curatorFramework() {
        // 重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);
        
        // 构建客户端
        return CuratorFrameworkFactory.builder()
            .connectString(connectString)
            .sessionTimeoutMs(sessionTimeoutMs)
            .connectionTimeoutMs(connectionTimeoutMs)
            .retryPolicy(retryPolicy)
            .namespace(namespace)
            .build();
    }
    
    // 分布式锁实现
    @Bean
    public InterProcessMutex distributedLock(CuratorFramework client) {
        return new InterProcessMutex(client, "/distributed-lock");
    }
    
    // 服务注册与发现
    @Bean
    public ServiceDiscovery<Object> serviceDiscovery(CuratorFramework client) throws Exception {
        return ServiceDiscoveryBuilder.builder(Object.class)
            .client(client)
            .basePath("/services")
            .serializer(new JsonInstanceSerializer<>(Object.class))
            .build();
    }
}
```

3. 配置 application.yml:

```yaml
zookeeper:
  connectString: localhost:2181
  sessionTimeoutMs: 60000
  connectionTimeoutMs: 15000
  namespace: my-app
```

### 集群配置

1. 更新配置支持集群:

```java
@Configuration
public class ZookeeperClusterConfig {
    
    @Bean(initMethod = "start", destroyMethod = "close")
    public CuratorFramework curatorFramework() {
        // 集群连接字符串
        String connectString = "zk1:2181,zk2:2181,zk3:2181";
        
        // 重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);
        
        // 构建客户端
        return CuratorFrameworkFactory.builder()
            .connectString(connectString)
            .sessionTimeoutMs(60000)
            .connectionTimeoutMs(15000)
            .retryPolicy(retryPolicy)
            .namespace("my-app")
            .build();
    }
    
    // Leader选举实现
    @Bean
    public LeaderLatch leaderLatch(CuratorFramework client) throws Exception {
        return new LeaderLatch(client, "/leader");
    }
    
    // 分布式计数器
    @Bean
    public DistributedAtomicLong distributedCounter(CuratorFramework client) throws Exception {
        return new DistributedAtomicLong(client, "/counter", 
            new ExponentialBackoffRetry(1000, 3));
    }
}
```

2. 集群配置 application.yml:

```yaml
zookeeper:
  connectString: zk1:2181,zk2:2181,zk3:2181
  sessionTimeoutMs: 60000
  connectionTimeoutMs: 15000
  namespace: my-app
  # 集群配置
  cluster:
    # 集群名称
    name: zk-cluster
    # 选举超时时间
    electionMs: 5000
    # 心跳间隔
    tickMs: 2000
```

### 使用示例

1. 分布式锁使用:

```java
@Service
@Slf4j
public class DistributedLockService {

    @Autowired
    private InterProcessMutex distributedLock;
    
    public void executeWithLock(String resourceId, Runnable task) {
        try {
            // 获取锁,设置超时时间
            if (distributedLock.acquire(30, TimeUnit.SECONDS)) {
                try {
                    // 执行任务
                    task.run();
                } finally {
                    // 释放锁
                    distributedLock.release();
                }
            } else {
                log.warn("获取分布式锁超时: {}", resourceId);
                throw new RuntimeException("获取分布式锁超时");
            }
        } catch (Exception e) {
            log.error("分布式锁操作异常: ", e);
            throw new RuntimeException("分布式锁操作失败", e);
        }
    }
}
```

2. 服务注册与发现:

```java
@Service
@Slf4j
public class ServiceRegistryService {

    @Autowired
    private ServiceDiscovery<Object> serviceDiscovery;
    
    public void registerService(String serviceName, String serviceAddress) {
        try {
            // 创建服务实例
            ServiceInstance<Object> instance = ServiceInstance.builder()
                .name(serviceName)
                .address(serviceAddress)
                .port(8080)
                .build();
                
            // 注册服务    
            serviceDiscovery.registerService(instance);
            log.info("服务注册成功: {}", serviceName);
            
        } catch (Exception e) {
            log.error("服务注册失败: ", e);
            throw new RuntimeException("服务注册失败", e);
        }
    }
    
    public List<ServiceInstance<Object>> discoverServices(String serviceName) {
        try {
            // 发现服务
            return serviceDiscovery.queryForInstances(serviceName);
        } catch (Exception e) {
            log.error("服务发现失败: ", e);
            throw new RuntimeException("服务发现失败", e);
        }
    }
}
```

## MongoDB 配置

### 单机版配置

1. 添加依赖:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-mongodb</artifactId>
</dependency>
```

2. 创建配置类:

```java
@Configuration
public class MongoConfig {

    @Bean
    public MongoCustomConversions mongoCustomConversions() {
        List<Converter<?, ?>> converters = new ArrayList<>();
        // 添加自定义转换器
        converters.add(new DateToLocalDateTimeConverter());
        converters.add(new LocalDateTimeToDateConverter());
        return new MongoCustomConversions(converters);
    }
    
    @Bean
    public MongoTemplate mongoTemplate(MongoClient mongoClient) {
        return new MongoTemplate(mongoClient, "test");
    }
    
    // 审计功能支持
    @Bean
    public AuditorAware<String> auditorProvider() {
        return () -> Optional.of(SecurityContextHolder.getContext()
            .getAuthentication().getName());
    }
}

// 自定义转换器
public class DateToLocalDateTimeConverter implements Converter<Date, LocalDateTime> {
    @Override
    public LocalDateTime convert(Date source) {
        return LocalDateTime.ofInstant(source.toInstant(), ZoneId.systemDefault());
    }
}

public class LocalDateTimeToDateConverter implements Converter<LocalDateTime, Date> {
    @Override
    public Date convert(LocalDateTime source) {
        return Date.from(source.atZone(ZoneId.systemDefault()).toInstant());
    }
}
```

3. 配置 application.yml:

```yaml
spring:
  data:
    mongodb:
      host: localhost
      port: 27017
      database: test
      # 身份认证
      username: admin
      password: admin
      # 连接池配置
      connection:
        minPoolSize: 0
        maxPoolSize: 100
        maxIdleTimeMS: 900000
        maxLifeTimeMS: 3600000
```

### 集群配置

1. 更新配置支持副本集:

```java
@Configuration
public class MongoClusterConfig {

    @Bean
    public MongoClient mongoClient() {
        // 集群节点列表
        List<ServerAddress> seeds = Arrays.asList(
            new ServerAddress("mongo1", 27017),
            new ServerAddress("mongo2", 27017),
            new ServerAddress("mongo3", 27017)
        );
        
        // 构建客户端配置
        MongoClientSettings settings = MongoClientSettings.builder()
            .applyToClusterSettings(builder -> 
                builder.hosts(seeds)
                    .mode(ClusterConnectionMode.MULTIPLE)
                    .requiredReplicaSetName("rs0"))
            .applyToConnectionPoolSettings(builder ->
                builder.maxSize(100)
                    .minSize(0)
                    .maxWaitTime(15, TimeUnit.SECONDS))
            .applyToSocketSettings(builder ->
                builder.connectTimeout(10, TimeUnit.SECONDS)
                    .readTimeout(30, TimeUnit.SECONDS))
            .writeConcern(WriteConcern.MAJORITY)
            .readPreference(ReadPreference.primaryPreferred())
            .build();
            
        return MongoClients.create(settings);
    }
    
    @Bean
    public MongoTemplate mongoTemplate(MongoClient mongoClient) {
        return new MongoTemplate(mongoClient, "test");
    }
}
```

2. 集群配置 application.yml:

```yaml
spring:
  data:
    mongodb:
      uri: mongodb://mongo1:27017,mongo2:27017,mongo3:27017/test?replicaSet=rs0
      # 副本集配置
      replica-set-name: rs0
      # 写关注
      write-concern: MAJORITY
      # 读偏好
      read-preference: primaryPreferred
      # 连接池配置
      connection:
        maxPoolSize: 100
        minPoolSize: 0
        maxWaitTimeMS: 15000
        maxConnectionLifeTimeMS: 3600000
```

### 使用示例

1. 实体类定义:

```java
@Document(collection = "users")
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class User {
    
    @Id
    private String id;
    
    @Field("name")
    private String name;
    
    @Field("age")
    private Integer age;
    
    @Field("email")
    private String email;
    
    @CreatedDate
    private LocalDateTime createTime;
    
    @LastModifiedDate
    private LocalDateTime updateTime;
    
    @Version
    private Long version;
}
```

2. Repository 接口:

```java
@Repository
public interface UserRepository extends MongoRepository<User, String> {
    
    // 自定义查询方法
    List<User> findByNameLike(String name);
    
    List<User> findByAgeBetween(Integer minAge, Integer maxAge);
    
    // 使用 @Query 注解
    @Query("{'age': {'$gte': ?0, '$lte': ?1}}")
    List<User> findUsersByAgeRange(Integer minAge, Integer maxAge);
    
    // 聚合查询
    @Aggregation(pipeline = {
        "{'$match': {'age': {'$gte': ?0}}}",
        "{'$group': {'_id': null, 'avgAge': {'$avg': '$age'}}}"
    })
    Double calculateAverageAgeAbove(Integer minAge);
}
```

3. Service 实现:

```java
@Service
@Slf4j
public class UserService {

    @Autowired
    private UserRepository userRepository;
    
    @Autowired
    private MongoTemplate mongoTemplate;
    
    // 基本CRUD操作好的,我继续完善 ClickHouse 的配置说明,并补充更多实用的查询示例。让我们深入了解如何充分利用 ClickHouse 的特性。



让我继续介绍 ClickHouse 的性能优化和最佳实践。对于日志分析这种大数据量的场景,性能优化尤为重要。

1. 表设计优化

首先，让我们看看如何优化表结构设计：

```sql
CREATE TABLE log_events
(
    id String,
    type String,
    level String,
    message String,
    tags String, -- 存储 JSON 格式的标签
    timestamp DateTime,
    date Date MATERIALIZED toDate(timestamp),
    -- 为了优化查询性能添加的计算列
    hour UInt8 MATERIALIZED toHour(timestamp),
    minute UInt8 MATERIALIZED toMinute(timestamp),
    is_error UInt8 MATERIALIZED level = 'ERROR'
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (date, level, type)
SETTINGS index_granularity = 8192;
```

这个表结构包含了几个重要的优化：

1. 使用分区键按月划分数据
2. 合理的排序键设计
3. 物化列加速查询
4. 适当的索引粒度设置

您需要我详细解释这些优化策略的原理和使用场景吗？或者您想了解其他方面的优化技巧？